{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9a0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import csv\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import re\n",
    "from requests.exceptions import RequestException\n",
    "from memory_profiler import profile\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "import queue\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154865ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to output to both file and console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data/log/studio_scraper.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "class StudioScraper:\n",
    "    def __init__(self):\n",
    "        self.equipment_categories = {\n",
    "            'gear', 'equipment', 'equip', 'control room', 'studio', 'tech', 'technik', 'recording',\n",
    "            'specs', 'desk', 'microphone', 'microphones', 'recording', 'equalizer', 'equaliser',\n",
    "            'facilities', 'console', 'outboard', 'instruments', 'monitoring', \n",
    "            'monitors', 'speakers', 'processors', 'compressors', 'kompressor', 'plugins',\n",
    "            'pre-amps', 'preamps', 'plug-ins', 'backline', 'instruments', 'tube', 'dynamic', \n",
    "            'condensers', 'ribbon'\n",
    "        }\n",
    "\n",
    "        self.target_equipment = {\n",
    "            'Microphones 0': {\n",
    "                'Telefunken ELA M 251': ['ELAM 251', 'ELA M251', 'ELA-M 251', 'Telefunken 251', 'ELA M 251E'],\n",
    "                'Neumann U47': ['U-47', 'U 47', 'VF14 U47', 'U47 Vintage', 'Neumann VF14'],\n",
    "                'Neumann U67': ['U-67', 'U 67', 'U.67', 'U67 Vintage', 'Neumann 67'],\n",
    "                'AKG C12': ['C-12', 'C 12', 'C.12', 'AKG C12VR', 'C12 Vintage']\n",
    "            },\n",
    "            'Microphones 1': {\n",
    "                'Neumann U87': ['U-87', 'U 87', 'U87ai', 'U87 Ai', 'U87A', 'U87 Vintage'],\n",
    "                'Neumann M49': ['M-49', 'M 49', 'M.49', 'M49 Vintage', 'Neumann M 49'],\n",
    "                'Telefunken U47': ['Telefunken U-47', 'Tele U47', 'Telefunken U 47'],\n",
    "                'AKG C414': ['C-414', 'C414 XLII', 'C414 XLS', 'C414B-ULS', 'C414 EB'],\n",
    "                'Coles 4038': ['Coles 4038S', '4038 Ribbon', 'Coles Ribbon'],\n",
    "                'Schoeps CMC6': ['CMC 6', 'CMC-6', 'Schoeps CMC6 MK', 'CMC6/MK'],\n",
    "                'Royer R-121': ['R121', 'R 121', 'Royer 121', 'R-121 Ribbon']\n",
    "            },\n",
    "            'Microphones 2': {\n",
    "                'AKG C414 EB': ['C414EB', 'C-414 EB', 'C414 EB Silver', 'C414 EB Vintage'],\n",
    "                'Neumann KM84': ['KM-84', 'KM 84', 'KM84i', 'KM 84 Vintage'],\n",
    "                'Sennheiser MD421': ['MD-421', 'MD 421 II', 'MD421-U'],\n",
    "                'Shure SM7B': ['SM7 B', 'SM-7B', 'SM7b', 'SM7'],\n",
    "                'Electro-Voice RE20': ['RE-20', 'RE 20', 'EV RE20', 'RE20 Broadcast'],\n",
    "                'Neumann TLM103': ['TLM-103', 'TLM 103', 'TLM.103']\n",
    "            },\n",
    "            'Microphones 3': {\n",
    "                'Shure SM57': ['SM-57', 'SM 57', 'SM57-LC'],\n",
    "                'Shure SM58': ['SM-58', 'SM 58', 'SM58-LC']\n",
    "            },\n",
    "            'Compressors 0': {\n",
    "                'Fairchild 670': ['Fairchild 670 Stereo', '670 Limiter', 'Fairchild 670/660'],\n",
    "                'Neve 2254': ['2254/E', '2254E', '2254A', 'Neve 2254E'],\n",
    "                'Teletronix LA2A': ['LA-2A', 'LA 2A', 'Teletronix LA-2A', 'LA2A Leveler']\n",
    "            },\n",
    "            'Compressors 1': {\n",
    "                'Universal Audio 1176LN': ['1176 LN', 'UA 1176', 'UREI 1176', '1176LN Rev'],\n",
    "                'Empirical Labs Distressor': ['EL8 Distressor', 'EL-8', 'EL8-X'],\n",
    "                'Tube-Tech CL1B': ['CL 1B', 'CL-1B', 'Tube Tech CL1B'],\n",
    "                'SSL G Bus Compressor': ['SSL G-Series', 'SSL G Comp', 'G Series Comp']\n",
    "            },\n",
    "            'Compressors 2': {\n",
    "                'DBX 160VU': ['DBX 160', '160 VU', 'DBX VU Compressor'],\n",
    "                'API 2500': ['API-2500', '2500 Comp', '2500 Compressor'],\n",
    "                'Thermionic Culture Phoenix': ['Phoenix Compressor', 'Thermionic Phoenix', 'Phoenix Tube Compressor'],\n",
    "                'Manley Vari-Mu': ['Vari Mu', 'VariMu', 'Manley Tube Compressor']\n",
    "            },\n",
    "            'Compressors 3': {\n",
    "                'Focusrite Red': ['Red Compressor', 'Focusrite Red Series', 'Focusrite Red 3']\n",
    "            },\n",
    "            'Preamps 0': {\n",
    "                'Neve 1073': ['1073 Preamp', 'AMS Neve 1073', 'Neve Classic 1073'],\n",
    "                'API 312': ['API-312', '312 Preamp', '312 Mic Pre']\n",
    "            },\n",
    "            'Preamps 1': {\n",
    "                'Neve 1073': ['1073 Pre', 'Neve Pre 1073', 'AMS 1073'],\n",
    "                'API 3124+': ['3124+', 'API 3124 Plus', '3124 Mic Pre'],\n",
    "                'API 512c': ['512C', 'API-512', '512 Mic Preamp'],\n",
    "                'Neve 1084': ['1084 Preamp', 'Neve 1084 EQ/Pre', 'AMS Neve 1084'],\n",
    "                'SSL SuperAnalogue': ['SuperAnalogue Preamp', 'SSL Super Analogue', 'SSL SA Pre']\n",
    "            },\n",
    "            'Preamps 2': {\n",
    "                'Universal Audio 610': ['UA 610', '610 Tube Preamp', 'Universal 610'],\n",
    "                'Focusrite ISA 110': ['ISA 110', 'Focusrite 110 Pre', 'Focusrite ISA'],\n",
    "                'Shadow Hills Gama': ['Gama Preamp', 'Shadow Hills Mic Pre', 'Shadow Hills G.A.M.A.'],\n",
    "                'Chandler TG2': ['TG2 Preamp', 'Chandler TG-2', 'Abbey Road TG2']\n",
    "            },\n",
    "            'Preamps 3': {\n",
    "                'Focusrite Scarlett': ['Scarlett Preamp', 'Focusrite Scarlett Series'],\n",
    "                'Presonus Studio 24C': ['24C Preamp', 'Studio 24C']\n",
    "            },\n",
    "            'Equalisation 0': {\n",
    "                'Pultec EQP-1A': ['EQP 1A', 'Pultec 1A EQ', 'Pultec Tube EQ'],\n",
    "                'Manley Massive Passive': ['Massive Passive EQ', 'Manley MP EQ']\n",
    "            },\n",
    "            'Equalisation 1': {\n",
    "                'API 550A': ['550A EQ', 'API 550 EQ'],\n",
    "                'Neve 1073': ['1073 EQ', 'Neve EQ 1073'],\n",
    "                'GML 8200': ['8200 EQ', 'GML Parametric EQ'],\n",
    "                'SSL 4000': ['SSL EQ 4000', '4000 Series EQ', 'SSL 4K EQ']\n",
    "            },\n",
    "            'Equalisation 2': {\n",
    "                'API 550B': ['550B EQ', 'API 550 EQ-B'],\n",
    "                'Focusrite ISA 110': ['ISA 110 EQ', 'Focusrite 110 EQ'],\n",
    "                'Chandler Curve Bender': ['Curve Bender', 'Chandler CB EQ']\n",
    "            },\n",
    "            'Equalisation 3': {\n",
    "                'Behringer graphic': ['Behringer EQ', 'Behringer Graphic EQ']\n",
    "            },\n",
    "            'Monitors 0': {\n",
    "                'Barefoot Sound MM27': ['MM27 Monitors', 'Barefoot MM27', 'Barefoot Sound'],\n",
    "                'ATC SCM150ASL Pro': ['SCM150 ASL', 'ATC SCM150', 'SCM150 Pro']\n",
    "            },\n",
    "            'Monitors 1': {\n",
    "                'ATC SCM25A': ['SCM25A', 'ATC SCM25'],\n",
    "                'Focal Twin 6 Be': ['Twin 6 Be', 'Focal Twin6', 'Focal Twin'],\n",
    "                'Genelec 8351A': ['8351A Monitors', 'Genelec 8351', 'The Ones 8351'],\n",
    "                'PMC BB5/XBD': ['BB5 XBD', 'PMC BB5 Monitors', 'PMC XBD'],\n",
    "                'Quested HQ210': ['HQ210 Monitors', 'Quested HQ-210', 'Quested Studio Monitors']\n",
    "            },\n",
    "            'Monitors 2': {\n",
    "                'Focal SM9': ['SM9 Monitors', 'Focal SM-9', 'Focal 3-Way Monitors'],\n",
    "                'Genelec 1031A': ['1031A Monitors', 'Genelec 1031'],\n",
    "                'Yamaha NS10M': ['NS-10M', 'NS10 Monitors', 'Yamaha NS10'],\n",
    "                'ADAM Audio S3H': ['S3H Monitors', 'Adam S3H', 'Adam Audio']\n",
    "            },\n",
    "            'Monitors 3': {\n",
    "                'Yamaha HS8': ['HS8 Monitors', 'Yamaha HS-8'],\n",
    "                'KRK Rokit': ['Rokit Monitors', 'KRK Rokit Series'],\n",
    "                'Avantone Mixcubes': ['Mixcubes', 'Avantone Cubes']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        self.visited_urls = set()\n",
    "        self.rate_limit_delay = 3\n",
    "        self.fuzzy_match_threshold = 85\n",
    "        \n",
    "        self.scoring_weights = {\n",
    "            'equipment_match': 3.0,    # Direct equipment matches\n",
    "            'category_match': 1.5,     # Equipment category matches\n",
    "            'url_structure': 1.0,      # URL contains relevant terms\n",
    "            'page_structure': 1.0,     # Page has equipment-like structure\n",
    "            'context_quality': 2.0     # Quality of surrounding content\n",
    "        }\n",
    "        \n",
    "        # Add grading system configuration\n",
    "        self.category_points = {\n",
    "            'Microphones': 30,\n",
    "            'Compressors': 30,\n",
    "            'Preamps': 30,\n",
    "            'Equalisation': 30,\n",
    "            'Monitors': 30\n",
    "        }\n",
    "        \n",
    "        self.tier_points = {\n",
    "            0: 7,  # Exceptional/Rare Vintage\n",
    "            1: 5,  # High-End\n",
    "            2: 3,  # Mid-Range\n",
    "            3: 1   # Entry-Level/Common\n",
    "        }\n",
    "        \n",
    "        self.grading_scale = {\n",
    "            (135, 150): 'Exceptional',\n",
    "            (110, 134): 'Excellent',\n",
    "            (85, 109): 'Good',\n",
    "            (60, 84): 'Fair',\n",
    "            (0, 59): 'Needs Improvement'\n",
    "        }\n",
    "\n",
    "    def get_category_base(self, category: str) -> str:\n",
    "        \"\"\"Extract base category name from category key.\"\"\"\n",
    "        return ''.join([i for i in category if not i.isdigit()]).strip()\n",
    "\n",
    "    def get_tier(self, category: str) -> int:\n",
    "        \"\"\"Extract tier number from category key.\"\"\"\n",
    "        return int(re.findall(r'\\d+', category)[0])\n",
    "\n",
    "    def calculate_equipment_grade(self, matches: Dict[str, List]) -> Dict:\n",
    "        \"\"\"Calculate equipment grade based on matches.\"\"\"\n",
    "        # Initialize category scores\n",
    "        category_scores = defaultdict(int)\n",
    "        equipment_counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        # Process matches\n",
    "        for category, equipment_matches in matches.items():\n",
    "            if not equipment_matches:\n",
    "                continue\n",
    "                \n",
    "            # Get unique equipment matches\n",
    "            unique_matches = {match['equipment'] for match in equipment_matches}\n",
    "            base_category = self.get_category_base(category)\n",
    "            tier = self.get_tier(category)\n",
    "            \n",
    "            # Count equipment in each tier\n",
    "            equipment_counts[base_category][tier] = len(unique_matches)\n",
    "            \n",
    "            # Calculate base points for this tier\n",
    "            if len(unique_matches) > 0:\n",
    "                # Base points for having equipment in this tier\n",
    "                base_points = self.tier_points[tier]\n",
    "                \n",
    "                # Add bonus points for multiple Tier 0 or Tier 1 items\n",
    "                if tier in [0, 1] and len(unique_matches) > 1:\n",
    "                    if len(unique_matches) == 2:\n",
    "                        base_points += 2  # Two items = +2 points\n",
    "                    else:\n",
    "                        base_points += 4  # Three or more items = +4 points\n",
    "                \n",
    "                category_scores[base_category] += base_points\n",
    "        \n",
    "        # Calculate total score and ensure it doesn't exceed category maximums\n",
    "        total_score = 0\n",
    "        for category, score in category_scores.items():\n",
    "            category_scores[category] = min(score, self.category_points[category])\n",
    "            total_score += category_scores[category]\n",
    "        \n",
    "        # Determine grade\n",
    "        grade = None\n",
    "        for (min_score, max_score), grade_label in self.grading_scale.items():\n",
    "            if min_score <= total_score <= max_score:\n",
    "                grade = grade_label\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'total_score': total_score,\n",
    "            'grade': grade,\n",
    "            'category_scores': dict(category_scores),\n",
    "            'equipment_counts': dict(equipment_counts)\n",
    "        }\n",
    "        \n",
    "    def rate_page_structure(self, soup: BeautifulSoup) -> float:\n",
    "        \"\"\"Rate the page structure based on how likely it is to be an equipment list.\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Check for structured lists\n",
    "        lists = soup.find_all(['ul', 'ol', 'dl'])\n",
    "        if lists:\n",
    "            score += 0.3\n",
    "            \n",
    "        # Check for tables\n",
    "        tables = soup.find_all('table')\n",
    "        if tables:\n",
    "            score += 0.3\n",
    "            \n",
    "        # Check for equipment-related headers\n",
    "        headers = soup.find_all(['h1', 'h2', 'h3', 'h4'])\n",
    "        for header in headers:\n",
    "            if any(term in header.get_text().lower() for term in self.equipment_categories):\n",
    "                score += 0.2\n",
    "                \n",
    "        # Check for equipment sections\n",
    "        sections = soup.find_all(['div', 'section'], class_=lambda x: x and any(\n",
    "            term in x.lower() for term in self.equipment_categories))\n",
    "        if sections:\n",
    "            score += 0.2\n",
    "            \n",
    "        return min(score, 1.0)  # Normalize to 0-1\n",
    "\n",
    "    def rate_context_quality(self, content: str, matches: List[Dict]) -> float:\n",
    "        \"\"\"Rate the quality of context around equipment matches.\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Check for technical specifications\n",
    "        tech_terms = ['specifications', 'specs', 'technical', 'model', 'serial']\n",
    "        if any(term in content.lower() for term in tech_terms):\n",
    "            score += 0.3\n",
    "            \n",
    "        # Check for categorization\n",
    "        if any(cat in content.lower() for cat in ['microphones', 'preamps', 'compressors', 'equalizers']):\n",
    "            score += 0.3\n",
    "            \n",
    "        # Check for professional terminology\n",
    "        pro_terms = ['studio', 'professional', 'audio', 'recording', 'mixing']\n",
    "        if any(term in content.lower() for term in pro_terms):\n",
    "            score += 0.2\n",
    "            \n",
    "        # Check for match density\n",
    "        if matches:\n",
    "            unique_matches = len(set(m['equipment'] for m in matches))\n",
    "            score += min(0.2, unique_matches * 0.02)\n",
    "            \n",
    "        return min(score, 1.0)\n",
    "\n",
    "    def calculate_url_rating(self, url: str, soup: BeautifulSoup, matches: List[Dict], content: str) -> Dict:\n",
    "        \"\"\"Calculate a comprehensive rating for the URL based on multiple factors.\"\"\"\n",
    "        url_lower = url.lower()\n",
    "        \n",
    "        # Calculate individual scores\n",
    "        scores = {\n",
    "            'equipment_match': min(1.0, len(matches) * 0.1),  # Equipment matches score\n",
    "            'category_match': sum(term in url_lower for term in self.equipment_categories) * 0.2,  # Category relevance\n",
    "            'url_structure': 1.0 if any(term in url_lower for term in ['gear', 'equipment', 'tech']) else 0.0,  # URL relevance\n",
    "            'page_structure': self.rate_page_structure(soup),  # Page structure score\n",
    "            'context_quality': self.rate_context_quality(content, matches)  # Context quality score\n",
    "        }\n",
    "        \n",
    "        # Calculate weighted total\n",
    "        weighted_total = sum(\n",
    "            scores[key] * self.scoring_weights[key] \n",
    "            for key in scores\n",
    "        )\n",
    "        \n",
    "        # Normalize to 0-100 scale\n",
    "        max_possible = sum(self.scoring_weights.values())\n",
    "        final_score = (weighted_total / max_possible) * 100\n",
    "        \n",
    "        return {\n",
    "            'score': round(final_score, 2),\n",
    "            'component_scores': {k: round(v * 100, 2) for k, v in scores.items()}\n",
    "        }\n",
    "\n",
    "    def calculate_relevance_score(self, url: str, text_content: str) -> float:\n",
    "        score = 0\n",
    "        url_lower = url.lower()\n",
    "        for term in self.equipment_categories:\n",
    "            if term in url_lower:\n",
    "                score += 2\n",
    "        \n",
    "        text_lower = text_content.lower()\n",
    "        for term in self.equipment_categories:\n",
    "            if term in text_lower:\n",
    "                score += 1\n",
    "                \n",
    "        for category in self.target_equipment.values():\n",
    "            for equipment in category:\n",
    "                if equipment.lower() in text_lower:\n",
    "                    score += 3\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def fuzzy_match_equipment(self, text: str, equipment: str) -> Tuple[bool, float]:\n",
    "        if equipment.lower() in text.lower():\n",
    "            return True, 100\n",
    "        \n",
    "        words = re.findall(r'\\b[\\w\\s-]+\\b', text)\n",
    "        max_score = 0\n",
    "        \n",
    "        for word_group in words:\n",
    "            score = fuzz.partial_ratio(equipment.lower(), word_group.lower())\n",
    "            max_score = max(max_score, score)\n",
    "            \n",
    "            if score >= self.fuzzy_match_threshold:\n",
    "                return True, score\n",
    "                \n",
    "        return False, max_score\n",
    "\n",
    "    def extract_page_content(self, soup: BeautifulSoup) -> Dict[str, str]:\n",
    "        content = defaultdict(str)\n",
    "        \n",
    "        # Headers and Titles\n",
    "        headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'title'])\n",
    "        content['headers'] = ' '.join(h.get_text(strip=True) for h in headers)\n",
    "        \n",
    "        # Navigation menus\n",
    "        nav_elements = soup.find_all(['nav', 'menu'])\n",
    "        nav_elements.extend(soup.find_all(class_=lambda x: x and ('nav' in x or 'menu' in x)))\n",
    "        content['navigation'] = ' '.join(n.get_text(strip=True) for n in nav_elements)\n",
    "        \n",
    "        # Lists and Tables\n",
    "        lists = soup.find_all(['ul', 'ol', 'dl', 'table'])\n",
    "        content['lists'] = ' '.join(l.get_text(strip=True) for l in lists)\n",
    "        \n",
    "        # Buttons, Links, and Interactive Elements\n",
    "        interactive = soup.find_all(['button', 'a', 'input', 'select'])\n",
    "        content['interactive'] = ' '.join(i.get_text(strip=True) if isinstance(i, Tag) else '' for i in interactive)\n",
    "        \n",
    "        # Equipment sections\n",
    "        equipment_sections = soup.find_all(\n",
    "            ['div', 'section', 'article'],\n",
    "            class_=lambda x: x and any(term in x.lower() for term in self.equipment_categories)\n",
    "        )\n",
    "        content['equipment_sections'] = ' '.join(e.get_text(strip=True) for e in equipment_sections)\n",
    "        \n",
    "        # Downloads\n",
    "        downloads = soup.find_all(\n",
    "            lambda tag: tag.name == 'a' and (\n",
    "                'download' in tag.get('href', '').lower() or\n",
    "                'download' in tag.get_text().lower() or\n",
    "                '.pdf' in tag.get('href', '').lower() or\n",
    "                '.doc' in tag.get('href', '').lower() or\n",
    "                'equipment' in tag.get('href', '').lower()\n",
    "            )\n",
    "        )\n",
    "        content['downloads'] = ' '.join(d.get('href', '') + ' ' + d.get_text(strip=True) for d in downloads)\n",
    "        \n",
    "        return content\n",
    "\n",
    "    def find_relevant_subpages(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:\n",
    "        relevant_pages = []\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            absolute_url = urljoin(base_url, href)\n",
    "            \n",
    "            if (absolute_url in self.visited_urls or\n",
    "                urlparse(absolute_url).netloc != urlparse(base_url).netloc):\n",
    "                continue\n",
    "                \n",
    "            url_score = 0\n",
    "            url_lower = absolute_url.lower()\n",
    "            \n",
    "            for term in self.equipment_categories:\n",
    "                if term in url_lower or term in link.get_text().lower():\n",
    "                    url_score += 1\n",
    "                    \n",
    "            if url_score > 0:\n",
    "                relevant_pages.append({\n",
    "                    'url': absolute_url,\n",
    "                    'initial_score': url_score,\n",
    "                    'link_text': link.get_text()\n",
    "                })\n",
    "                \n",
    "        return sorted(relevant_pages, key=lambda x: x['initial_score'], reverse=True)\n",
    "\n",
    "    @profile\n",
    "    def scrape_page(self, url: str) -> Dict:\n",
    "        \"\"\"Enhanced scrape_page method with unique equipment matching.\"\"\"\n",
    "        if url in self.visited_urls:\n",
    "            logging.info(f\"Skipping already visited page: {url}\")\n",
    "            return None\n",
    "            \n",
    "        self.visited_urls.add(url)\n",
    "        time.sleep(self.rate_limit_delay)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            page_content = self.extract_page_content(soup)\n",
    "            full_content = ' '.join(page_content.values())\n",
    "            \n",
    "            # Track unique matches using equipment name as key\n",
    "            unique_matches = {}\n",
    "            \n",
    "            for category, equipment_list in self.target_equipment.items():\n",
    "                for equipment, variants in equipment_list.items():\n",
    "                    # Check main equipment name and variants\n",
    "                    all_names = [equipment] + variants\n",
    "                    best_match = None\n",
    "                    best_score = 0\n",
    "                    \n",
    "                    for name in all_names:\n",
    "                        matched, score = self.fuzzy_match_equipment(full_content, name)\n",
    "                        if matched and score > best_score:\n",
    "                            best_score = score\n",
    "                            best_match = {\n",
    "                                'equipment': equipment,\n",
    "                                'match_score': score,\n",
    "                                'context': self.get_match_context(full_content, name)\n",
    "                            }\n",
    "                    \n",
    "                    if best_match and best_score > self.fuzzy_match_threshold:\n",
    "                        unique_matches[equipment] = best_match\n",
    "            \n",
    "            # Organize unique matches by category\n",
    "            matches_by_category = defaultdict(list)\n",
    "            for match in unique_matches.values():\n",
    "                for category, equipment_list in self.target_equipment.items():\n",
    "                    if match['equipment'] in equipment_list:\n",
    "                        matches_by_category[category].append(match)\n",
    "            \n",
    "            url_rating = self.calculate_url_rating(url, soup, list(unique_matches.values()), full_content)\n",
    "            \n",
    "            logging.info(f\"Successfully scraped page: {url} (Score: {url_rating['score']})\")\n",
    "            return {\n",
    "                'url': url,\n",
    "                'url_rating': url_rating,\n",
    "                'matches': dict(matches_by_category),\n",
    "                'content_types': page_content\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def get_match_context(self, text: str, equipment: str, context_chars: int = 100) -> str:\n",
    "        text_lower = text.lower()\n",
    "        equip_lower = equipment.lower()\n",
    "        \n",
    "        index = text_lower.find(equip_lower)\n",
    "        if index == -1:\n",
    "            return \"\"\n",
    "            \n",
    "        start = max(0, index - context_chars)\n",
    "        end = min(len(text), index + len(equipment) + context_chars)\n",
    "        \n",
    "        return f\"...{text[start:end]}...\"\n",
    "\n",
    "    \n",
    "    def scrape_website(self, studio_name: str, website_url: str) -> Dict:\n",
    "        \"\"\"Scrape entire website for equipment information.\"\"\"\n",
    "        logging.info(f\"\\nProcessing studio: {studio_name}\")\n",
    "        logging.info(f\"Website: {website_url}\")\n",
    "        \n",
    "        result = {\n",
    "            'Studio Name': studio_name,\n",
    "            'Website': website_url,\n",
    "            'Status': 'Inactive',\n",
    "            'Error': '',\n",
    "            'Pages': [],\n",
    "            'Equipment_URLs': '',\n",
    "            'Equipment_Grade': '',\n",
    "            **{category: '' for category in self.target_equipment.keys()}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Initial page scan\n",
    "            initial_page = self.scrape_page(website_url)\n",
    "            if not initial_page:\n",
    "                raise Exception(\"Failed to scan initial page\")\n",
    "                \n",
    "            result['Status'] = 'Active'\n",
    "            result['Pages'].append(initial_page)\n",
    "            \n",
    "            # Find and scan relevant subpages\n",
    "            response = requests.get(website_url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            relevant_subpages = self.find_relevant_subpages(soup, website_url)\n",
    "            \n",
    "            logging.info(f\"Found {len(relevant_subpages)} relevant subpages\")\n",
    "            \n",
    "            # Use ThreadPoolExecutor for parallel scanning of subpages\n",
    "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "                future_to_url = {\n",
    "                    executor.submit(self.scrape_page, page['url']): page['url']\n",
    "                    for page in relevant_subpages[:5]\n",
    "                }\n",
    "                \n",
    "                for future in future_to_url:\n",
    "                    page_result = future.result()\n",
    "                    if page_result:\n",
    "                        result['Pages'].append(page_result)\n",
    "            \n",
    "            # Aggregate matches across all pages\n",
    "            all_matches = defaultdict(list)\n",
    "            equipment_urls_with_scores = []\n",
    "            \n",
    "            for page in result['Pages']:\n",
    "                has_matches = False\n",
    "                for category, matches in page['matches'].items():\n",
    "                    if matches:\n",
    "                        has_matches = True\n",
    "                        all_matches[category].extend(matches)\n",
    "                \n",
    "                if has_matches:\n",
    "                    equipment_urls_with_scores.append(\n",
    "                        (page['url'], page['url_rating']['score'])\n",
    "                    )\n",
    "            \n",
    "            # Calculate equipment grade\n",
    "            grade_info = self.calculate_equipment_grade(all_matches)\n",
    "            \n",
    "            # Format equipment grade string\n",
    "            grade_details = (\n",
    "                f\"Grade: {grade_info['grade']} \"\n",
    "                f\"(Total: {grade_info['total_score']}/150 pts - \"\n",
    "                f\"Mic: {grade_info['category_scores'].get('Microphones', 0)}, \"\n",
    "                f\"Comp: {grade_info['category_scores'].get('Compressors', 0)}, \"\n",
    "                f\"Pre: {grade_info['category_scores'].get('Preamps', 0)}, \"\n",
    "                f\"EQ: {grade_info['category_scores'].get('Equalisation', 0)}, \"\n",
    "                f\"Mon: {grade_info['category_scores'].get('Monitors', 0)})\"\n",
    "            )\n",
    "            \n",
    "            result['Equipment_Grade'] = grade_details\n",
    "            \n",
    "            # Format final results\n",
    "            for category in self.target_equipment.keys():\n",
    "                if category in all_matches:\n",
    "                    # Ensure unique matches by equipment name\n",
    "                    unique_matches = {\n",
    "                        match['equipment']: match \n",
    "                        for match in all_matches[category]\n",
    "                    }.values()\n",
    "                    \n",
    "                    result[category] = '; '.join(\n",
    "                        f\"{m['equipment']} (score: {m['match_score']})\" \n",
    "                        for m in sorted(unique_matches, key=lambda x: x['match_score'], reverse=True)\n",
    "                    )\n",
    "            \n",
    "            # Add equipment URLs with scores\n",
    "            equipment_urls_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            result['Equipment_URLs'] = '; '.join(\n",
    "                f\"{url} (score: {score:.2f})\"\n",
    "                for url, score in equipment_urls_with_scores\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            result['Status'] = 'Error'\n",
    "            result['Error'] = str(e)\n",
    "            logging.error(f\"Error processing {studio_name}: {str(e)}\")\n",
    "            \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def process_studio_list(self, studios: List[Dict[str, str]], output_file: str):\n",
    "        \"\"\"Process list of studios and save results.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for idx, studio in enumerate(studios, 1):\n",
    "            logging.info(f\"\\nProcessing studio {idx}/{len(studios)}\")\n",
    "            result = self.scrape_website(studio['studio_name'], studio['website'])\n",
    "            results.append(result)\n",
    "            \n",
    "            if len(results) % 5 == 0:\n",
    "                self.save_results(results, output_file)\n",
    "                logging.info(f\"Progress saved after {len(results)} studios\")\n",
    "        \n",
    "        self.save_results(results, output_file)\n",
    "        logging.info(f\"\\nProcessing complete. Processed {len(results)} studios.\")\n",
    "\n",
    "    \n",
    "    def save_results(self, results: List[Dict], output_file: str):\n",
    "        \"\"\"Enhanced save_results method with equipment grading.\"\"\"\n",
    "        # Save main results\n",
    "        main_df = pd.DataFrame([\n",
    "            {k: v for k, v in r.items() if k != 'Pages'}\n",
    "            for r in results\n",
    "        ])\n",
    "        \n",
    "        # Reorder columns to include Equipment_Grade after Equipment_URLs\n",
    "        columns = ['Studio Name', 'Website', 'Equipment_URLs', 'Equipment_Grade', 'Status', 'Error'] + \\\n",
    "                 [col for col in main_df.columns if col not in [\n",
    "                     'Studio Name', 'Website', 'Equipment_URLs', 'Equipment_Grade', 'Status', 'Error'\n",
    "                 ]]\n",
    "        main_df = main_df[columns]\n",
    "        \n",
    "        main_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Save detailed page-level results\n",
    "        detailed_results = []\n",
    "        for result in results:\n",
    "            for page in result.get('Pages', []):\n",
    "                detailed_results.append({\n",
    "                    'Studio Name': result['Studio Name'],\n",
    "                    'Page URL': page['url'],\n",
    "                    'Overall Score': page['url_rating']['score'],\n",
    "                    'Equipment Match Score': page['url_rating']['component_scores']['equipment_match'],\n",
    "                    'Category Match Score': page['url_rating']['component_scores']['category_match'],\n",
    "                    'URL Structure Score': page['url_rating']['component_scores']['url_structure'],\n",
    "                    'Page Structure Score': page['url_rating']['component_scores']['page_structure'],\n",
    "                    'Context Quality Score': page['url_rating']['component_scores']['context_quality'],\n",
    "                    'Content Types': str(page['content_types']),\n",
    "                    'Matches': str(page['matches'])\n",
    "                })\n",
    "                \n",
    "        if detailed_results:\n",
    "            detailed_file = output_file.replace('.csv', '_detailed.csv')\n",
    "            pd.DataFrame(detailed_results).to_csv(detailed_file, index=False)\n",
    "            logging.info(f\"Results saved to {output_file} and detailed results to {detailed_file}\")\n",
    "        else:\n",
    "            logging.info(f\"Results saved to {output_file}\")\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(cls, csv_file: str) -> 'StudioScraper':\n",
    "        \"\"\"Create a StudioScraper instance and load studios from CSV.\"\"\"\n",
    "        logging.info(f\"\\nLoading studios from {csv_file}\")\n",
    "        scraper = cls()\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            required_columns = {'studio_name', 'website'}\n",
    "            if not all(col in df.columns for col in required_columns):\n",
    "                raise ValueError(f\"CSV must contain columns: {required_columns}\")\n",
    "            \n",
    "            df['website'] = df['website'].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "            df = df.dropna(subset=['website'])\n",
    "            \n",
    "            scraper.studios = df.to_dict('records')\n",
    "            logging.info(f\"Successfully loaded {len(scraper.studios)} studios\")\n",
    "            \n",
    "            return scraper\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading CSV file {csv_file}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def run(self, output_file: str = 'data/studio_equip_lists/studio_equipment_results_ii.csv'):\n",
    "        \"\"\"Run the scraper on all loaded studios.\"\"\"\n",
    "        if not hasattr(self, 'studios'):\n",
    "            raise ValueError(\"No studios loaded. Use from_csv() to load studios first.\")\n",
    "            \n",
    "        self.process_studio_list(self.studios, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7fd54f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-03 12:09:34,125 - INFO - \n",
      "Loading studios from studio_websites_ii.csv\n",
      "2025-02-03 12:09:34,163 - INFO - Successfully loaded 51 studios\n",
      "2025-02-03 12:09:34,177 - INFO - \n",
      "Processing studio 1/51\n",
      "2025-02-03 12:09:34,177 - INFO - \n",
      "Processing studio: Audio Sorcery\n",
      "2025-02-03 12:09:34,178 - INFO - Website: https://audiosorcery.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:09:37,477 - ERROR - Error scraping https://audiosorcery.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:09:37,480 - ERROR - Error processing Audio Sorcery: Failed to scan initial page\n",
      "2025-02-03 12:09:37,480 - INFO - \n",
      "Processing studio 2/51\n",
      "2025-02-03 12:09:37,481 - INFO - \n",
      "Processing studio: Audio-Vision\n",
      "2025-02-03 12:09:37,482 - INFO - Website: https://audiovisionstudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:09:41,154 - ERROR - Error scraping https://audiovisionstudios.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:09:41,157 - ERROR - Error processing Audio-Vision: Failed to scan initial page\n",
      "2025-02-03 12:09:41,158 - INFO - \n",
      "Processing studio 3/51\n",
      "2025-02-03 12:09:41,158 - INFO - \n",
      "Processing studio: Big Scary Tree\n",
      "2025-02-03 12:09:41,160 - INFO - Website: https://bigscarytree.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:09:44,847 - ERROR - Error scraping https://bigscarytree.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:09:44,849 - ERROR - Error processing Big Scary Tree: Failed to scan initial page\n",
      "2025-02-03 12:09:44,849 - INFO - \n",
      "Processing studio 4/51\n",
      "2025-02-03 12:09:44,850 - INFO - \n",
      "Processing studio: Bridger Productions\n",
      "2025-02-03 12:09:44,850 - INFO - Website: https://www.bridgerproductions.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:09:48,543 - ERROR - Error scraping https://www.bridgerproductions.com/: 403 Client Error: Forbidden for url: https://www.bridgerproductions.com/\n",
      "2025-02-03 12:09:48,548 - ERROR - Error processing Bridger Productions: Failed to scan initial page\n",
      "2025-02-03 12:09:48,548 - INFO - \n",
      "Processing studio 5/51\n",
      "2025-02-03 12:09:48,549 - INFO - \n",
      "Processing studio: Clear Lake Recording Studios\n",
      "2025-02-03 12:09:48,550 - INFO - Website: https://www.clearlakerecordingstudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:09:51,775 - ERROR - Error scraping https://www.clearlakerecordingstudios.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:09:51,777 - ERROR - Error processing Clear Lake Recording Studios: Failed to scan initial page\n",
      "2025-02-03 12:09:51,786 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv\n",
      "2025-02-03 12:09:51,787 - INFO - Progress saved after 5 studios\n",
      "2025-02-03 12:09:51,787 - INFO - \n",
      "Processing studio 6/51\n",
      "2025-02-03 12:09:51,788 - INFO - \n",
      "Processing studio: Dark Horse Recording\n",
      "2025-02-03 12:09:51,789 - INFO - Website: https://darkhorserecording.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:09:55,461 - ERROR - Error scraping https://darkhorserecording.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:09:55,462 - ERROR - Error processing Dark Horse Recording: Failed to scan initial page\n",
      "2025-02-03 12:09:55,462 - INFO - \n",
      "Processing studio 7/51\n",
      "2025-02-03 12:09:55,463 - INFO - \n",
      "Processing studio: Downtown Music Studios\n",
      "2025-02-03 12:09:55,463 - INFO - Website: https://downtownmusic.com/services/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:09:58,695 - ERROR - Error scraping https://downtownmusic.com/services/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:09:58,697 - ERROR - Error processing Downtown Music Studios: Failed to scan initial page\n",
      "2025-02-03 12:09:58,698 - INFO - \n",
      "Processing studio 8/51\n",
      "2025-02-03 12:09:58,699 - INFO - \n",
      "Processing studio: Fonoprint\n",
      "2025-02-03 12:09:58,699 - INFO - Website: https://www.fonoprint.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:01,960 - ERROR - Error scraping https://www.fonoprint.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:10:01,963 - ERROR - Error processing Fonoprint: Failed to scan initial page\n",
      "2025-02-03 12:10:01,964 - INFO - \n",
      "Processing studio 9/51\n",
      "2025-02-03 12:10:01,964 - INFO - \n",
      "Processing studio: Geejam Studios\n",
      "2025-02-03 12:10:01,965 - INFO - Website: https://geejamstudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:05,499 - ERROR - Error scraping https://geejamstudios.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:10:05,501 - ERROR - Error processing Geejam Studios: Failed to scan initial page\n",
      "2025-02-03 12:10:05,502 - INFO - \n",
      "Processing studio 10/51\n",
      "2025-02-03 12:10:05,503 - INFO - \n",
      "Processing studio: Glasgow Recording Studio\n",
      "2025-02-03 12:10:05,503 - INFO - Website: https://glasgowmusicstudios.co.uk/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:08,770 - ERROR - Error scraping https://glasgowmusicstudios.co.uk/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:10:08,772 - ERROR - Error processing Glasgow Recording Studio: Failed to scan initial page\n",
      "2025-02-03 12:10:08,790 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv\n",
      "2025-02-03 12:10:08,791 - INFO - Progress saved after 10 studios\n",
      "2025-02-03 12:10:08,792 - INFO - \n",
      "Processing studio 11/51\n",
      "2025-02-03 12:10:08,792 - INFO - \n",
      "Processing studio: Magik Studios\n",
      "2025-02-03 12:10:08,792 - INFO - Website: https://themagikstudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:12,424 - ERROR - Error scraping https://themagikstudios.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:10:12,427 - ERROR - Error processing Magik Studios: Failed to scan initial page\n",
      "2025-02-03 12:10:12,428 - INFO - \n",
      "Processing studio 12/51\n",
      "2025-02-03 12:10:12,429 - INFO - \n",
      "Processing studio: Manifold Recording\n",
      "2025-02-03 12:10:12,431 - INFO - Website: https://manifoldrecording.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:16,060 - ERROR - Error scraping https://manifoldrecording.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:10:16,061 - ERROR - Error processing Manifold Recording: Failed to scan initial page\n",
      "2025-02-03 12:10:16,062 - INFO - \n",
      "Processing studio 13/51\n",
      "2025-02-03 12:10:16,065 - INFO - \n",
      "Processing studio: Matinée Multilingual\n",
      "2025-02-03 12:10:16,066 - INFO - Website: https://matinee.co.uk/recording-studios-in-reading/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:19,344 - ERROR - Error scraping https://matinee.co.uk/recording-studios-in-reading/: 502 Server Error: Bad Gateway for url: https://matinee.co.uk/recording-studios-in-reading/\n",
      "2025-02-03 12:10:19,348 - ERROR - Error processing Matinée Multilingual: Failed to scan initial page\n",
      "2025-02-03 12:10:19,349 - INFO - \n",
      "Processing studio 14/51\n",
      "2025-02-03 12:10:19,349 - INFO - \n",
      "Processing studio: Mission Control Studios\n",
      "2025-02-03 12:10:19,350 - INFO - Website: https://www.mission-control-studios.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:22,997 - ERROR - Error scraping https://www.mission-control-studios.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:10:23,001 - ERROR - Error processing Mission Control Studios: Failed to scan initial page\n",
      "2025-02-03 12:10:23,001 - INFO - \n",
      "Processing studio 15/51\n",
      "2025-02-03 12:10:23,002 - INFO - \n",
      "Processing studio: Morplay Studios\n",
      "2025-02-03 12:10:23,002 - INFO - Website: https://morplaystudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:26,631 - ERROR - Error scraping https://morplaystudios.com/: 520 Server Error:  for url: https://morplaystudios.com/\n",
      "2025-02-03 12:10:26,635 - ERROR - Error processing Morplay Studios: Failed to scan initial page\n",
      "2025-02-03 12:10:26,643 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv\n",
      "2025-02-03 12:10:26,643 - INFO - Progress saved after 15 studios\n",
      "2025-02-03 12:10:26,644 - INFO - \n",
      "Processing studio 16/51\n",
      "2025-02-03 12:10:26,644 - INFO - \n",
      "Processing studio: Noisebox Studios\n",
      "2025-02-03 12:10:26,645 - INFO - Website: https://noiseboxstudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:30,412 - ERROR - Error scraping https://noiseboxstudios.com/: 520 Server Error:  for url: https://noiseboxstudios.com/\n",
      "2025-02-03 12:10:30,418 - ERROR - Error processing Noisebox Studios: Failed to scan initial page\n",
      "2025-02-03 12:10:30,419 - INFO - \n",
      "Processing studio 17/51\n",
      "2025-02-03 12:10:30,419 - INFO - \n",
      "Processing studio: Oak Recording Studios\n",
      "2025-02-03 12:10:30,420 - INFO - Website: https://www.oakrecordingstudio.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:34,354 - INFO - Successfully scraped page: https://www.oakrecordingstudio.com/ (Score: 55.29)\n",
      "2025-02-03 12:10:34,943 - INFO - Found 9 relevant subpages\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.pyERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:34,950 - INFO - Skipping already visited page: https://www.oakrecordingstudio.com/rates.html\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:38,876 - INFO - Successfully scraped page: https://www.oakrecordingstudio.com/rates.html (Score: 11.76)\n",
      "2025-02-03 12:10:38,907 - INFO - Successfully scraped page: https://www.oakrecordingstudio.com/mix.html (Score: 11.76)\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:38,940 - INFO - Successfully scraped page: https://www.oakrecordingstudio.com/studios.html (Score: 11.76)\n",
      "2025-02-03 12:10:42,592 - INFO - Successfully scraped page: https://www.oakrecordingstudio.com/denim.html (Score: 7.06)\n",
      "2025-02-03 12:10:42,594 - INFO - \n",
      "Processing studio 18/51\n",
      "2025-02-03 12:10:42,595 - INFO - \n",
      "Processing studio: Omega Studios\n",
      "2025-02-03 12:10:42,595 - INFO - Website: https://omegastudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:46,124 - ERROR - Error scraping https://omegastudios.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:10:46,127 - ERROR - Error processing Omega Studios: Failed to scan initial page\n",
      "2025-02-03 12:10:46,128 - INFO - \n",
      "Processing studio 19/51\n",
      "2025-02-03 12:10:46,129 - INFO - \n",
      "Processing studio: Paragon Studios\n",
      "2025-02-03 12:10:46,129 - INFO - Website: https://paragon-studios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:49,751 - ERROR - Error scraping https://paragon-studios.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:10:49,753 - ERROR - Error processing Paragon Studios: Failed to scan initial page\n",
      "2025-02-03 12:10:49,754 - INFO - \n",
      "Processing studio 20/51\n",
      "2025-02-03 12:10:49,754 - INFO - \n",
      "Processing studio: Parhelion Recording Studios\n",
      "2025-02-03 12:10:49,755 - INFO - Website: https://www.parhelionrecordingstudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:53,420 - ERROR - Error scraping https://www.parhelionrecordingstudios.com/: 403 Client Error: Forbidden for url: https://www.parhelionrecordingstudios.com/\n",
      "2025-02-03 12:10:53,423 - ERROR - Error processing Parhelion Recording Studios: Failed to scan initial page\n",
      "2025-02-03 12:10:53,429 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv and detailed results to studio_equip_lists/equipment_results_ii_detailed.csv\n",
      "2025-02-03 12:10:53,430 - INFO - Progress saved after 20 studios\n",
      "2025-02-03 12:10:53,430 - INFO - \n",
      "Processing studio 21/51\n",
      "2025-02-03 12:10:53,430 - INFO - \n",
      "Processing studio: Pilot Recording\n",
      "2025-02-03 12:10:53,431 - INFO - Website: https://pilotrecording.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:10:57,068 - ERROR - Error scraping https://pilotrecording.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:10:57,070 - ERROR - Error processing Pilot Recording: Failed to scan initial page\n",
      "2025-02-03 12:10:57,071 - INFO - \n",
      "Processing studio 22/51\n",
      "2025-02-03 12:10:57,071 - INFO - \n",
      "Processing studio: Pyram-Axis Productions\n",
      "2025-02-03 12:10:57,072 - INFO - Website: https://pyramaxis.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:00,729 - ERROR - Error scraping https://pyramaxis.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:00,731 - ERROR - Error processing Pyram-Axis Productions: Failed to scan initial page\n",
      "2025-02-03 12:11:00,731 - INFO - \n",
      "Processing studio 23/51\n",
      "2025-02-03 12:11:00,732 - INFO - \n",
      "Processing studio: Railway Studios\n",
      "2025-02-03 12:11:00,732 - INFO - Website: https://railwaystudios.com.mt/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:03,950 - ERROR - Error scraping https://railwaystudios.com.mt/: 520 Server Error:  for url: https://railwaystudios.com.mt/\n",
      "2025-02-03 12:11:03,956 - ERROR - Error processing Railway Studios: Failed to scan initial page\n",
      "2025-02-03 12:11:03,958 - INFO - \n",
      "Processing studio 24/51\n",
      "2025-02-03 12:11:03,958 - INFO - \n",
      "Processing studio: Rak Studio 1\n",
      "2025-02-03 12:11:03,958 - INFO - Website: https://rakstudios.co.uk/studios/studio-1\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:07,188 - ERROR - Error scraping https://rakstudios.co.uk/studios/studio-1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:07,190 - ERROR - Error processing Rak Studio 1: Failed to scan initial page\n",
      "2025-02-03 12:11:07,191 - INFO - \n",
      "Processing studio 25/51\n",
      "2025-02-03 12:11:07,192 - INFO - \n",
      "Processing studio: Rak Studio 2\n",
      "2025-02-03 12:11:07,192 - INFO - Website: https://rakstudios.co.uk/studios/studio-2\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:10,368 - ERROR - Error scraping https://rakstudios.co.uk/studios/studio-2: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:10,371 - ERROR - Error processing Rak Studio 2: Failed to scan initial page\n",
      "2025-02-03 12:11:10,382 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv and detailed results to studio_equip_lists/equipment_results_ii_detailed.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-03 12:11:10,382 - INFO - Progress saved after 25 studios\n",
      "2025-02-03 12:11:10,383 - INFO - \n",
      "Processing studio 26/51\n",
      "2025-02-03 12:11:10,383 - INFO - \n",
      "Processing studio: Rak Studio 3\n",
      "2025-02-03 12:11:10,384 - INFO - Website: https://rakstudios.co.uk/studios/studio-3\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:13,591 - ERROR - Error scraping https://rakstudios.co.uk/studios/studio-3: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:13,595 - ERROR - Error processing Rak Studio 3: Failed to scan initial page\n",
      "2025-02-03 12:11:13,596 - INFO - \n",
      "Processing studio 27/51\n",
      "2025-02-03 12:11:13,597 - INFO - \n",
      "Processing studio: Rak Studio 4\n",
      "2025-02-03 12:11:13,599 - INFO - Website: https://rakstudios.co.uk/studios/studio-4\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:16,786 - ERROR - Error scraping https://rakstudios.co.uk/studios/studio-4: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:16,788 - ERROR - Error processing Rak Studio 4: Failed to scan initial page\n",
      "2025-02-03 12:11:16,789 - INFO - \n",
      "Processing studio 28/51\n",
      "2025-02-03 12:11:16,790 - INFO - \n",
      "Processing studio: Record Plant\n",
      "2025-02-03 12:11:16,790 - INFO - Website: https://recordplant.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:20,540 - ERROR - Error scraping https://recordplant.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:20,542 - ERROR - Error processing Record Plant: Failed to scan initial page\n",
      "2025-02-03 12:11:20,543 - INFO - \n",
      "Processing studio 29/51\n",
      "2025-02-03 12:11:20,543 - INFO - \n",
      "Processing studio: Record Plant, Los Angeles\n",
      "2025-02-03 12:11:20,544 - INFO - Website: https://recordplant.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:20,545 - INFO - Skipping already visited page: https://recordplant.com/\n",
      "2025-02-03 12:11:20,546 - ERROR - Error processing Record Plant, Los Angeles: Failed to scan initial page\n",
      "2025-02-03 12:11:20,546 - INFO - \n",
      "Processing studio 30/51\n",
      "2025-02-03 12:11:20,547 - INFO - \n",
      "Processing studio: Red Bus Studios\n",
      "2025-02-03 12:11:20,547 - INFO - Website: https://redbusmusic.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:26,138 - INFO - Successfully scraped page: https://redbusmusic.com/ (Score: 39.29)\n",
      "2025-02-03 12:11:26,859 - INFO - Found 3 relevant subpages\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:30,862 - INFO - Successfully scraped page: https://redbusmusic.com/equipment/ (Score: 80.0)\n",
      "2025-02-03 12:11:31,180 - INFO - Successfully scraped page: https://redbusmusic.com/studio-one/ (Score: 57.65)\n",
      "2025-02-03 12:11:31,236 - INFO - Successfully scraped page: https://redbusmusic.com/studio-two/ (Score: 57.65)\n",
      "2025-02-03 12:11:31,242 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv and detailed results to studio_equip_lists/equipment_results_ii_detailed.csv\n",
      "2025-02-03 12:11:31,242 - INFO - Progress saved after 30 studios\n",
      "2025-02-03 12:11:31,242 - INFO - \n",
      "Processing studio 31/51\n",
      "2025-02-03 12:11:31,243 - INFO - \n",
      "Processing studio: Signature Sound\n",
      "2025-02-03 12:11:31,243 - INFO - Website: https://signaturesound.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:34,982 - ERROR - Error scraping https://signaturesound.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:34,985 - ERROR - Error processing Signature Sound: Failed to scan initial page\n",
      "2025-02-03 12:11:34,985 - INFO - \n",
      "Processing studio 32/51\n",
      "2025-02-03 12:11:34,986 - INFO - \n",
      "Processing studio: Sprout City Studios\n",
      "2025-02-03 12:11:34,987 - INFO - Website: https://www.sproutcity.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:38,617 - ERROR - Error scraping https://www.sproutcity.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:38,619 - ERROR - Error processing Sprout City Studios: Failed to scan initial page\n",
      "2025-02-03 12:11:38,620 - INFO - \n",
      "Processing studio 33/51\n",
      "2025-02-03 12:11:38,620 - INFO - \n",
      "Processing studio: Strongroom Studio 1\n",
      "2025-02-03 12:11:38,621 - INFO - Website: https://www.strongroom.com/studio/strongroom-1/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:41,848 - ERROR - Error scraping https://www.strongroom.com/studio/strongroom-1/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:41,851 - ERROR - Error processing Strongroom Studio 1: Failed to scan initial page\n",
      "2025-02-03 12:11:41,851 - INFO - \n",
      "Processing studio 34/51\n",
      "2025-02-03 12:11:41,852 - INFO - \n",
      "Processing studio: Strongroom Studio 2\n",
      "2025-02-03 12:11:41,852 - INFO - Website: https://www.strongroom.com/studio/strongroom-2/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:45,021 - ERROR - Error scraping https://www.strongroom.com/studio/strongroom-2/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:45,023 - ERROR - Error processing Strongroom Studio 2: Failed to scan initial page\n",
      "2025-02-03 12:11:45,024 - INFO - \n",
      "Processing studio 35/51\n",
      "2025-02-03 12:11:45,025 - INFO - \n",
      "Processing studio: Strongroom Studio 3\n",
      "2025-02-03 12:11:45,025 - INFO - Website: https://www.strongroom.com/studio/strongroom-3/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:48,210 - ERROR - Error scraping https://www.strongroom.com/studio/strongroom-3/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:48,213 - ERROR - Error processing Strongroom Studio 3: Failed to scan initial page\n",
      "2025-02-03 12:11:48,219 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv and detailed results to studio_equip_lists/equipment_results_ii_detailed.csv\n",
      "2025-02-03 12:11:48,220 - INFO - Progress saved after 35 studios\n",
      "2025-02-03 12:11:48,220 - INFO - \n",
      "Processing studio 36/51\n",
      "2025-02-03 12:11:48,221 - INFO - \n",
      "Processing studio: Strongroom Studio 4\n",
      "2025-02-03 12:11:48,221 - INFO - Website: https://www.strongroom.com/studio/strongroom-4/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:51,396 - ERROR - Error scraping https://www.strongroom.com/studio/strongroom-4/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:51,398 - ERROR - Error processing Strongroom Studio 4: Failed to scan initial page\n",
      "2025-02-03 12:11:51,398 - INFO - \n",
      "Processing studio 37/51\n",
      "2025-02-03 12:11:51,399 - INFO - \n",
      "Processing studio: Studio 11\n",
      "2025-02-03 12:11:51,399 - INFO - Website: https://www.studio11chicago.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:54,592 - ERROR - Error scraping https://www.studio11chicago.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:54,594 - ERROR - Error processing Studio 11: Failed to scan initial page\n",
      "2025-02-03 12:11:54,595 - INFO - \n",
      "Processing studio 38/51\n",
      "2025-02-03 12:11:54,595 - INFO - \n",
      "Processing studio: Sunset Sound\n",
      "2025-02-03 12:11:54,596 - INFO - Website: https://www.sunsetsound.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:11:58,215 - ERROR - Error scraping https://www.sunsetsound.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:11:58,217 - ERROR - Error processing Sunset Sound: Failed to scan initial page\n",
      "2025-02-03 12:11:58,218 - INFO - \n",
      "Processing studio 39/51\n",
      "2025-02-03 12:11:58,219 - INFO - \n",
      "Processing studio: Tape To Tape\n",
      "2025-02-03 12:11:58,219 - INFO - Website: https://tapelondonstudio.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:01,480 - ERROR - Error scraping https://tapelondonstudio.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:12:01,483 - ERROR - Error processing Tape To Tape: Failed to scan initial page\n",
      "2025-02-03 12:12:01,484 - INFO - \n",
      "Processing studio 40/51\n",
      "2025-02-03 12:12:01,485 - INFO - \n",
      "Processing studio: The Blue Room Recording\n",
      "2025-02-03 12:12:01,485 - INFO - Website: https://www.bluerecorders.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:07,226 - INFO - Successfully scraped page: https://www.bluerecorders.com/ (Score: 56.47)\n",
      "2025-02-03 12:12:07,439 - INFO - Found 11 relevant subpages\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:17,525 - INFO - Successfully scraped page: https://www.bluerecorders.com/s/Blue-South-B-Room-Gear-List.pdf (Score: 15.29)\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:17,645 - INFO - Successfully scraped page: https://www.bluerecorders.com/s/Blue-South-A-Room-Gear-List.pdf (Score: 15.29)\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:20,283 - ERROR - Error scraping https://www.bluerecorders.com/see-the-studio: Exceeded 30 redirects.\n",
      "2025-02-03 12:12:25,172 - INFO - Successfully scraped page: https://www.bluerecorders.com/#studioa (Score: 60.0)\n",
      "2025-02-03 12:12:25,242 - INFO - Successfully scraped page: https://www.bluerecorders.com/#studiob (Score: 60.0)\n",
      "2025-02-03 12:12:25,249 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv and detailed results to studio_equip_lists/equipment_results_ii_detailed.csv\n",
      "2025-02-03 12:12:25,249 - INFO - Progress saved after 40 studios\n",
      "2025-02-03 12:12:25,249 - INFO - \n",
      "Processing studio 41/51\n",
      "2025-02-03 12:12:25,249 - INFO - \n",
      "Processing studio: The Castle Recording Studios\n",
      "2025-02-03 12:12:25,250 - INFO - Website: https://www.castlerecordingstudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:29,093 - INFO - Successfully scraped page: https://www.castlerecordingstudios.com/ (Score: 51.53)\n",
      "2025-02-03 12:12:29,253 - INFO - Found 28 relevant subpages\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:33,307 - INFO - Successfully scraped page: https://www.castlerecordingstudios.com/cart (Score: 40.47)\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:33,727 - INFO - Successfully scraped page: https://www.castlerecordingstudios.com/studio-a (Score: 70.59)\n",
      "2025-02-03 12:12:33,782 - INFO - Successfully scraped page: https://www.castlerecordingstudios.com/#page (Score: 51.53)\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:37,224 - INFO - Successfully scraped page: https://www.castlerecordingstudios.com/studio-b (Score: 70.59)\n",
      "2025-02-03 12:12:37,631 - INFO - Successfully scraped page: https://www.castlerecordingstudios.com/mic-locker (Score: 63.53)\n",
      "2025-02-03 12:12:37,633 - INFO - \n",
      "Processing studio 42/51\n",
      "2025-02-03 12:12:37,634 - INFO - \n",
      "Processing studio: The Cave, Nashville, TN\n",
      "2025-02-03 12:12:37,634 - INFO - Website: https://www.thecavestudios.net/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:42,634 - INFO - Successfully scraped page: https://www.thecavestudios.net/ (Score: 46.82)\n",
      "2025-02-03 12:12:43,368 - INFO - Found 10 relevant subpages\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.pyERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:48,195 - INFO - Successfully scraped page: https://www.thecavestudios.net/studio/ (Score: 27.06)\n",
      "2025-02-03 12:12:48,214 - INFO - Successfully scraped page: https://www.thecavestudios.net/about/ (Score: 14.12)\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:49,039 - INFO - Successfully scraped page: https://www.thecavestudios.net (Score: 46.82)\n",
      "2025-02-03 12:12:52,566 - INFO - Successfully scraped page: https://www.thecavestudios.net/services/ (Score: 11.76)\n",
      "2025-02-03 12:12:52,575 - INFO - Successfully scraped page: https://www.thecavestudios.net/staff/ (Score: 11.76)\n",
      "2025-02-03 12:12:52,579 - INFO - \n",
      "Processing studio 43/51\n",
      "2025-02-03 12:12:52,579 - INFO - \n",
      "Processing studio: The Church, London\n",
      "2025-02-03 12:12:52,579 - INFO - Website: https://thechurchstudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:55,866 - ERROR - Error scraping https://thechurchstudios.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:12:55,868 - ERROR - Error processing The Church, London: Failed to scan initial page\n",
      "2025-02-03 12:12:55,869 - INFO - \n",
      "Processing studio 44/51\n",
      "2025-02-03 12:12:55,870 - INFO - \n",
      "Processing studio: Theta Sound Studio\n",
      "2025-02-03 12:12:55,870 - INFO - Website: https://www.thetasound.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:12:59,094 - ERROR - Error scraping https://www.thetasound.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:12:59,097 - ERROR - Error processing Theta Sound Studio: Failed to scan initial page\n",
      "2025-02-03 12:12:59,097 - INFO - \n",
      "Processing studio 45/51\n",
      "2025-02-03 12:12:59,098 - INFO - \n",
      "Processing studio: Tileyard Studios\n",
      "2025-02-03 12:12:59,099 - INFO - Website: https://tyxstudios.com/london/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:02,370 - ERROR - Error scraping https://tyxstudios.com/london/: 520 Server Error:  for url: https://tyxstudios.com/london/\n",
      "2025-02-03 12:13:02,374 - ERROR - Error processing Tileyard Studios: Failed to scan initial page\n",
      "2025-02-03 12:13:02,384 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv and detailed results to studio_equip_lists/equipment_results_ii_detailed.csv\n",
      "2025-02-03 12:13:02,384 - INFO - Progress saved after 45 studios\n",
      "2025-02-03 12:13:02,385 - INFO - \n",
      "Processing studio 46/51\n",
      "2025-02-03 12:13:02,385 - INFO - \n",
      "Processing studio: Tweed Recording\n",
      "2025-02-03 12:13:02,385 - INFO - Website: https://tweedrecording.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-03 12:13:06,099 - ERROR - Error scraping https://tweedrecording.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:13:06,102 - ERROR - Error processing Tweed Recording: Failed to scan initial page\n",
      "2025-02-03 12:13:06,103 - INFO - \n",
      "Processing studio 47/51\n",
      "2025-02-03 12:13:06,104 - INFO - \n",
      "Processing studio: Village (Recorder), The\n",
      "2025-02-03 12:13:06,104 - INFO - Website: https://www.villagestudios.com/studios\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:11,482 - INFO - Successfully scraped page: https://www.villagestudios.com/studios (Score: 28.0)\n",
      "2025-02-03 12:13:11,679 - INFO - Found 19 relevant subpages\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:11,680 - INFO - Skipping already visited page: https://www.villagestudios.com/gear\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:17,187 - INFO - Successfully scraped page: https://www.villagestudios.com/gear (Score: 82.35)\n",
      "2025-02-03 12:13:17,210 - INFO - Successfully scraped page: https://www.villagestudios.com/studio-bhistory (Score: 19.29)\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:17,255 - INFO - Skipping already visited page: https://www.villagestudios.com/gear\n",
      "2025-02-03 12:13:17,529 - INFO - Successfully scraped page: https://www.villagestudios.com/studio-ahistory (Score: 17.65)\n",
      "2025-02-03 12:13:17,531 - INFO - \n",
      "Processing studio 48/51\n",
      "2025-02-03 12:13:17,531 - INFO - \n",
      "Processing studio: Whitehouse Studios\n",
      "2025-02-03 12:13:17,532 - INFO - Website: https://www.whitehouse-studios.co.uk/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:21,315 - INFO - Successfully scraped page: https://www.whitehouse-studios.co.uk/ (Score: 42.12)\n",
      "2025-02-03 12:13:21,496 - INFO - Found 12 relevant subpages\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:21,497 - INFO - Skipping already visited page: https://www.whitehouse-studios.co.uk/recording\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:25,881 - INFO - Successfully scraped page: https://www.whitehouse-studios.co.uk (Score: 42.12)\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:26,309 - INFO - Successfully scraped page: https://www.whitehouse-studios.co.uk/rehearsals (Score: 48.0)\n",
      "2025-02-03 12:13:26,351 - INFO - Successfully scraped page: https://www.whitehouse-studios.co.uk/recording (Score: 56.0)\n",
      "2025-02-03 12:13:30,482 - INFO - Successfully scraped page: https://www.whitehouse-studios.co.uk/vocal-sessions (Score: 46.82)\n",
      "2025-02-03 12:13:30,485 - INFO - \n",
      "Processing studio 49/51\n",
      "2025-02-03 12:13:30,485 - INFO - \n",
      "Processing studio: Yellow Dog Studios\n",
      "2025-02-03 12:13:30,486 - INFO - Website: https://www.yellowdogstudios.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:34,076 - INFO - Successfully scraped page: https://www.yellowdogstudios.com/ (Score: 11.76)\n",
      "2025-02-03 12:13:34,279 - INFO - Found 10 relevant subpages\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:37,954 - INFO - Successfully scraped page: https://www.yellowdogstudios.com/copy-of-studio (Score: 19.76)\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:38,325 - INFO - Successfully scraped page: https://www.yellowdogstudios.com (Score: 11.76)\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:38,626 - INFO - Successfully scraped page: https://www.yellowdogstudios.com/studo (Score: 19.76)\n",
      "2025-02-03 12:13:41,543 - INFO - Successfully scraped page: https://www.yellowdogstudios.com/copy-of-studio-b (Score: 19.76)\n",
      "2025-02-03 12:13:41,998 - INFO - Successfully scraped page: https://www.yellowdogstudios.com/copy-2-of-studio-c (Score: 19.76)\n",
      "2025-02-03 12:13:42,000 - INFO - \n",
      "Processing studio 50/51\n",
      "2025-02-03 12:13:42,000 - INFO - \n",
      "Processing studio: Young Turks Studio\n",
      "2025-02-03 12:13:42,001 - INFO - Website: https://www.young-space.co.uk/thestudios\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:46,297 - INFO - Successfully scraped page: https://www.young-space.co.uk/thestudios (Score: 12.94)\n",
      "2025-02-03 12:13:46,461 - INFO - Found 4 relevant subpages\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:46,462 - INFO - Skipping already visited page: https://www.young-space.co.uk/thestudio\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:46,463 - INFO - Skipping already visited page: https://www.young-space.co.uk/thestudio\n",
      "2025-02-03 12:13:49,828 - ERROR - Error scraping https://www.young-space.co.uk/thestudio: 404 Client Error: Not Found for url: https://www.young-space.co.uk/thestudio\n",
      "2025-02-03 12:13:50,729 - INFO - Successfully scraped page: https://www.young-space.co.uk/thestudios#page (Score: 12.94)\n",
      "2025-02-03 12:13:50,739 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv and detailed results to studio_equip_lists/equipment_results_ii_detailed.csv\n",
      "2025-02-03 12:13:50,739 - INFO - Progress saved after 50 studios\n",
      "2025-02-03 12:13:50,739 - INFO - \n",
      "Processing studio 51/51\n",
      "2025-02-03 12:13:50,739 - INFO - \n",
      "Processing studio: Zurichflorida\n",
      "2025-02-03 12:13:50,739 - INFO - Website: https://zurichflorida.com/\n",
      "ERROR: Could not find file /var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_51672/755900924.py\n",
      "2025-02-03 12:13:54,311 - INFO - Successfully scraped page: https://zurichflorida.com/ (Score: 9.41)\n",
      "2025-02-03 12:13:54,574 - INFO - Found 0 relevant subpages\n",
      "2025-02-03 12:13:54,589 - INFO - Results saved to studio_equip_lists/equipment_results_ii.csv and detailed results to studio_equip_lists/equipment_results_ii_detailed.csv\n",
      "2025-02-03 12:13:54,589 - INFO - \n",
      "Processing complete. Processed 51 studios.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Create scraper instance from CSV\n",
    "        scraper = StudioScraper.from_csv('data/studio_websites_ii.csv')\n",
    "        \n",
    "        # Run the scraper\n",
    "        scraper.run('data/studio_equip_lists/equipment_results_ii.csv')\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Script execution failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249fe820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
