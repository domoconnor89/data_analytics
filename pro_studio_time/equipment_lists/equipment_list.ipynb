{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb59e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "import sys\n",
    "import gc\n",
    "from thefuzz import fuzz\n",
    "from thefuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b8544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquipmentMatcher:\n",
    "    def __init__(self):\n",
    "        self.equipment_categories = {\n",
    "            'Microphones 0': {\n",
    "                'Telefunken ELA M 251': ['ELAM 251', 'ELA M251', 'ELA-M 251', 'Telefunken 251', 'ELA M 251E'],\n",
    "                'Neumann U47': ['U-47', 'U 47', 'VF14 U47', 'U47 Vintage', 'Neumann VF14'],\n",
    "                'Neumann U67': ['U-67', 'U 67', 'U.67', 'U67 Vintage', 'Neumann 67'],\n",
    "                'AKG C12': ['C-12', 'C 12', 'C.12', 'AKG C12VR', 'C12 Vintage']\n",
    "            },\n",
    "            'Microphones 1': {\n",
    "                'Neumann U87': ['U-87', 'U 87', 'U87ai', 'U87 Ai', 'U87A', 'U87 Vintage'],\n",
    "                'Neumann M49': ['M-49', 'M 49', 'M.49', 'M49 Vintage', 'Neumann M 49'],\n",
    "                'Telefunken U47': ['Telefunken U-47', 'Tele U47', 'Telefunken U 47'],\n",
    "                'AKG C414': ['C-414', 'C414 XLII', 'C414 XLS', 'C414B-ULS', 'C414 EB'],\n",
    "                'Coles 4038': ['Coles 4038S', '4038 Ribbon', 'Coles Ribbon'],\n",
    "                'Schoeps CMC6': ['CMC 6', 'CMC-6', 'Schoeps CMC6 MK', 'CMC6/MK'],\n",
    "                'Royer R-121': ['R121', 'R 121', 'Royer 121', 'R-121 Ribbon']\n",
    "            },\n",
    "            'Microphones 2': {\n",
    "                'AKG C414 EB': ['C414EB', 'C-414 EB', 'C414 EB Silver', 'C414 EB Vintage'],\n",
    "                'Neumann KM84': ['KM-84', 'KM 84', 'KM84i', 'KM 84 Vintage'],\n",
    "                'Sennheiser MD421': ['MD-421', 'MD 421 II', 'MD421-U'],\n",
    "                'Shure SM7B': ['SM7 B', 'SM-7B', 'SM7b', 'SM7'],\n",
    "                'Electro-Voice RE20': ['RE-20', 'RE 20', 'EV RE20', 'RE20 Broadcast'],\n",
    "                'Neumann TLM103': ['TLM-103', 'TLM 103', 'TLM.103']\n",
    "            },\n",
    "            'Microphones 3': {\n",
    "                'Shure SM57': ['SM-57', 'SM 57', 'SM57-LC'],\n",
    "                'Shure SM58': ['SM-58', 'SM 58', 'SM58-LC']\n",
    "            },\n",
    "            'Compressors 0': {\n",
    "                'Fairchild 670': ['Fairchild 670 Stereo', '670 Limiter', 'Fairchild 670/660'],\n",
    "                'Neve 2254': ['2254/E', '2254E', '2254A', 'Neve 2254E'],\n",
    "                'Teletronix LA2A': ['LA-2A', 'LA 2A', 'Teletronix LA-2A', 'LA2A Leveler']\n",
    "            },\n",
    "            'Compressors 1': {\n",
    "                'Universal Audio 1176LN': ['1176 LN', 'UA 1176', 'UREI 1176', '1176LN Rev'],\n",
    "                'Empirical Labs Distressor': ['EL8 Distressor', 'EL-8', 'EL8-X'],\n",
    "                'Tube-Tech CL1B': ['CL 1B', 'CL-1B', 'Tube Tech CL1B'],\n",
    "                'SSL G Bus Compressor': ['SSL G-Series', 'SSL G Comp', 'G Series Comp']\n",
    "            },\n",
    "            'Compressors 2': {\n",
    "                'DBX 160VU': ['DBX 160', '160 VU', 'DBX VU Compressor'],\n",
    "                'API 2500': ['API-2500', '2500 Comp', '2500 Compressor'],\n",
    "                'Thermionic Culture Phoenix': ['Phoenix Compressor', 'Thermionic Phoenix', 'Phoenix Tube Compressor'],\n",
    "                'Manley Vari-Mu': ['Vari Mu', 'VariMu', 'Manley Tube Compressor']\n",
    "            },\n",
    "            'Compressors 3': {\n",
    "                'Focusrite Red': ['Red Compressor', 'Focusrite Red Series', 'Focusrite Red 3']\n",
    "            },\n",
    "            'Preamps 0': {\n",
    "                'Neve 1073': ['1073 Preamp', 'AMS Neve 1073', 'Neve Classic 1073'],\n",
    "                'API 312': ['API-312', '312 Preamp', '312 Mic Pre']\n",
    "            },\n",
    "            'Preamps 1': {\n",
    "                'Neve 1073': ['1073 Pre', 'Neve Pre 1073', 'AMS 1073'],\n",
    "                'API 3124+': ['3124+', 'API 3124 Plus', '3124 Mic Pre'],\n",
    "                'API 512c': ['512C', 'API-512', '512 Mic Preamp'],\n",
    "                'Neve 1084': ['1084 Preamp', 'Neve 1084 EQ/Pre', 'AMS Neve 1084'],\n",
    "                'SSL SuperAnalogue': ['SuperAnalogue Preamp', 'SSL Super Analogue', 'SSL SA Pre']\n",
    "            },\n",
    "            'Preamps 2': {\n",
    "                'Universal Audio 610': ['UA 610', '610 Tube Preamp', 'Universal 610'],\n",
    "                'Focusrite ISA 110': ['ISA 110', 'Focusrite 110 Pre', 'Focusrite ISA'],\n",
    "                'Shadow Hills Gama': ['Gama Preamp', 'Shadow Hills Mic Pre', 'Shadow Hills G.A.M.A.'],\n",
    "                'Chandler TG2': ['TG2 Preamp', 'Chandler TG-2', 'Abbey Road TG2']\n",
    "            },\n",
    "            'Preamps 3': {\n",
    "                'Focusrite Scarlett': ['Scarlett Preamp', 'Focusrite Scarlett Series'],\n",
    "                'Presonus Studio 24C': ['24C Preamp', 'Studio 24C']\n",
    "            },\n",
    "            'Equalisation 0': {\n",
    "                'Pultec EQP-1A': ['EQP 1A', 'Pultec 1A EQ', 'Pultec Tube EQ'],\n",
    "                'Manley Massive Passive': ['Massive Passive EQ', 'Manley MP EQ']\n",
    "            },\n",
    "            'Equalisation 1': {\n",
    "                'API 550A': ['550A EQ', 'API 550 EQ'],\n",
    "                'Neve 1073': ['1073 EQ', 'Neve EQ 1073'],\n",
    "                'GML 8200': ['8200 EQ', 'GML Parametric EQ'],\n",
    "                'SSL 4000': ['SSL EQ 4000', '4000 Series EQ', 'SSL 4K EQ']\n",
    "            },\n",
    "            'Equalisation 2': {\n",
    "                'API 550B': ['550B EQ', 'API 550 EQ-B'],\n",
    "                'Focusrite ISA 110': ['ISA 110 EQ', 'Focusrite 110 EQ'],\n",
    "                'Chandler Curve Bender': ['Curve Bender', 'Chandler CB EQ']\n",
    "            },\n",
    "            'Equalisation 3': {\n",
    "                'Behringer graphic': ['Behringer EQ', 'Behringer Graphic EQ']\n",
    "            },\n",
    "            'Monitors 0': {\n",
    "                'Barefoot Sound MM27': ['MM27 Monitors', 'Barefoot MM27', 'Barefoot Sound'],\n",
    "                'ATC SCM150ASL Pro': ['SCM150 ASL', 'ATC SCM150', 'SCM150 Pro']\n",
    "            },\n",
    "            'Monitors 1': {\n",
    "                'ATC SCM25A': ['SCM25A', 'ATC SCM25'],\n",
    "                'Focal Twin 6 Be': ['Twin 6 Be', 'Focal Twin6', 'Focal Twin'],\n",
    "                'Genelec 8351A': ['8351A Monitors', 'Genelec 8351', 'The Ones 8351'],\n",
    "                'PMC BB5/XBD': ['BB5 XBD', 'PMC BB5 Monitors', 'PMC XBD'],\n",
    "                'Quested HQ210': ['HQ210 Monitors', 'Quested HQ-210', 'Quested Studio Monitors']\n",
    "            },\n",
    "            'Monitors 2': {\n",
    "                'Focal SM9': ['SM9 Monitors', 'Focal SM-9', 'Focal 3-Way Monitors'],\n",
    "                'Genelec 1031A': ['1031A Monitors', 'Genelec 1031'],\n",
    "                'Yamaha NS10M': ['NS-10M', 'NS10 Monitors', 'Yamaha NS10'],\n",
    "                'ADAM Audio S3H': ['S3H Monitors', 'Adam S3H', 'Adam Audio']\n",
    "            },\n",
    "            'Monitors 3': {\n",
    "                'Yamaha HS8': ['HS8 Monitors', 'Yamaha HS-8'],\n",
    "                'KRK Rokit': ['Rokit Monitors', 'KRK Rokit Series'],\n",
    "                'Avantone Mixcubes': ['Mixcubes', 'Avantone Cubes']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Common manufacturer variations\n",
    "        self.manufacturer_variations = {\n",
    "            'Neumann': ['Neum', 'Neuman', 'Neumaan', 'Numann'],\n",
    "            'Telefunken': ['Telefunken Elektroakustik', 'Telef', 'Tele', 'Telefungen'],\n",
    "            'AKG': ['AKG Acoustics', 'AKG-', 'AGK'],\n",
    "            'Shure': ['Sure', 'Schure', 'Shr'],\n",
    "            'Universal Audio': ['UA', 'UREI', 'Uaudio'],\n",
    "            'Neve': ['Nieve', 'Nev', 'AMS Neve'],\n",
    "            'SSL': ['Solid State Logic', 'SSl', 'S.S.L.']\n",
    "        }\n",
    "\n",
    "    def fuzzy_match(self, text: str, equipment: str, threshold: int = 85) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Perform fuzzy matching on text with various preprocessing steps.\n",
    "        \n",
    "        Args:\n",
    "            text: Source text to search in\n",
    "            equipment: Equipment name to search for\n",
    "            threshold: Minimum similarity score (0-100)\n",
    "            \n",
    "        Returns:\n",
    "            Dict with match details if found, None otherwise\n",
    "        \"\"\"\n",
    "        # Generate variations of the equipment name\n",
    "        equipment_variations = self.generate_variations(equipment)\n",
    "        \n",
    "        # Clean and prepare text for matching\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Split text into chunks for better matching\n",
    "        chunks = self.split_into_chunks(cleaned_text)\n",
    "        \n",
    "        best_match = None\n",
    "        highest_score = 0\n",
    "        match_context = ''\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            for variant in equipment_variations:\n",
    "                # Try different fuzzy matching algorithms\n",
    "                ratio = fuzz.ratio(chunk.lower(), variant.lower())\n",
    "                partial_ratio = fuzz.partial_ratio(chunk.lower(), variant.lower())\n",
    "                token_sort_ratio = fuzz.token_sort_ratio(chunk.lower(), variant.lower())\n",
    "                \n",
    "                # Use the highest score among different algorithms\n",
    "                score = max(ratio, partial_ratio, token_sort_ratio)\n",
    "                \n",
    "                if score > highest_score and score >= threshold:\n",
    "                    highest_score = score\n",
    "                    best_match = variant\n",
    "                    match_context = chunk\n",
    "        \n",
    "        if best_match:\n",
    "            return {\n",
    "                'matched_text': best_match,\n",
    "                'confidence': highest_score,\n",
    "                'context': match_context,\n",
    "                'original_term': equipment\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def generate_variations(self, equipment: str) -> List[str]:\n",
    "        \"\"\"Generate various possible representations of equipment names.\"\"\"\n",
    "        variations = [equipment]\n",
    "        parts = equipment.split()\n",
    "        \n",
    "        # Handle manufacturer variations\n",
    "        if parts[0] in self.manufacturer_variations:\n",
    "            for variant in self.manufacturer_variations[parts[0]]:\n",
    "                variations.append(f\"{variant} {' '.join(parts[1:])}\")\n",
    "        \n",
    "        # Handle model number variations\n",
    "        if len(parts) > 1:\n",
    "            model = parts[-1]\n",
    "            # Add variations with different separators\n",
    "            variations.extend([\n",
    "                f\"{' '.join(parts[:-1])}-{model}\",\n",
    "                f\"{' '.join(parts[:-1])} {model}\",\n",
    "                f\"{' '.join(parts[:-1])}.{model}\"\n",
    "            ])\n",
    "        \n",
    "        return variations\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text for better matching.\"\"\"\n",
    "        # Remove special characters but keep spaces\n",
    "        text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "        # Normalize whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "    def split_into_chunks(self, text: str, chunk_size: int = 100) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks for better matching.\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size // 2):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def find_equipment_matches(self, text: str) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Find all equipment matches in text using fuzzy matching.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for category, equipment_dict in self.equipment_categories.items():\n",
    "            category_matches = []\n",
    "            \n",
    "            for equipment, variations in equipment_dict.items():\n",
    "                # Try exact matches first\n",
    "                exact_match = any(variation.lower() in text.lower() for variation in variations)\n",
    "                \n",
    "                if exact_match:\n",
    "                    category_matches.append({\n",
    "                        'equipment': equipment,\n",
    "                        'confidence': 100,\n",
    "                        'match_type': 'exact'\n",
    "                    })\n",
    "                else:\n",
    "                    # Try fuzzy matching\n",
    "                    fuzzy_match = self.fuzzy_match(text, equipment)\n",
    "                    if fuzzy_match:\n",
    "                        category_matches.append({\n",
    "                            'equipment': equipment,\n",
    "                            'confidence': fuzzy_match['confidence'],\n",
    "                            'match_type': 'fuzzy',\n",
    "                            'context': fuzzy_match['context']\n",
    "                        })\n",
    "            \n",
    "            if category_matches:\n",
    "                results[category] = category_matches\n",
    "        \n",
    "        return results\n",
    "\n",
    "class StudioScraper:\n",
    "    def __init__(self, csv_path: str, batch_size: int = 50):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with enhanced logging and equipment matcher.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to CSV containing studio information\n",
    "            batch_size: Number of studios to process in each batch\n",
    "        \"\"\"\n",
    "        # Setup logging\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # Load studios in batches\n",
    "        self.csv_path = csv_path\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "        }\n",
    "        \n",
    "        # Initialize equipment matcher\n",
    "        self.equipment_matcher = EquipmentMatcher()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure detailed logging system.\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(f'data/log/scraper_log_{timestamp}.log'),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def scrape_single_studio(self, row: pd.Series) -> Dict:\n",
    "        \"\"\"\n",
    "        Scrape equipment information for a single studio.\n",
    "\n",
    "        Args:\n",
    "            row: Pandas Series containing studio information\n",
    "\n",
    "        Returns:\n",
    "            Dict containing all scraped information and status\n",
    "        \"\"\"\n",
    "        url = row['website']  # Note: capitalization matters\n",
    "        studio_name = row['studio_name']  # Note: capitalization matters\n",
    "\n",
    "        self.logger.info(f\"Processing studio: {studio_name}\")\n",
    "\n",
    "        # Initialize result dictionary with basic info\n",
    "        result = {\n",
    "            'studio_name': studio_name,\n",
    "            'website': url,\n",
    "            'status': 'Unknown',\n",
    "            'error_message': None,\n",
    "            'equipment_page_url': None\n",
    "        }\n",
    "\n",
    "        # Add empty columns for each equipment category\n",
    "        for category in self.equipment_matcher.equipment_categories.keys():\n",
    "            result[category] = ''\n",
    "\n",
    "        # Check website status\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            result['status'] = 'Active'\n",
    "\n",
    "            # Get all internal links\n",
    "            all_links = self.get_all_links(url)\n",
    "\n",
    "            # Threshold for link relevance score\n",
    "            threshold = 0.4\n",
    "\n",
    "            all_equipment_matches = {}\n",
    "            for link in all_links:\n",
    "                try:\n",
    "                    page_response = requests.get(link, headers=self.headers, timeout=10)\n",
    "                    page_text = page_response.text\n",
    "\n",
    "                    # Score link relevance\n",
    "                    link_score = self.score_link_relevance(link)\n",
    "\n",
    "                    # Search for equipment matches\n",
    "                    equipment_matches = self.equipment_matcher.find_equipment_matches(page_text)\n",
    "\n",
    "                    # Add page URL to matches if found\n",
    "                    if equipment_matches and link_score > threshold:\n",
    "                        for category, matches in equipment_matches.items():\n",
    "                            if category not in all_equipment_matches:\n",
    "                                all_equipment_matches[category] = []\n",
    "\n",
    "                            for match in matches:\n",
    "                                match_details = match.copy()\n",
    "                                match_details['page_url'] = link\n",
    "                                all_equipment_matches[category].append(match_details)\n",
    "\n",
    "                    time.sleep(2)  # Rate limiting\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing page {link}: {str(e)}\")\n",
    "\n",
    "            # Update result with comprehensive matches\n",
    "            for category, matches in all_equipment_matches.items():\n",
    "                match_texts = []\n",
    "                for match in matches:\n",
    "                    match_text = f\"{match['equipment']} ({match['confidence']}% {match['match_type']}) - Page: {match['page_url']}\"\n",
    "                    match_texts.append(match_text)\n",
    "\n",
    "                result[category] = ' | '.join(match_texts)\n",
    "\n",
    "            # Update equipment page URL if matches found\n",
    "            if all_equipment_matches:\n",
    "                result['equipment_page_url'] = ', '.join(set(match['page_url'] for matches in all_equipment_matches.values() for match in matches))\n",
    "\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            result['status'] = 'Inactive'\n",
    "            result['error_message'] = 'Connection Error'\n",
    "        except requests.exceptions.Timeout:\n",
    "            result['status'] = 'Inactive'\n",
    "            result['error_message'] = 'Timeout Error'\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            result['status'] = 'Inactive'\n",
    "            result['error_message'] = f'HTTP Error: {str(e)}'\n",
    "        except Exception as e:\n",
    "            result['status'] = 'Error'\n",
    "            result['error_message'] = f'Unknown Error: {str(e)}'\n",
    "\n",
    "        return result\n",
    "\n",
    "    def score_link_relevance(self, link: str) -> float:\n",
    "        \"\"\"Score link relevance for equipment pages\"\"\"\n",
    "        relevance_keywords = ['gear', 'equipment', 'studio', 'tech', 'specs', 'desk',\n",
    "                              'microphone', 'microphones', 'recording', 'facilities',\n",
    "                              'console', 'outboard','instruments', 'monitoring', 'monitors']\n",
    "        score = 0\n",
    "\n",
    "        # Keyword presence\n",
    "        for keyword in relevance_keywords:\n",
    "            if keyword in link.lower():\n",
    "                score += 0.3\n",
    "\n",
    "        # Path depth (prefer shorter paths)\n",
    "        path_segments = urlparse(link).path.split('/')\n",
    "        score -= 0.1 * len(path_segments)\n",
    "\n",
    "        return max(0, min(1, score))\n",
    "    \n",
    "    def get_all_links(self, url: str) -> List[str]:\n",
    "        \"\"\"Get all internal links from a webpage with rate limiting.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            base_domain = urlparse(url).netloc\n",
    "            \n",
    "            links = []\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                link = urljoin(url, a['href'])\n",
    "                if urlparse(link).netloc == base_domain:\n",
    "                    links.append(link)\n",
    "            \n",
    "            time.sleep(2)  # Rate limiting\n",
    "            return list(set(links))\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting links from {url}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def process_batch(self, batch_df: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"Process a batch of studios with parallel execution.\"\"\"\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:  # Limited workers for rate limiting\n",
    "            future_to_studio = {\n",
    "                executor.submit(self.scrape_single_studio, row): row['studio_name']\n",
    "                for _, row in batch_df.iterrows()\n",
    "            }\n",
    "            \n",
    "            for future in future_to_studio:\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Batch processing error: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def scrape_all_studios(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape all studios in batches to manage memory usage.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with all results\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        # Read CSV in chunks\n",
    "        for chunk_num, chunk in enumerate(pd.read_csv(self.csv_path, chunksize=self.batch_size)):\n",
    "            self.logger.info(f\"Processing batch {chunk_num + 1}\")\n",
    "            \n",
    "            results = self.process_batch(chunk)\n",
    "            all_results.extend(results)\n",
    "            \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "            # Save intermediate results every 100 studios\n",
    "            if len(all_results) % 100 == 0:\n",
    "                self.save_intermediate_results(all_results)\n",
    "        \n",
    "        return pd.DataFrame(all_results)\n",
    "\n",
    "    def save_intermediate_results(self, results: List[Dict]):\n",
    "        \"\"\"Save intermediate results to prevent data loss.\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        pd.DataFrame(results).to_csv(f'data/log/intermediate_results_{timestamp}.csv', index=False)\n",
    "\n",
    "    def save_results(self, results_df: pd.DataFrame, output_path: str):\n",
    "        \"\"\"Save the final results to a CSV file.\"\"\"\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        self.logger.info(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a88b1187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-03 12:24:41,641 - INFO - Processing batch 1\n",
      "2025-02-03 12:24:41,646 - INFO - Processing studio: Audio Sorcery\n",
      "2025-02-03 12:24:41,648 - INFO - Processing studio: Audio-Vision\n",
      "2025-02-03 12:24:41,648 - INFO - Processing studio: Big Scary Tree\n",
      "2025-02-03 12:25:33,814 - INFO - Processing studio: Bridger Productions\n",
      "2025-02-03 12:25:47,799 - INFO - Processing studio: Clear Lake Recording Studios\n",
      "2025-02-03 12:27:21,524 - INFO - Processing studio: Dark Horse Recording\n",
      "2025-02-03 12:28:07,046 - INFO - Processing studio: Downtown Music Studios\n",
      "2025-02-03 12:28:20,081 - INFO - Processing studio: Fonoprint\n",
      "2025-02-03 12:28:50,590 - INFO - Processing studio: Geejam Studios\n",
      "2025-02-03 12:29:09,272 - INFO - Processing studio: Glasgow Recording Studio\n",
      "2025-02-03 12:30:44,202 - INFO - Processing studio: Magik Studios\n",
      "2025-02-03 12:33:15,280 - ERROR - Error processing page https://themagikstudios.com/houston-vocal-recording-services/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "2025-02-03 12:33:15,888 - INFO - Processing studio: Manifold Recording\n",
      "2025-02-03 12:33:36,853 - INFO - Processing studio: Matinée Multilingual\n",
      "2025-02-03 12:34:19,155 - INFO - Processing studio: Mission Control Studios\n",
      "2025-02-03 12:35:09,844 - INFO - Processing studio: Morplay Studios\n",
      "2025-02-03 12:35:24,231 - INFO - Processing studio: Noisebox Studios\n",
      "2025-02-03 12:36:55,205 - INFO - Processing studio: Oak Recording Studios\n",
      "2025-02-03 12:37:28,769 - INFO - Processing studio: Omega Studios\n",
      "2025-02-03 12:40:02,209 - INFO - Processing studio: Paragon Studios\n",
      "2025-02-03 12:41:51,174 - INFO - Processing studio: Parhelion Recording Studios\n",
      "2025-02-03 12:44:29,583 - INFO - Processing batch 2\n",
      "2025-02-03 12:44:29,585 - INFO - Processing studio: Pilot Recording\n",
      "2025-02-03 12:44:29,585 - INFO - Processing studio: Pyram-Axis Productions\n",
      "2025-02-03 12:44:29,587 - INFO - Processing studio: Railway Studios\n",
      "2025-02-03 12:44:59,233 - INFO - Processing studio: Rak Studio 1\n",
      "2025-02-03 12:45:03,841 - INFO - Processing studio: Rak Studio 2\n",
      "2025-02-03 12:45:38,217 - INFO - Processing studio: Rak Studio 3\n",
      "2025-02-03 12:56:32,017 - INFO - Processing studio: Rak Studio 4\n",
      "2025-02-03 12:59:16,999 - INFO - Processing studio: Record Plant\n",
      "2025-02-03 12:59:51,435 - INFO - Processing studio: Record Plant, Los Angeles\n",
      "2025-02-03 13:00:45,528 - INFO - Processing studio: Red Bus Studios\n",
      "2025-02-03 13:01:41,863 - INFO - Processing studio: Signature Sound\n",
      "2025-02-03 13:04:58,657 - INFO - Processing studio: Sprout City Studios\n",
      "2025-02-03 13:12:29,492 - ERROR - Error processing page https://recordplant.com/wp-content/uploads/2015/05/81LFx7SM8AL._SL1400_.jpg: HTTPSConnectionPool(host='recordplant.com', port=443): Read timed out. (read timeout=10)\n",
      "2025-02-03 13:12:31,753 - ERROR - Error processing page https://www.sproutcity.com/category/events/: HTTPSConnectionPool(host='www.sproutcity.com', port=443): Read timed out. (read timeout=10)\n",
      "2025-02-03 13:12:48,962 - INFO - Processing studio: Strongroom Studio 1\n",
      "2025-02-03 13:14:59,480 - INFO - Processing studio: Strongroom Studio 2\n",
      "2025-02-03 13:15:39,396 - INFO - Processing studio: Strongroom Studio 3\n",
      "2025-02-03 13:24:35,975 - INFO - Processing studio: Strongroom Studio 4\n",
      "2025-02-03 13:26:02,848 - INFO - Processing studio: Studio 11\n",
      "2025-02-03 13:27:35,045 - INFO - Processing studio: Sunset Sound\n",
      "2025-02-03 13:27:45,258 - INFO - Processing studio: Tape To Tape\n",
      "2025-02-03 13:28:47,318 - INFO - Processing studio: The Blue Room Recording\n",
      "2025-02-03 13:31:37,649 - ERROR - Error processing page https://www.bluerecorders.com/see-the-studio: Exceeded 30 redirects.\n",
      "2025-02-03 13:34:10,347 - ERROR - Error processing page https://www.bluerecorders.com/about: Exceeded 30 redirects.\n",
      "2025-02-03 13:36:43,294 - INFO - Processing batch 3\n",
      "2025-02-03 13:36:43,296 - INFO - Processing studio: The Castle Recording Studios\n",
      "2025-02-03 13:36:43,297 - INFO - Processing studio: The Cave, Nashville, TN\n",
      "2025-02-03 13:36:43,298 - INFO - Processing studio: The Church, London\n",
      "2025-02-03 13:37:41,710 - INFO - Processing studio: Theta Sound Studio\n",
      "2025-02-03 13:38:10,388 - INFO - Processing studio: Tileyard Studios\n",
      "2025-02-03 13:39:49,605 - INFO - Processing studio: Tweed Recording\n",
      "2025-02-03 13:40:27,868 - INFO - Processing studio: Village (Recorder), The\n",
      "2025-02-03 13:40:33,821 - INFO - Processing studio: Whitehouse Studios\n",
      "2025-02-03 13:45:43,050 - INFO - Processing studio: Yellow Dog Studios\n",
      "2025-02-03 13:47:54,163 - INFO - Processing studio: Young Turks Studio\n",
      "2025-02-03 13:48:28,878 - INFO - Processing studio: Zurichflorida\n",
      "2025-02-03 13:49:13,088 - INFO - Results saved to studio_equipment_results_ii.csv\n"
     ]
    }
   ],
   "source": [
    "scraper = StudioScraper('data/studio_websites_ii.csv', batch_size=20)\n",
    "results = scraper.scrape_all_studios()\n",
    "scraper.save_results(results, 'data/studio_equip_lists/studio_equipment_results_ii.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e228a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
