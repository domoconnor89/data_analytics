{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5543198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "class AllStudiosScraper:\n",
    "    BASE_URL = \"https://www.allstudios.co.uk\"\n",
    "    \n",
    "    def __init__(self, output_file='data/studios_data.csv'):\n",
    "        self.output_file = output_file\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def get_studio_list(self):\n",
    "        \"\"\"Retrieve list of studios from the main page\"\"\"\n",
    "        url = f\"{self.BASE_URL}/studios/recording-studio?clear=true\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            studios = []\n",
    "            for li in soup.select('ul.studios-list li'):\n",
    "                studio_link = li.find('a', href=lambda href: href and '/studios/recording-studio/' in href)\n",
    "                studio_website = li.find('a', class_='studio_index_button1')\n",
    "                \n",
    "                if studio_link and studio_website:\n",
    "                    studio_name = studio_link.text.strip()\n",
    "                    studio_page = self.BASE_URL + studio_link['href']\n",
    "                    website = studio_website.get('href', 'N/A')\n",
    "                    \n",
    "                    studios.append({\n",
    "                        'name': studio_name,\n",
    "                        'page': studio_page,\n",
    "                        'website': website\n",
    "                    })\n",
    "            \n",
    "            return studios\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching studio list: {e}\")\n",
    "            return []\n",
    "###    \n",
    "    def extract_location_from_map(self, soup):\n",
    "        \"\"\"Extract location information from Google Maps static map URL\"\"\"\n",
    "        location_info = {\n",
    "            'latitude': 'N/A',\n",
    "            'longitude': 'N/A',\n",
    "            'city': 'N/A',\n",
    "            'country': 'N/A'\n",
    "        }\n",
    "\n",
    "        # Find the Google Maps static map image\n",
    "        map_img = soup.find('img', src=re.compile(r'https://maps\\.googleapis\\.com/maps/api/staticmap'))\n",
    "\n",
    "        if map_img and map_img.get('src'):\n",
    "            map_url = map_img['src']\n",
    "\n",
    "            # Extract coordinates\n",
    "            coord_match = re.search(r'center=([^&]+)', map_url)\n",
    "            if coord_match:\n",
    "                try:\n",
    "                    latitude, longitude = coord_match.group(1).split(',')\n",
    "                    location_info['latitude'] = latitude.strip()\n",
    "                    location_info['longitude'] = longitude.strip()\n",
    "\n",
    "                except ValueError:\n",
    "                    print(\"Could not parse coordinates from map URL\")\n",
    "\n",
    "        return location_info\n",
    "###   \n",
    "    def extract_contact_info(self, soup):\n",
    "        \"\"\"Advanced contact information extraction\"\"\"\n",
    "        contact_info = {\n",
    "            'postal_address': 'N/A',\n",
    "            'contact_name': 'N/A',\n",
    "            'phone': 'N/A',\n",
    "            'email': 'N/A'\n",
    "        }\n",
    "\n",
    "        # Strategy 1: Look for specific table with studio-view class\n",
    "        studio_table = soup.find('table', class_='studio-view')\n",
    "        if studio_table:\n",
    "            # Try extracting from table rows\n",
    "            rows = studio_table.find_all('tr')\n",
    "            for row in rows:\n",
    "                row_text = row.get_text(strip=True)\n",
    "\n",
    "                # Extract contact name\n",
    "                if 'Contact Name:' in row_text:\n",
    "                    contact_info['contact_name'] = row_text.replace('Contact Name:', '').strip()\n",
    "\n",
    "                # Method 1: Simple string slicing for phone number\n",
    "                if row_text.startswith('Tel:'):\n",
    "                    contact_info['phone'] = row_text[5:].strip()\n",
    "\n",
    "                # Method 2: Regex extraction for phone number\n",
    "                phone_match = re.search(r'Tel:\\s*(.+)', row_text)\n",
    "                if phone_match and contact_info['phone'] == 'N/A':\n",
    "                    contact_info['phone'] = phone_match.group(1).strip()\n",
    "\n",
    "        # Strategy 2: Look for contact information in div elements (kept from original code)\n",
    "        contact_divs = soup.find_all(['div', 'p'], text=re.compile(r'(Contact Name|Tel:|Phone:|Telephone:)'))\n",
    "        for div in contact_divs:\n",
    "            div_text = div.get_text(strip=True)\n",
    "\n",
    "            # Extract contact name\n",
    "            name_match = re.search(r'Contact Name:\\s*(.+)', div_text, re.IGNORECASE)\n",
    "            if name_match:\n",
    "                contact_info['contact_name'] = name_match.group(1).strip()\n",
    "\n",
    "            # Extract phone number\n",
    "            phone_match = re.search(r'(Tel:|Phone:|Telephone:)\\s*(\\+?[\\d\\s\\(\\)-]+)', div_text, re.IGNORECASE)\n",
    "            if phone_match and contact_info['phone'] == 'N/A':\n",
    "                contact_info['phone'] = phone_match.group(2).strip()\n",
    "\n",
    "        # Strategy 3: Look for email link\n",
    "        email_link = soup.find('a', href=re.compile(r'^mailto:'))\n",
    "        if email_link:\n",
    "            contact_info['email'] = email_link['href'].replace('mailto:', '').strip()\n",
    "\n",
    "        # Strategy 4: Look for postal address\n",
    "        if studio_table:\n",
    "            address_row = studio_table.find('p')\n",
    "            if address_row:\n",
    "                contact_info['postal_address'] = address_row.get_text(strip=True)\n",
    "\n",
    "        return contact_info\n",
    "    \n",
    "    def get_studio_details(self, studio_page):\n",
    "        \"\"\"Retrieve detailed information for a specific studio\"\"\"\n",
    "        try:\n",
    "            response = requests.get(studio_page, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Use advanced extraction method\n",
    "            return self.extract_contact_info(soup)\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching studio details for {studio_page}: {e}\")\n",
    "            return {\n",
    "                'postal_address': 'N/A',\n",
    "                'contact_name': 'N/A',\n",
    "                'phone': 'N/A',\n",
    "                'email': 'N/A'\n",
    "            }\n",
    "    \n",
    "    def scrape_all_studios(self, max_studios=None):\n",
    "        \"\"\"Scrape details for studios\"\"\"\n",
    "        studios = self.get_studio_list()\n",
    "        \n",
    "        # Limit studios if max_studios is specified\n",
    "        if max_studios is not None:\n",
    "            studios = studios[:max_studios]\n",
    "        \n",
    "        # Prepare CSV file\n",
    "        with open(self.output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['Studio Name', 'Studio Website', 'Contact Name', 'Email', 'Phone', 'Postal Address']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            # Track progress\n",
    "            total_studios = len(studios)\n",
    "            print(f\"Total studios to scrape: {total_studios}\")\n",
    "            \n",
    "            for index, studio in enumerate(studios, 1):\n",
    "                print(f\"Scraping details for {studio['name']} (Studio {index} of {total_studios})...\")\n",
    "                \n",
    "                # Get additional details\n",
    "                details = self.get_studio_details(studio['page'])\n",
    "                \n",
    "                # Combine studio info with details\n",
    "                full_details = {\n",
    "                    'Studio Name': studio['name'],\n",
    "                    'Studio Website': studio['website'],\n",
    "                    'Contact Name': details.get('contact_name', 'N/A'),\n",
    "                    'Email': details.get('email', 'N/A'),\n",
    "                    'Phone': details.get('phone', 'N/A'),\n",
    "                    'Postal Address': details.get('postal_address', 'N/A')\n",
    "                }\n",
    "                \n",
    "                writer.writerow(full_details)\n",
    "                \n",
    "                # Random delay to be respectful of the website\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        print(f\"Scraping complete. Data saved to {self.output_file}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = AllStudiosScraper()\n",
    "    # Scrape first 10 studios for testing\n",
    "    scraper.scrape_all_studios(max_studios=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a8ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828be33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8384a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94f9615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total studios to scrape: 481\n",
      "Scraping details for Far Heath Studios (Studio 1 of 481)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_15660/3875694485.py:107: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  contact_divs = soup.find_all(['div', 'p'], text=re.compile(r'(Contact Name|Tel:|Phone:|Telephone:)'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping details for 123 Studios (Studio 2 of 481)...\n",
      "Scraping details for 3Sixty Studios (Studio 3 of 481)...\n",
      "Scraping details for 5A Studios (Studio 4 of 481)...\n",
      "Scraping details for 7 West Studios (Studio 5 of 481)...\n",
      "Scraping details for 77 Sound Studio (Studio 6 of 481)...\n",
      "Scraping details for 80 HERTZ Recording & Post Production Studios (Studio 7 of 481)...\n",
      "Scraping details for A Bloody Good Record Recording Studio (Studio 8 of 481)...\n",
      "Scraping details for A Sharp Recording Studios (Studio 9 of 481)...\n",
      "Scraping details for A-Tonal Recording Studio (Studio 10 of 481)...\n",
      "Scraping details for A.K.A Studio (Studio 11 of 481)...\n",
      "Scraping details for Abbey Music Studios (Studio 12 of 481)...\n",
      "Scraping details for Abbey Road Penthouse (Studio 13 of 481)...\n",
      "Scraping details for Abbey Road Studio 1 (Studio 14 of 481)...\n",
      "Scraping details for Abbey Road Studio 2 (Studio 15 of 481)...\n",
      "Scraping details for Abbey Road Studio 3 (Studio 16 of 481)...\n",
      "Scraping details for Absolute Music Studios (Studio 17 of 481)...\n",
      "Scraping details for Agafay Studio - Marrakech (Studio 18 of 481)...\n",
      "Scraping details for Air Lyndhurst Hall (Studio 19 of 481)...\n",
      "Scraping details for Air Studio 1 (Studio 20 of 481)...\n",
      "Scraping details for Air Studio 2 (Studio 21 of 481)...\n",
      "Scraping details for Air Studio 3 (Studio 22 of 481)...\n",
      "Scraping details for Air-Edel Studio 1 (Studio 23 of 481)...\n",
      "Scraping details for Air-Edel Studio 2 (Studio 24 of 481)...\n",
      "Scraping details for Airtight Studio 1 (Studio 25 of 481)...\n",
      "Scraping details for Airtight Studio 2 (Studio 26 of 481)...\n",
      "Scraping details for All Of Music Studio 1 (Studio 27 of 481)...\n",
      "Scraping details for Allstar Studio (Studio 28 of 481)...\n",
      "Scraping details for Ambiance Recording Studio (Studio 29 of 481)...\n",
      "Scraping details for Analogmix.com (Studio 30 of 481)...\n",
      "Scraping details for Analogue Baby (Studio 31 of 481)...\n",
      "Scraping details for Analogue Barn Studios (Studio 32 of 481)...\n",
      "Scraping details for Anton Road Studio (Studio 33 of 481)...\n",
      "Scraping details for April Media Productions (Studio 34 of 481)...\n",
      "Scraping details for Arclite Productions (Studio 35 of 481)...\n",
      "Scraping details for Arm Your Ears (online only) (Studio 36 of 481)...\n",
      "Scraping details for Artesuono Recording Studios (Studio 37 of 481)...\n",
      "Scraping details for Artillery Studios (Studio 38 of 481)...\n",
      "Scraping details for Ascape Recording Studio (Studio 39 of 481)...\n",
      "Scraping details for Astar Studios (Studio 40 of 481)...\n",
      "Scraping details for Attic Attack Studios (Studio 41 of 481)...\n",
      "Scraping details for Audible Images Recording Studios (Studio 42 of 481)...\n",
      "Scraping details for Audio Sorcery (Studio 43 of 481)...\n",
      "Scraping details for AudioBeach Studios (Studio 44 of 481)...\n",
      "Scraping details for AudioForge (Studio 45 of 481)...\n",
      "Scraping details for Auratonestudio (Studio 46 of 481)...\n",
      "Scraping details for Avatar Studio A (Studio 47 of 481)...\n",
      "Scraping details for Avatar Studio B (Studio 48 of 481)...\n",
      "Scraping details for Avatar Studio C (Studio 49 of 481)...\n",
      "Scraping details for Avatar Studio E (Studio 50 of 481)...\n",
      "Scraping details for Avatar Studio G (Studio 51 of 481)...\n",
      "Scraping details for Avatar Studio R (Studio 52 of 481)...\n",
      "Scraping details for Avatar Studio W: Writing Room (Studio 53 of 481)...\n",
      "Scraping details for b9studio (Studio 54 of 481)...\n",
      "Scraping details for Beat Street Studio: Neve Room (Studio 55 of 481)...\n",
      "Scraping details for Beat Suite Studio (Studio 56 of 481)...\n",
      "Scraping details for Beckview Studios (Studio 57 of 481)...\n",
      "Could not parse coordinates from map URL\n",
      "Scraping details for Beckview Studios (Studio 58 of 481)...\n",
      "Could not parse coordinates from map URL\n",
      "Scraping details for Beckview Studios (Studio 59 of 481)...\n",
      "Could not parse coordinates from map URL\n",
      "Scraping details for Berlin Studio 1 (Studio 60 of 481)...\n",
      "Scraping details for Berry Hill Recording Studio (Studio 61 of 481)...\n",
      "Scraping details for Blackvoice Studio (Studio 62 of 481)...\n",
      "Scraping details for Blast Recording Studio (Studio 63 of 481)...\n",
      "Scraping details for Blue Box Studio 1 (Studio 64 of 481)...\n",
      "Scraping details for Blue Box Studio 2 (Studio 65 of 481)...\n",
      "Scraping details for BonaFide Studio (Studio 66 of 481)...\n",
      "Scraping details for Born In A Barn Studio (Studio 67 of 481)...\n",
      "Scraping details for Brighton Road Recording Studios (Studio 68 of 481)...\n",
      "Scraping details for Broadoak Studios (Studio 69 of 481)...\n",
      "Scraping details for Broadwood Music Productions (Studio 70 of 481)...\n",
      "Scraping details for Bucket Recording Studios (Studio 71 of 481)...\n",
      "Scraping details for Buckinghamshire New University Studio 1 (Studio 72 of 481)...\n",
      "Scraping details for Buckinghamshire New University Studio 2 (Studio 73 of 481)...\n",
      "Scraping details for Buckinghamshire New University Studio 4 (Studio 74 of 481)...\n",
      "Scraping details for Buffalo Studio (Studio 75 of 481)...\n",
      "Scraping details for Buffalo Studio (Studio 76 of 481)...\n",
      "Scraping details for Buffalo Studio (Studio 77 of 481)...\n",
      "Scraping details for Bush Studio 1 (Studio 78 of 481)...\n",
      "Scraping details for Butcher Row Studios (Studio 79 of 481)...\n",
      "Scraping details for Ca Va Sound (Studio 80 of 481)...\n",
      "Scraping details for Castlesound Studios (Studio 81 of 481)...\n",
      "Scraping details for Chamber Studio (Studio 82 of 481)...\n",
      "Scraping details for Chapel Studios (Studio 83 of 481)...\n",
      "Scraping details for Chem19 Studio 1 (Studio 84 of 481)...\n",
      "Scraping details for Chem19 Studio 2 (Studio 85 of 481)...\n",
      "Scraping details for Circle Studios Birmingham (Studio 86 of 481)...\n",
      "Scraping details for Conversion Studios (Studio 87 of 481)...\n",
      "Scraping details for Conway Recording Studio A (Studio 88 of 481)...\n",
      "Scraping details for Conway Recording Studio B (Studio 89 of 481)...\n",
      "Scraping details for Conway Recording Studio C (Studio 90 of 481)...\n",
      "Scraping details for Coozes Recording Studio (Studio 91 of 481)...\n",
      "Scraping details for Crooks Hall Studio (Studio 92 of 481)...\n",
      "Scraping details for Dairy Studio 1 - Neve Tracking Room (Studio 93 of 481)...\n",
      "Scraping details for Dairy Studio 2 - SSL Mix Room (Studio 94 of 481)...\n",
      "Scraping details for Dairy Studio 3 - Writing / Recording (Studio 95 of 481)...\n",
      "Error fetching studio details for https://www.allstudios.co.uk/studios/recording-studio/dairy-studio-3-writing-/-recording: 404 Client Error: Not Found for url: https://www.allstudios.co.uk/studios/recording-studio/dairy-studio-3-writing-/-recording\n",
      "Scraping details for Darkhorse Recording Studio Surrey (Studio 96 of 481)...\n",
      "Scraping details for Dave Speakman Music (Studio 97 of 481)...\n",
      "Could not parse coordinates from map URL\n",
      "Scraping details for DBM Studios (Studio 98 of 481)...\n",
      "Scraping details for Dean Street Studio 1 (Studio 99 of 481)...\n",
      "Scraping details for Dean Street Studio 2 (Studio 100 of 481)...\n",
      "Scraping details for Dean Street Studio 3 (Studio 101 of 481)...\n",
      "Scraping details for Dean Street Studio 5 (Studio 102 of 481)...\n",
      "Scraping details for Definition Arts Studio (Studio 103 of 481)...\n",
      "Scraping details for Distant City Studios (Studio 104 of 481)...\n",
      "Scraping details for Dubrek Studios (Studio 105 of 481)...\n",
      "Scraping details for Earthworks Studio A (Studio 106 of 481)...\n",
      "Scraping details for Eastcote Studio 1 (Studio 107 of 481)...\n",
      "Scraping details for Echo Studios (Studio 108 of 481)...\n",
      "Scraping details for Edge Recording Studio (Studio 109 of 481)...\n",
      "Scraping details for Edwin Street Recording Studio (Studio 110 of 481)...\n",
      "Scraping details for Einstein Studios (Studio 111 of 481)...\n",
      "Scraping details for Electric Lady Studios: Studio A (Studio 112 of 481)...\n",
      "Scraping details for Electric Lady Studios: Studio B (Studio 113 of 481)...\n",
      "Scraping details for Electric Lady Studios: Studio C (Studio 114 of 481)...\n",
      "Scraping details for Electric Lady Studios: Studio D (Studio 115 of 481)...\n",
      "Scraping details for Element Studio (Studio 116 of 481)...\n",
      "Scraping details for Embrio Production (Studio 117 of 481)...\n",
      "Scraping details for Engine Room Audio (Studio 118 of 481)...\n",
      "Scraping details for Enmore Audio (Studio 119 of 481)...\n",
      "Scraping details for Estudio del Sur (Studio 120 of 481)...\n",
      "Scraping details for Evolution Recording Studios (Studio 121 of 481)...\n",
      "Scraping details for Fabric Audio Recording (Studio 122 of 481)...\n",
      "Scraping details for Factory Street Recording (Studio 123 of 481)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping details for Fairview Recording Studio (Studio 124 of 481)...\n",
      "Scraping details for Fantasy Recording Studios: Studio A (Studio 125 of 481)...\n",
      "Scraping details for Fantasy Recording Studios: Studio D (Studio 126 of 481)...\n",
      "Scraping details for  (Studio 127 of 481)...\n",
      "Scraping details for Faultline Studios (Studio 128 of 481)...\n",
      "Scraping details for Finnvox Studios (Studio 129 of 481)...\n",
      "Scraping details for Fire K Studios (Studio 130 of 481)...\n",
      "Scraping details for Fireplace Studios (Studio 131 of 481)...\n",
      "Scraping details for Flank Recording Studio (Studio 132 of 481)...\n",
      "Scraping details for Flipside Recording Studios (Studio 133 of 481)...\n",
      "Scraping details for Fluidity-Studios (Studio 134 of 481)...\n",
      "Scraping details for Fluke Productions (Studio 135 of 481)...\n",
      "Scraping details for Flux Studios: Dangerous Room (Studio 136 of 481)...\n",
      "Scraping details for Flux Studios: Fabulous Room (Studio 137 of 481)...\n",
      "Scraping details for Flux Studios: Revolution Room (Studio 138 of 481)...\n",
      "Scraping details for Foel Studio (Studio 139 of 481)...\n",
      "Scraping details for Ford Lane Studios (Studio 140 of 481)...\n",
      "Scraping details for Form & Filter (Studio 141 of 481)...\n",
      "Scraping details for Forward Studios - Aurelius Room (Studio 142 of 481)...\n",
      "Scraping details for Forward Studios - Caesar Room (Studio 143 of 481)...\n",
      "Scraping details for Fossil Studios (Studio 144 of 481)...\n",
      "Scraping details for FP Recording Studio (Studio 145 of 481)...\n",
      "Scraping details for Freefall Studios (Studio 146 of 481)...\n",
      "Scraping details for Full Tilt Productions (Studio 147 of 481)...\n",
      "Scraping details for Furnace Residential Recording Studio (Studio 148 of 481)...\n",
      "Scraping details for Fusion Arts Oswestry Recording Studio (Studio 149 of 481)...\n",
      "Scraping details for FWD Motion Studios Croydon (Studio 150 of 481)...\n",
      "Scraping details for Gadget Studios (Studio 151 of 481)...\n",
      "Scraping details for Gav's Recording Studio & Rehearsal Rooms (Studio 152 of 481)...\n",
      "Scraping details for Germano Studios (Studio 153 of 481)...\n",
      "Scraping details for Ghost Machine (Studio 154 of 481)...\n",
      "Scraping details for Giant Wafer Studios (Studio 155 of 481)...\n",
      "Scraping details for Glasgow Recording Studio (Studio 156 of 481)...\n",
      "Scraping details for Glass Studio 1 (Studio 157 of 481)...\n",
      "Scraping details for Glass Studio 2 (Studio 158 of 481)...\n",
      "Scraping details for Glastonbury Studio 9 (Studio 159 of 481)...\n",
      "Scraping details for Goldfingers Studio Brussels (Studio 160 of 481)...\n",
      "Scraping details for Gracieland Studio (Studio 161 of 481)...\n",
      "Scraping details for Grand Cru Studio (Studio 162 of 481)...\n",
      "Scraping details for Grand Studio (Studio 163 of 481)...\n",
      "Scraping details for Grapevine Studios (Studio 164 of 481)...\n",
      "Scraping details for GTM Studios (Studio 165 of 481)...\n",
      "Scraping details for Harbour Recording Studio (Studio 166 of 481)...\n",
      "Scraping details for Hardstudios: Studio A (Studio 167 of 481)...\n",
      "Scraping details for Hardstudios: Studio B (Studio 168 of 481)...\n",
      "Scraping details for Hardstudios: Studio D (Studio 169 of 481)...\n",
      "Scraping details for Hastings Old Town Recording Studio (Studio 170 of 481)...\n",
      "Scraping details for Hatch Farm Studio 1 (Studio 171 of 481)...\n",
      "Scraping details for Hats Off Studios (Studio 172 of 481)...\n",
      "Scraping details for Headroom Studios (Studio 173 of 481)...\n",
      "Scraping details for Heart Of Gold Studios (Studio 174 of 481)...\n",
      "Scraping details for Heartbeat Studio (Studio 175 of 481)...\n",
      "Scraping details for Hellfire Studios III (Studio 176 of 481)...\n",
      "Scraping details for Hockley Street Studios (Studio 177 of 481)...\n",
      "Scraping details for Home Farm Studios (Studio 178 of 481)...\n",
      "Scraping details for Hook Road Studio (Studio 179 of 481)...\n",
      "Scraping details for Horizontal Music Studios (Studio 180 of 481)...\n",
      "Scraping details for Hot Milk Studio (Studio 181 of 481)...\n",
      "Scraping details for Hoxa HQ (Studio 182 of 481)...\n",
      "Scraping details for HS Recording (Studio 183 of 481)...\n",
      "Scraping details for Hypermonosonic Studios (Studio 184 of 481)...\n",
      "Scraping details for Iguana Studio 1 (Studio 185 of 481)...\n",
      "Scraping details for Iguana Studio 2 (Studio 186 of 481)...\n",
      "Scraping details for Influential Studios (Studio 187 of 481)...\n",
      "Scraping details for Infrasonic Sound Studio (Studio 188 of 481)...\n",
      "Scraping details for JB Recording Studio 1 (Studio 189 of 481)...\n",
      "Scraping details for JB Recording Studio 2 (Studio 190 of 481)...\n",
      "Scraping details for Jingletown Studio A (Studio 191 of 481)...\n",
      "Scraping details for Jingletown Studio C (Studio 192 of 481)...\n",
      "Scraping details for Kaleidoscope Sound: Studio A (Studio 193 of 481)...\n",
      "Scraping details for Kaleidoscope Sound: The Patio (Studio 194 of 481)...\n",
      "Scraping details for karlson recording studio (Studio 195 of 481)...\n",
      "Scraping details for Karma Sound Studio 1 (Studio 196 of 481)...\n",
      "Scraping details for Karma Sound Studio 2 (Studio 197 of 481)...\n",
      "Scraping details for Keep Recording (Studio 198 of 481)...\n",
      "Scraping details for Keep The Edge Studios (Studio 199 of 481)...\n",
      "Scraping details for Kensaltown Studios (Studio 200 of 481)...\n",
      "Scraping details for Kensaltown: Studio E - Toast (Studio 201 of 481)...\n",
      "Scraping details for King Sound Studio (Studio 202 of 481)...\n",
      "Could not parse coordinates from map URL\n",
      "Scraping details for KJM Studios (Studio 203 of 481)...\n",
      "Scraping details for Konk Studio 1: Neve Room (Studio 204 of 481)...\n",
      "Scraping details for Konk Studio 2: SSL Room (Studio 205 of 481)...\n",
      "Scraping details for Kore Studio 1 (Studio 206 of 481)...\n",
      "Scraping details for Kore Studio 2 (Studio 207 of 481)...\n",
      "Scraping details for Kraute Studios (Studio 208 of 481)...\n",
      "Scraping details for La Chunky Studio (Studio 209 of 481)...\n",
      "Scraping details for La Fabrique (Studio 210 of 481)...\n",
      "Scraping details for La Fabrique: The Loft (Studio 211 of 481)...\n",
      "Scraping details for Lautstumm (Studio 212 of 481)...\n",
      "Scraping details for Leeders Vale Studios (Studio 213 of 481)...\n",
      "Scraping details for Legacy Studio (Studio 214 of 481)...\n",
      "Scraping details for Lever Street Studios (Studio 215 of 481)...\n",
      "Scraping details for Life After Dark Studio (Studio 216 of 481)...\n",
      "Scraping details for Limefield Studio (Studio 217 of 481)...\n",
      "Scraping details for Little Monkey Studios (Studio 218 of 481)...\n",
      "Scraping details for Lofi Studios & 7 West Studios (Studio 219 of 481)...\n",
      "Scraping details for Loft Music Studios (Studio 220 of 481)...\n",
      "Scraping details for Long Track Studios (Studio 221 of 481)...\n",
      "Scraping details for Lost Boys Studio (Studio 222 of 481)...\n",
      "Could not parse coordinates from map URL\n",
      "Scraping details for Lovebuzz Productions (Studio 223 of 481)...\n",
      "Scraping details for LTS Recording and PA Services (Studio 224 of 481)...\n",
      "Scraping details for Mad Hat Studios (Studio 225 of 481)...\n",
      "Scraping details for Magic & Sound Studio 1 (Studio 226 of 481)...\n",
      "Scraping details for Manor Park Studio (Studio 227 of 481)...\n",
      "Scraping details for MAP Recording Studio (Studio 228 of 481)...\n",
      "Scraping details for Masterlink Productions (Studio 229 of 481)...\n",
      "Scraping details for Matin√©e Multilingual (Studio 230 of 481)...\n",
      "Scraping details for Maybank Studios (Studio 231 of 481)...\n",
      "Scraping details for Melrose Yard Studios (Studio 232 of 481)...\n",
      "Scraping details for Mersey City Music Studio (Studio 233 of 481)...\n",
      "Scraping details for Metropolis Studio A (Studio 234 of 481)...\n",
      "Scraping details for Metropolis Studio B (Studio 235 of 481)...\n",
      "Scraping details for Metropolis Studio C (Studio 236 of 481)...\n",
      "Scraping details for Metropolis Studio E (Studio 237 of 481)...\n",
      "Scraping details for Metway Studios (Studio 238 of 481)...\n",
      "Scraping details for Mighty Atom Studios (Studio 239 of 481)...\n",
      "Scraping details for Milk Studios (Studio 240 of 481)...\n",
      "Scraping details for Modern Wave Studio (Studio 241 of 481)...\n",
      "Scraping details for Moles Studio (Studio 242 of 481)...\n",
      "Scraping details for Momentum Studios (Studio 243 of 481)...\n",
      "Scraping details for Monkey Puzzle House Studios (Studio 244 of 481)...\n",
      "Scraping details for Monochrome Productions (Studio 245 of 481)...\n",
      "Scraping details for MSR Studio A (Studio 246 of 481)...\n",
      "Scraping details for MSR Studio B (Studio 247 of 481)...\n",
      "Scraping details for Music Lan, Spain (Studio 248 of 481)...\n",
      "Scraping details for Mutiny (Studio 249 of 481)...\n",
      "Scraping details for Mutts Nutts Recording Studio (Studio 250 of 481)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping details for Mwnci (Monkey) Studios (Studio 251 of 481)...\n",
      "Scraping details for Nevo Sound Studios (Studio 252 of 481)...\n",
      "Scraping details for NextDoorStudio (Studio 253 of 481)...\n",
      "Scraping details for NightBird Recording Studio A (Studio 254 of 481)...\n",
      "Scraping details for NightBird Recording Studio B (Studio 255 of 481)...\n",
      "Scraping details for No Shame Labs (Studio 256 of 481)...\n",
      "Scraping details for North Seven Studios (Studio 257 of 481)...\n",
      "Scraping details for Oak Recording Studios (Studio 258 of 481)...\n",
      "Scraping details for Old Foundry Studios (Studio 259 of 481)...\n",
      "Scraping details for One Louder Studio 1 (Studio 260 of 481)...\n",
      "Scraping details for One Louder Studio 2 (Studio 261 of 481)...\n",
      "Scraping details for Online Music Mixing (Studio 262 of 481)...\n",
      "Scraping details for Orchard Recording Studios (Studio 263 of 481)...\n",
      "Scraping details for Outhouse Studios (Studio 264 of 481)...\n",
      "Scraping details for Oxford Audio Post Production (Studio 265 of 481)...\n",
      "Scraping details for Oxford Duplication Centre (Studio 266 of 481)...\n",
      "Scraping details for Parlour Recording Studio (Studio 267 of 481)...\n",
      "Scraping details for Parr Street Studios - Currently CLOSED (Studio 268 of 481)...\n",
      "Scraping details for Peak Studios (Studio 269 of 481)...\n",
      "Scraping details for Perry Road Studios (Studio 270 of 481)...\n",
      "Scraping details for Perry Vale Studios (Studio 271 of 481)...\n",
      "Scraping details for Phosphene Productions (Studio 272 of 481)...\n",
      "Scraping details for Pierce Rooms Programming Rooms (x2) (Studio 273 of 481)...\n",
      "Scraping details for Pierce Rooms Studio 1 (Studio 274 of 481)...\n",
      "Scraping details for Planck Music Studio (Studio 275 of 481)...\n",
      "Scraping details for Plato Studios: Main Studio (Studio 276 of 481)...\n",
      "Scraping details for Plato Studios: Production Studio (Studio 277 of 481)...\n",
      "Scraping details for Plus11 Recording Studio (Studio 278 of 481)...\n",
      "Scraping details for Polygone Studio A (Studio 279 of 481)...\n",
      "Scraping details for Polygone Studio B (Studio 280 of 481)...\n",
      "Scraping details for Powerplay Studios (Studio 281 of 481)...\n",
      "Scraping details for Premier Studios: Studio A (Studio 282 of 481)...\n",
      "Scraping details for Premier Studios: Studio B (Studio 283 of 481)...\n",
      "Scraping details for Premier Studios: Writing Rooms x2 (Studio 284 of 481)...\n",
      "Scraping details for Premises Studio A (Solar Powered) (Studio 285 of 481)...\n",
      "Scraping details for Press Play Studio (Studio 286 of 481)...\n",
      "Scraping details for Priory Recording Studio (Studio 287 of 481)...\n",
      "Scraping details for Pro 2 Studios (Studio 288 of 481)...\n",
      "Scraping details for Produc'son (Studio 289 of 481)...\n",
      "Scraping details for Propagation House Studios (Studio 290 of 481)...\n",
      "Scraping details for Pulse Studios (Studio 291 of 481)...\n",
      "Scraping details for Quad Lakeside (Studio 292 of 481)...\n",
      "Scraping details for Quad Recording Studios: Q1 (Studio 293 of 481)...\n",
      "Scraping details for Quad Recording Studios: Q2 (Studio 294 of 481)...\n",
      "Scraping details for Radar Sounds (Studio 295 of 481)...\n",
      "Scraping details for Railway Recording Studios: Studio B (Studio 296 of 481)...\n",
      "Scraping details for Railway Studios (Studio 297 of 481)...\n",
      "Scraping details for Rak Studio 1 (Studio 298 of 481)...\n",
      "Scraping details for Rak Studio 2 (Studio 299 of 481)...\n",
      "Scraping details for Rak Studio 3 (Studio 300 of 481)...\n",
      "Scraping details for Rak Studio 4 (Studio 301 of 481)...\n",
      "Scraping details for Ravenscourt: Studio 3 (Studio 302 of 481)...\n",
      "Scraping details for Real World The Big Room (Studio 303 of 481)...\n",
      "Scraping details for Real World The Wood Room (Studio 304 of 481)...\n",
      "Scraping details for Red Bus: Studio 1 (Studio 305 of 481)...\n",
      "Scraping details for Red Kite Studio (Studio 306 of 481)...\n",
      "Scraping details for Redbridge Recording Studio (Studio 307 of 481)...\n",
      "Scraping details for Redwood Studios (Studio 308 of 481)...\n",
      "Scraping details for Reel Recording Studio (Studio 309 of 481)...\n",
      "Scraping details for REKrooms - Red Room (Studio 310 of 481)...\n",
      "Scraping details for Resident Studios: Studio A (Studio 311 of 481)...\n",
      "Scraping details for Retro Central Recording Studio Portsmouth (Studio 312 of 481)...\n",
      "Scraping details for Revolution Studio 1 (Studio 313 of 481)...\n",
      "Scraping details for Revolution Studio 2 (Studio 314 of 481)...\n",
      "Scraping details for Rex Studio Pink (Studio 315 of 481)...\n",
      "Could not parse coordinates from map URL\n",
      "Scraping details for Rex Studio Pink (Studio 316 of 481)...\n",
      "Could not parse coordinates from map URL\n",
      "Scraping details for Rex Studio Pink (Studio 317 of 481)...\n",
      "Could not parse coordinates from map URL\n",
      "Scraping details for Rhondda Street Studios (Studio 318 of 481)...\n",
      "Scraping details for Ridgeway Sound Studio (Studio 319 of 481)...\n",
      "Scraping details for Rimshot Studio (Studio 320 of 481)...\n",
      "Scraping details for Riverside Music Complex (Studio 321 of 481)...\n",
      "Scraping details for RMS Studio 1 (Studio 322 of 481)...\n",
      "Scraping details for RMS Studio 2 (Studio 323 of 481)...\n",
      "Scraping details for Roasted Studios (Studio 324 of 481)...\n",
      "Scraping details for Roc2 Studios (Studio 325 of 481)...\n",
      "Scraping details for Rock Hard Studios (Studio 326 of 481)...\n",
      "Scraping details for Rofl Audio (Studio 327 of 481)...\n",
      "Scraping details for Rogue Recording Studios (Studio 328 of 481)...\n",
      "Could not parse coordinates from map URL\n",
      "Scraping details for Rollover 1 (Studio 329 of 481)...\n",
      "Scraping details for Rollover 2 (Studio 330 of 481)...\n",
      "Scraping details for Rollover 3 (Studio 331 of 481)...\n",
      "Scraping details for Rooflight Production (Studio 332 of 481)...\n",
      "Scraping details for Room With A View Studio (Studio 333 of 481)...\n",
      "Scraping details for RPL Acton Studios (Studio 334 of 481)...\n",
      "Scraping details for Sain Studio 1 (Studio 335 of 481)...\n",
      "Scraping details for Sain Studio 2 (Studio 336 of 481)...\n",
      "Scraping details for Sain Studio 3 (Studio 337 of 481)...\n",
      "Scraping details for Saltlands Studios: A Room (Studio 338 of 481)...\n",
      "Scraping details for Saltlands Studios: B Room (Studio 339 of 481)...\n",
      "Scraping details for Sanctuary Studios (Studio 340 of 481)...\n",
      "Scraping details for Sarm Music Village- Island Life (Orange) Suite (Studio 341 of 481)...\n",
      "Scraping details for Sarm Music Village- Perfect (Green) Suite (Studio 342 of 481)...\n",
      "Scraping details for Sarm Music Village- ZTT (Red) Studio (Studio 343 of 481)...\n",
      "Scraping details for Satellite Recording Studios (Studio 344 of 481)...\n",
      "Scraping details for Sawmills Studio 1 (Studio 345 of 481)...\n",
      "Scraping details for SDOS Recording Studios (Studio 346 of 481)...\n",
      "Scraping details for Select Recording Studios (Studio 347 of 481)...\n",
      "Scraping details for Sensible Main Studio (Studio 348 of 481)...\n",
      "Scraping details for Shaken Oak Studios (Studio 349 of 481)...\n",
      "Scraping details for Shireshead Studios (Studio 350 of 481)...\n",
      "Scraping details for Shock & Awe Recording Studio (Studio 351 of 481)...\n",
      "Scraping details for Shushstudio (Studio 352 of 481)...\n",
      "Scraping details for Sideways Sounds (Studio 353 of 481)...\n",
      "Scraping details for Sinewavz (Studio 354 of 481)...\n",
      "Scraping details for Smokehouse Studios (Studio 355 of 481)...\n",
      "Scraping details for Snap Studios (Studio 356 of 481)...\n",
      "Scraping details for SNK Studio 1 (Studio 357 of 481)...\n",
      "Scraping details for SNK Studio 2 (Studio 358 of 481)...\n",
      "Scraping details for SNK Studio 3 (Studio 359 of 481)...\n",
      "Scraping details for Sonic Forge Music (Studio 360 of 481)...\n",
      "Scraping details for Sonic One Studio (Studio 361 of 481)...\n",
      "Scraping details for Sonic Vista Studios (Studio 362 of 481)...\n",
      "Scraping details for Sonic Visuals Recording Studio (Studio 363 of 481)...\n",
      "Scraping details for Sound Gallery Studios (Studio 364 of 481)...\n",
      "Scraping details for Sound Productions (Studio 365 of 481)...\n",
      "Scraping details for Sound Savers (Studio 366 of 481)...\n",
      "Scraping details for Sounding Sweet (Studio 367 of 481)...\n",
      "Scraping details for Soundlab Studios (Studio 368 of 481)...\n",
      "Scraping details for Soundscape Studios (Studio 369 of 481)...\n",
      "Scraping details for Soundshift Studios (Studio 370 of 481)...\n",
      "Scraping details for SoundSource Design (Studio 371 of 481)...\n",
      "Scraping details for SoundSource Studios (Studio 372 of 481)...\n",
      "Scraping details for SoundView Studios (Studio 373 of 481)...\n",
      "Scraping details for Spinroad Recording Studios (Studio 374 of 481)...\n",
      "Scraping details for Springvale Recording Studio (Studio 375 of 481)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping details for Squirky Studios (Studio 376 of 481)...\n",
      "Scraping details for SSR London Ghost Studio (Studio 377 of 481)...\n",
      "Scraping details for SSR London Icon Studio (Studio 378 of 481)...\n",
      "Scraping details for SSR London Neve Studio (Studio 379 of 481)...\n",
      "Scraping details for Stag Studios (Studio 380 of 481)...\n",
      "Scraping details for Start Together: Studio 1 (Studio 381 of 481)...\n",
      "Scraping details for Start Together: Studio 2 (Studio 382 of 481)...\n",
      "Scraping details for Start Together: Studio 3 (Studio 383 of 481)...\n",
      "Scraping details for State Of The Ark Studios - CURRENTLY LONGTERM LET (Studio 384 of 481)...\n",
      "Scraping details for Steelworks Studio 1 (Studio 385 of 481)...\n",
      "Scraping details for Strange Weather (Studio 386 of 481)...\n",
      "Scraping details for Strongroom Studio 1 (Studio 387 of 481)...\n",
      "Scraping details for Strongroom Studio 3 (Studio 388 of 481)...\n",
      "Scraping details for Strongroom Studio 4 (Studio 389 of 481)...\n",
      "Scraping details for Strummers Recording Studio (Studio 390 of 481)...\n",
      "Scraping details for Studio 11 (Studio 391 of 481)...\n",
      "Scraping details for Studio 33 (Studio 392 of 481)...\n",
      "Scraping details for Studio 6/49 (Studio 393 of 481)...\n",
      "Error fetching studio details for https://www.allstudios.co.uk/studios/recording-studio/studio-6/49: 404 Client Error: Not Found for url: https://www.allstudios.co.uk/studios/recording-studio/studio-6/49\n",
      "Scraping details for Studio City UK (Studio 394 of 481)...\n",
      "Scraping details for Studio G: The 'A' Room (Studio 395 of 481)...\n",
      "Scraping details for Studio JAMA (Studio 396 of 481)...\n",
      "Scraping details for Studio Kekkonen: Studio 1 (Studio 397 of 481)...\n",
      "Scraping details for Studio Kekkonen: Studio 2 (Studio 398 of 481)...\n",
      "Scraping details for Studio Le Roy (Studio 399 of 481)...\n",
      "Scraping details for Studio Radioaktywni (Studio 400 of 481)...\n",
      "Scraping details for Studio Relief Switzerland (Studio 401 of 481)...\n",
      "Scraping details for StudiOwz (Studio 402 of 481)...\n",
      "Scraping details for Suff Studio (Studio 403 of 481)...\n",
      "Scraping details for Sugar Cane Studios  (CURRENTLY ON LONGTERM LET) (Studio 404 of 481)...\n",
      "Scraping details for Sundial Recordings (Studio 405 of 481)...\n",
      "Scraping details for Superhet (Studio 406 of 481)...\n",
      "Scraping details for SweetSounds Studio: Broadway Room (Studio 407 of 481)...\n",
      "Scraping details for SweetSounds Studio: Crosby Room (Studio 408 of 481)...\n",
      "Scraping details for Syncbox Post (Studio 409 of 481)...\n",
      "Scraping details for T-Bone Tunes Recording Studio (Studio 410 of 481)...\n",
      "Scraping details for Temple Of Boom (Studio 411 of 481)...\n",
      "Scraping details for Ten21 Recording Studios (Studio 412 of 481)...\n",
      "Scraping details for Terminus: Studio A (Studio 413 of 481)...\n",
      "Scraping details for Terminus: Studio B (Studio 414 of 481)...\n",
      "Scraping details for The Attic Sounds (Studio 415 of 481)...\n",
      "Scraping details for The Blue Studios (Studio 416 of 481)...\n",
      "Scraping details for The Brother Sound (Studio 417 of 481)...\n",
      "Scraping details for The Bunker Studio A (Studio 418 of 481)...\n",
      "Scraping details for The Bunker Studio B (Studio 419 of 481)...\n",
      "Scraping details for The Button Factory (Studio 420 of 481)...\n",
      "Scraping details for The Byre (Studio 421 of 481)...\n",
      "Scraping details for The Chairworks - Big Room (Studio 422 of 481)...\n",
      "Scraping details for The Chapel Studios, London: Studio 1 (Studio 423 of 481)...\n",
      "Scraping details for The Chapel Studios, London: Studio 2 (Studio 424 of 481)...\n",
      "Scraping details for The Crypt Studio 1 - North London (Studio 425 of 481)...\n",
      "Scraping details for The Darkroom (Studio 426 of 481)...\n",
      "Scraping details for The Den Studios (Dudley) (Studio 427 of 481)...\n",
      "Scraping details for The Elementz Studios (Studio 428 of 481)...\n",
      "Scraping details for The Fold Studios (Studio 429 of 481)...\n",
      "Scraping details for The Garage (Studio 430 of 481)...\n",
      "Scraping details for The Garden, Brooklyn (Studio 431 of 481)...\n",
      "Scraping details for The Hatch (Studio 432 of 481)...\n",
      "Scraping details for The Hideout Recording Studio (Studio 433 of 481)...\n",
      "Scraping details for The Hub Sound Studios (Studio 434 of 481)...\n",
      "Scraping details for The Lab Studios (Studio 435 of 481)...\n",
      "Scraping details for The Lodge Studio 1 (Studio 436 of 481)...\n",
      "Scraping details for The Lodge Studio 2 (Studio 437 of 481)...\n",
      "Scraping details for The Lounge (Studio 438 of 481)...\n",
      "Scraping details for The Old Smithy (Studio 439 of 481)...\n",
      "Scraping details for The Overlook aka Blackmon Audio (Studio 440 of 481)...\n",
      "Scraping details for The Recording Studio London (Studio 441 of 481)...\n",
      "Scraping details for The Shrubbery (Studio 442 of 481)...\n",
      "Scraping details for The Silk Mill Recording Studios (Studio 443 of 481)...\n",
      "Scraping details for The Soundhouse (Studio 444 of 481)...\n",
      "Scraping details for The Stables Recording Studio (Studio 445 of 481)...\n",
      "Scraping details for The Tape Rooms (Studio 446 of 481)...\n",
      "Scraping details for The Warehouse Studio 1 (Studio 447 of 481)...\n",
      "Scraping details for The Way Studio (Studio 448 of 481)...\n",
      "Scraping details for Threecircles Recording Studio (Studio 449 of 481)...\n",
      "Scraping details for Tilehouse Recording Studios (Studio 450 of 481)...\n",
      "Scraping details for Tower Studios (Studio 451 of 481)...\n",
      "Scraping details for Toy Box Studios (Studio 452 of 481)...\n",
      "Scraping details for Translator Audio, Brooklyn (Studio 453 of 481)...\n",
      "Scraping details for Transmission Recording Studios (Studio 454 of 481)...\n",
      "Scraping details for Turistas Sonoros (Studio 455 of 481)...\n",
      "Scraping details for Unit 2 Studios (Studio 456 of 481)...\n",
      "Scraping details for Unit Recording Studio (Studio 457 of 481)...\n",
      "Scraping details for Univibe Recording Studio (Studio 458 of 481)...\n",
      "Scraping details for Unsigned Studios (Studio 459 of 481)...\n",
      "Scraping details for Upward Studios (Studio 460 of 481)...\n",
      "Scraping details for Urban Development (Studio 461 of 481)...\n",
      "Scraping details for Urchin Studios - Studio C (Studio 462 of 481)...\n",
      "Scraping details for Vale Studios (Studio 463 of 481)...\n",
      "Scraping details for Vega Studio (Studio 464 of 481)...\n",
      "Scraping details for VIP Lounge (Studio 465 of 481)...\n",
      "Scraping details for Wall of Waves (Studio 466 of 481)...\n",
      "Scraping details for WareHaus Recording Studio, LLC (Studio 467 of 481)...\n",
      "Scraping details for Watercolour Music (Studio 468 of 481)...\n",
      "Scraping details for Waterland Studio Venice (Studio 469 of 481)...\n",
      "Scraping details for WAX Recording Studio / Brighton (Studio 470 of 481)...\n",
      "Error fetching studio details for https://www.allstudios.co.uk/studios/recording-studio/wax-recording-studio-/-brighton: 404 Client Error: Not Found for url: https://www.allstudios.co.uk/studios/recording-studio/wax-recording-studio-/-brighton\n",
      "Scraping details for Wendyhouse Productions (Studio 471 of 481)...\n",
      "Scraping details for Western Recording Company (Studio 472 of 481)...\n",
      "Scraping details for Whitehall Studios (Studio 473 of 481)...\n",
      "Scraping details for Whitehouse Studios (Studio 474 of 481)...\n",
      "Scraping details for Whitelab Records (Studio 475 of 481)...\n",
      "Scraping details for Wilding Sounds Studio (Studio 476 of 481)...\n",
      "Scraping details for Wimbledon Sound (Studio 477 of 481)...\n",
      "Scraping details for Windmill Lane Recording Studio 1 (Studio 478 of 481)...\n",
      "Scraping details for Windmill Lane Recording Studio 1 (Studio 479 of 481)...\n",
      "Scraping details for Windmill Lane Recording Studio 1 (Studio 480 of 481)...\n",
      "Scraping details for Windmill Lane Recording Studio 2 (Studio 481 of 481)...\n",
      "Scraping complete. Data saved to studios_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "class AllStudiosScraper:\n",
    "    BASE_URL = \"https://www.allstudios.co.uk\"\n",
    "    \n",
    "    def __init__(self, output_file='data/studios_data.csv'):\n",
    "        self.output_file = output_file\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def get_studio_list(self):\n",
    "        \"\"\"Retrieve list of studios from the main page\"\"\"\n",
    "        url = f\"{self.BASE_URL}/studios/recording-studio?clear=true\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            studios = []\n",
    "            for li in soup.select('ul.studios-list li'):\n",
    "                studio_link = li.find('a', href=lambda href: href and '/studios/recording-studio/' in href)\n",
    "                studio_website = li.find('a', class_='studio_index_button1')\n",
    "                \n",
    "                if studio_link and studio_website:\n",
    "                    studio_name = studio_link.text.strip()\n",
    "                    studio_page = self.BASE_URL + studio_link['href']\n",
    "                    website = studio_website.get('href', 'N/A')\n",
    "                    \n",
    "                    studios.append({\n",
    "                        'name': studio_name,\n",
    "                        'page': studio_page,\n",
    "                        'website': website\n",
    "                    })\n",
    "            \n",
    "            return studios\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching studio list: {e}\")\n",
    "            return []\n",
    "###    \n",
    "    def extract_location_from_map(self, soup):\n",
    "        \"\"\"Extract location information from Google Maps static map URL\"\"\"\n",
    "        location_info = {\n",
    "            'latitude': 'N/A',\n",
    "            'longitude': 'N/A',\n",
    "            'city': 'N/A',\n",
    "            'country': 'N/A'\n",
    "        }\n",
    "\n",
    "        # Find the Google Maps static map image\n",
    "        map_img = soup.find('img', src=re.compile(r'https://maps\\.googleapis\\.com/maps/api/staticmap'))\n",
    "\n",
    "        if map_img and map_img.get('src'):\n",
    "            map_url = map_img['src']\n",
    "\n",
    "            # Extract coordinates\n",
    "            coord_match = re.search(r'center=([^&]+)', map_url)\n",
    "            if coord_match:\n",
    "                try:\n",
    "                    latitude, longitude = coord_match.group(1).split(',')\n",
    "                    location_info['latitude'] = latitude.strip()\n",
    "                    location_info['longitude'] = longitude.strip()\n",
    "\n",
    "                except ValueError:\n",
    "                    print(\"Could not parse coordinates from map URL\")\n",
    "\n",
    "        return location_info\n",
    "###   \n",
    "    def extract_contact_info(self, soup):\n",
    "        \"\"\"Advanced contact information extraction\"\"\"\n",
    "        contact_info = {\n",
    "            'postal_address': 'N/A',\n",
    "            'contact_name': 'N/A',\n",
    "            'phone': 'N/A',\n",
    "            'email': 'N/A'\n",
    "        }\n",
    "\n",
    "        # Strategy 1: Look for specific table with studio-view class\n",
    "        studio_table = soup.find('table', class_='studio-view')\n",
    "        if studio_table:\n",
    "            # Try extracting from table rows\n",
    "            rows = studio_table.find_all('tr')\n",
    "            for row in rows:\n",
    "                row_text = row.get_text(strip=True)\n",
    "\n",
    "                # Extract contact name\n",
    "                if 'Contact Name:' in row_text:\n",
    "                    contact_info['contact_name'] = row_text.replace('Contact Name:', '').strip()\n",
    "\n",
    "                # Method 1: Simple string slicing for phone number\n",
    "                if row_text.startswith('Tel:'):\n",
    "                    contact_info['phone'] = row_text[5:].strip()\n",
    "\n",
    "                # Method 2: Regex extraction for phone number\n",
    "                phone_match = re.search(r'Tel:\\s*(.+)', row_text)\n",
    "                if phone_match and contact_info['phone'] == 'N/A':\n",
    "                    contact_info['phone'] = phone_match.group(1).strip()\n",
    "\n",
    "        # Strategy 2: Look for contact information in div elements (kept from original code)\n",
    "        contact_divs = soup.find_all(['div', 'p'], text=re.compile(r'(Contact Name|Tel:|Phone:|Telephone:)'))\n",
    "        for div in contact_divs:\n",
    "            div_text = div.get_text(strip=True)\n",
    "\n",
    "            # Extract contact name\n",
    "            name_match = re.search(r'Contact Name:\\s*(.+)', div_text, re.IGNORECASE)\n",
    "            if name_match:\n",
    "                contact_info['contact_name'] = name_match.group(1).strip()\n",
    "\n",
    "            # Extract phone number\n",
    "            phone_match = re.search(r'(Tel:|Phone:|Telephone:)\\s*(\\+?[\\d\\s\\(\\)-]+)', div_text, re.IGNORECASE)\n",
    "            if phone_match and contact_info['phone'] == 'N/A':\n",
    "                contact_info['phone'] = phone_match.group(2).strip()\n",
    "\n",
    "        # Strategy 3: Look for email link\n",
    "        email_link = soup.find('a', href=re.compile(r'^mailto:'))\n",
    "        if email_link:\n",
    "            contact_info['email'] = email_link['href'].replace('mailto:', '').strip()\n",
    "\n",
    "        # Strategy 4: Look for postal address\n",
    "        if studio_table:\n",
    "            address_row = studio_table.find('p')\n",
    "            if address_row:\n",
    "                contact_info['postal_address'] = address_row.get_text(strip=True)\n",
    "\n",
    "        return contact_info\n",
    "    \n",
    "    def get_studio_details(self, studio_page):\n",
    "        \"\"\"Retrieve detailed information for a specific studio\"\"\"\n",
    "        try:\n",
    "            response = requests.get(studio_page, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Use advanced extraction methods\n",
    "            contact_info = self.extract_contact_info(soup)\n",
    "            location_info = self.extract_location_from_map(soup)\n",
    "\n",
    "            # Combine the dictionaries\n",
    "            full_details = {**contact_info, **location_info}\n",
    "            return full_details\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching studio details for {studio_page}: {e}\")\n",
    "            return {\n",
    "                'postal_address': 'N/A',\n",
    "                'contact_name': 'N/A',\n",
    "                'phone': 'N/A',\n",
    "                'email': 'N/A',\n",
    "                'latitude': 'N/A',\n",
    "                'longitude': 'N/A',\n",
    "                'city': 'N/A',\n",
    "                'country': 'N/A'\n",
    "            }\n",
    "    \n",
    "    def scrape_all_studios(self, max_studios=None):\n",
    "        \"\"\"Scrape details for studios\"\"\"\n",
    "        studios = self.get_studio_list()\n",
    "        \n",
    "        # Limit studios if max_studios is specified\n",
    "        if max_studios is not None:\n",
    "            studios = studios[:max_studios]\n",
    "\n",
    "        # Prepare CSV file\n",
    "        with open(self.output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['Studio Name', 'Studio Website', 'Contact Name', 'Email', 'Phone', \n",
    "                          'Postal Address', 'Latitude', 'Longitude', 'City', 'Country']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "                 # Track progress\n",
    "            total_studios = len(studios)\n",
    "            print(f\"Total studios to scrape: {total_studios}\")\n",
    "            \n",
    "            for index, studio in enumerate(studios, 1):\n",
    "                print(f\"Scraping details for {studio['name']} (Studio {index} of {total_studios})...\")\n",
    "                \n",
    "                # Get additional details\n",
    "                details = self.get_studio_details(studio['page'])\n",
    "\n",
    "                # Combine studio info with details\n",
    "\n",
    "                full_details = {\n",
    "                    'Studio Name': studio['name'],\n",
    "                    'Studio Website': studio['website'],\n",
    "                    'Contact Name': details.get('contact_name', 'N/A'),\n",
    "                    'Email': details.get('email', 'N/A'),\n",
    "                    'Phone': details.get('phone', 'N/A'),\n",
    "                    'Postal Address': details.get('postal_address', 'N/A'),\n",
    "                    'Latitude': details.get('latitude', 'N/A'),\n",
    "                    'Longitude': details.get('longitude', 'N/A'),\n",
    "                    'City': details.get('city', 'N/A'),\n",
    "                    'Country': details.get('country', 'N/A')\n",
    "                }\n",
    "\n",
    "                writer.writerow(full_details)\n",
    "\n",
    "                # Random delay to be respectful of the website\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        print(f\"Scraping complete. Data saved to {self.output_file}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = AllStudiosScraper()\n",
    "    # Scrape first 10 studios for testing\n",
    "    scraper.scrape_all_studios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3606bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to studios_contact_info.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'data/studios_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize Geolocator\n",
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "# Function to validate and reverse geocode coordinates\n",
    "def get_location(lat, lon):\n",
    "    try:\n",
    "        # Check for valid coordinates\n",
    "        if pd.isna(lat) or pd.isna(lon):\n",
    "            return \"Invalid\", \"Invalid\"\n",
    "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
    "            return \"Invalid\", \"Invalid\"\n",
    "        \n",
    "        # Perform reverse geocoding\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location and location.raw.get('address'):\n",
    "            address = location.raw['address']\n",
    "            city = address.get('city', address.get('town', address.get('village', 'Unknown')))\n",
    "            country = address.get('country', 'Unknown')\n",
    "            return city, country\n",
    "        return \"Unknown\", \"Unknown\"\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Timeout\", \"Timeout\"\n",
    "    except Exception as e:\n",
    "        return \"Error\", \"Error\"\n",
    "\n",
    "# Apply the function only to rows with valid latitude and longitude\n",
    "df['City'], df['Country'] = zip(*df.apply(\n",
    "    lambda row: get_location(row.get('Latitude'), row.get('Longitude')), axis=1\n",
    "))\n",
    "\n",
    "# Save the updated DataFrame\n",
    "output_file = 'data/studios_contact_info.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a5fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5f898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef5b14de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_15660/507719236.py:87: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['Coton' 'London' 'London' 'City of Westminster' 'Glasgow' 'Dulliken'\n",
      " 'Manchester' 'City of New York' 'Sydney' 'London' 'Rƒ´ga' 'London'\n",
      " 'London' 'London' 'London' 'London' 'Bournemouth'\n",
      " 'Marrakech ‚µé‚µï‚µï‚¥∞‚¥Ω‚µØ‚µõ ŸÖÿ±ÿßŸÉÿ¥' 'London' 'London' 'London' 'London' 'London'\n",
      " 'London' 'Manchester' 'Manchester' 'London' 'Chelmsford' 'Verteillac'\n",
      " 'M√ºnchen' 'Runcorn' 'Gravesham' 'Test Valley' 'Plymouth' 'London'\n",
      " 'London' 'Pezzeit' 'London' 'London' 'Heywood' 'Bristol' 'McCandless'\n",
      " 'Wealden' 'Brighton' 'Calderdale' 'Geisenfeld' 'City of New York'\n",
      " 'City of New York' 'City of New York' 'City of New York'\n",
      " 'City of New York' 'City of New York' 'City of New York'\n",
      " 'Frankfurt am Main' 'Leeds' 'Newcastle upon Tyne' 'Bispham'\n",
      " 'Forest of Dean' 'London' 'Newcastle upon Tyne' 'London' 'London'\n",
      " 'London' 'Rugby' 'Mid Sussex' 'Rother' 'Swale' 'London' 'High Wycombe'\n",
      " 'High Wycombe' 'High Wycombe' 'London' 'London' 'Glasgow' 'Pencaitland'\n",
      " 'City of Edinburgh' 'East Lindsey' 'Blantyre' 'Blantyre' 'Birmingham'\n",
      " 'Gillingham' 'Los Angeles' 'Los Angeles' 'Los Angeles' 'Oxford'\n",
      " 'West Suffolk' 'London' 'London' 'Epsom and Ewell' 'Birmingham'\n",
      " 'City of Westminster' 'City of Westminster' 'City of Westminster'\n",
      " 'City of Westminster' 'London' 'Calderdale' 'Derby' 'London' 'London'\n",
      " 'Buckingham' 'Oakgrove' 'Bury' 'Clady' 'City of New York'\n",
      " 'City of New York' 'City of New York' 'City of New York' 'Hull'\n",
      " 'Sveta Nedelja' 'City of New York' 'Sydney' 'Mar√≠a Pinto' 'Oxford'\n",
      " 'ŒîŒÆŒºŒøœÇ ŒöŒµœÅŒ±œÑœÉŒπŒΩŒØŒøœÖ - ŒîœÅŒ±œÄŒµœÑœÉœéŒΩŒ±œÇ' 'Bradford' 'West Ella' 'Berkeley'\n",
      " 'Berkeley' 'Coton' 'San Francisco' 'Helsinki' 'Baldwin'\n",
      " 'City of New York' 'Stoke-on-Trent' 'Coventry' 'London'\n",
      " 'City of New York' 'City of New York' 'City of New York' 'N/A' 'Arun'\n",
      " 'London' 'Grottaferrata' 'Grottaferrata' 'London' 'Capannori' 'London'\n",
      " 'Upper Allen Township' '–†–∞–∑–≥—Ä–∞–¥' 'Oswestry' 'London' 'London' 'Salford'\n",
      " 'City of New York' 'London' 'N/A' 'Glasgow' 'Birkenhead' 'Birkenhead'\n",
      " 'Glastonbury' 'K√∏benhavn' 'Rochdale' 'London' 'Ribble Valley' 'Solihull'\n",
      " 'Adur' 'Rochford' 'Winterthur' 'Winterthur' 'Winterthur' 'St Leonards'\n",
      " 'Runnymede' 'West Oxfordshire' 'Bergen op Zoom' 'London'\n",
      " 'North Middleton' 'South Derbyshire' 'Birmingham' 'Hertsmere' 'London'\n",
      " 'Weston-super-Mare' 'Berlin' 'London' 'North Hertfordshire'\n",
      " 'City of Nottingham' 'London' 'London' 'Garston' 'Los Angeles' 'London'\n",
      " 'London' 'Oakland' 'Oakland' 'Union City' 'Union City' \"'s-Hertogenbosch\"\n",
      " '‡πÄ‡∏Ç‡∏≤‡∏ä‡∏µ‡∏à‡∏£‡∏£‡∏¢‡πå' '‡πÄ‡∏Ç‡∏≤‡∏ä‡∏µ‡∏à‡∏£‡∏£‡∏¢‡πå' 'Raglan' 'Quincy' 'London' 'London' 'Hereford'\n",
      " 'London' 'London' 'London' 'London' 'La Plata' 'Glasgow'\n",
      " 'Saint-R√©my-de-Provence' 'Saint-R√©my-de-Provence' 'Undingen' 'Ebbw Vale'\n",
      " 'Zwaagdijk-Oost' 'Manchester' 'City of New York' 'Middleton' 'Cheltenham'\n",
      " 'Glasgow' 'Newcastle upon Tyne' 'Colchester' 'London' 'Llannon'\n",
      " 'South Staffordshire' 'Estepona' 'N/A' 'London' 'Guildford' 'Reading'\n",
      " 'Glasgow' 'York' 'Liverpool' 'London' 'London' 'London' 'London'\n",
      " 'Brighton' 'Mumbles' 'London' 'Staffordshire Moorlands' 'Bath' 'Plymouth'\n",
      " 'Stratford-on-Avon' 'City of New York' 'City of New York'\n",
      " 'Avinyonet de Puigvent√≥s' 'Dublin' 'Leeds' 'Llanglydwen' 'London'\n",
      " 'M√∂nchengladbach' 'West Hollywood' 'Los Angeles' 'City of New York'\n",
      " 'London' 'Old Toronto' 'Lewes' 'London' 'London' 'Barton St. David'\n",
      " 'Reading' 'Oxford' 'Cherwell' 'N/A' 'Liverpool' 'Bradford'\n",
      " 'Huntingdonshire' 'London' 'Austin' 'London' 'London' 'London' 'London'\n",
      " 'London' 'Chelmsford' 'Blagnac' 'Blagnac' 'Aesch' 'City of New York'\n",
      " 'City of New York' 'City of New York' 'London' 'London' 'Lichfield'\n",
      " 'Wakefield' 'Toulouse' 'North Tamerton' 'London' 'Town of Warwick'\n",
      " 'City of New York' 'City of New York' 'London' 'ŒëŒ∏ŒÆŒΩŒ±' 'Lisburn' 'London'\n",
      " 'London' 'London' 'London' 'London' 'Box' 'Box' 'London' 'Llandovery'\n",
      " 'Bolton' 'City of Westminster' 'Wakefield' 'T√©bessa ‚µú‚¥±‚¥ª‚µô‚µô‚¥∞ ÿ™ÿ®ÿ≥ÿ©' 'London'\n",
      " 'Fareham' 'Cheadle Hulme' 'Cheadle Hulme' 'Swansea' 'Vale of White Horse'\n",
      " 'Swale' 'Busby' 'London' 'London' 'North West Leicestershire' 'Wrexham'\n",
      " 'Blackpool' 'City of Nottingham' 'London' 'London' 'London' 'Bristol'\n",
      " 'St Leonards' 'London' 'Dinas Dinlle' 'Dinas Dinlle' 'Dinas Dinlle'\n",
      " 'City of New York' 'City of New York' 'Watford' 'London' 'London'\n",
      " 'London' 'London' 'Golant' 'Chilthorne Domer' 'London' 'London'\n",
      " 'West Oxfordshire' 'Wyre' 'London' 'Slough' 'London' 'London' 'London'\n",
      " 'London' 'London' 'London' 'London' 'Cardiff' 'Llangennech' 'Eivissa'\n",
      " 'N/A' 'Exeter' 'Trafford' 'N/A' 'Stratford-on-Avon' 'Epping Forest'\n",
      " 'ŒõŒµŒºŒµœÉœåœÇ' 'London' 'London' 'Stevenage' 'Lindome' 'Babergh' 'London'\n",
      " 'London' 'London' 'London' 'Witham' 'Belfast' 'Belfast' 'Belfast'\n",
      " 'London' 'Sheffield' 'City of New York' 'London' 'London' 'London'\n",
      " 'London' 'Pendle' 'South Holland' 'London' 'City of New York' 'Ptuj'\n",
      " 'Helsinki' 'Helsinki' 'Amsterdam' 'Blachownia' 'Lossy' 'Bradford'\n",
      " 'London' 'London' 'N/A' 'City of New York' 'City of New York' 'London'\n",
      " 'Bristol' 'Dudley' 'Maidstone' 'City of New York' 'City of New York'\n",
      " 'London' 'London' 'Bellville' 'City of New York' 'City of New York'\n",
      " 'London' 'Kilmorack' 'Wakefield' 'London' 'London' 'London' 'London'\n",
      " 'Dudley' 'City of Nottingham' 'London' 'Civitella in Val di Chiana'\n",
      " 'City of New York' 'Malvern Hills' 'Harborough' 'Cambridge' 'London'\n",
      " 'Northampton' 'Northampton' 'Wigan' 'Worcester' 'N/A' 'London'\n",
      " 'West Suffolk' 'Newcastle-under-Lyme' 'London' 'Eastbourne' 'Bristol'\n",
      " 'Vancouver' 'London' 'Uttlesford' 'Denham' 'London' 'Bristol'\n",
      " 'City of New York' 'Mid Sussex' 'Sant Joan Desp√≠' 'London'\n",
      " 'North Hertfordshire' 'Birmingham' 'London' 'Melbourne' 'London' 'London'\n",
      " 'Pernes-les-Fontaines' 'Penzance' 'London' 'Jersey City' 'Corran'\n",
      " 'Venezia' 'London' 'Liverpool' 'Tuttlingen' 'Reading' 'Preston' 'Bristol'\n",
      " 'London' 'Dublin']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  cleaned_df.loc[valid_coords_mask, 'City'] = location_info.loc[valid_coords_mask, 'City']\n",
      "/var/folders/zf/w4lxv8m15z9442k97fpp97ch0000gn/T/ipykernel_15660/507719236.py:88: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'Schweiz/Suisse/Svizzera/Svizra' 'United Kingdom'\n",
      " 'United States' 'Australia' 'United Kingdom' 'Latvija' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'Maroc ‚µç‚µé‚µñ‚µî‚µâ‚¥± ÿßŸÑŸÖÿ∫ÿ±ÿ®' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'France' 'Deutschland' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'Italia'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United States' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'Deutschland' 'United States' 'United States' 'United States'\n",
      " 'United States' 'United States' 'United States' 'United States'\n",
      " 'Deutschland' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United States' 'United States' 'United States' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United States' 'United States'\n",
      " 'United States' 'United States' 'United Kingdom' 'Hrvatska'\n",
      " 'United States' 'Australia' 'Chile' 'United Kingdom' 'ŒïŒªŒªŒ¨œÇ'\n",
      " 'United Kingdom' 'United Kingdom' 'United States' 'United States'\n",
      " 'United Kingdom' 'United States' 'Suomi / Finland' 'United States'\n",
      " 'United States' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United States' 'United States' 'United States' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'Italia' 'Italia' 'United Kingdom'\n",
      " 'Italia' 'United Kingdom' 'United States' '–ë—ä–ª–≥–∞—Ä–∏—è' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United States'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'Danmark' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'Schweiz/Suisse/Svizzera/Svizra'\n",
      " 'Schweiz/Suisse/Svizzera/Svizra' 'Schweiz/Suisse/Svizzera/Svizra'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'Nederland'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'Deutschland'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United States' 'United Kingdom'\n",
      " 'United Kingdom' 'United States' 'United States' 'United States'\n",
      " 'United States' 'Nederland' '‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢' '‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢' 'United Kingdom'\n",
      " 'United States' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'Argentina' 'United Kingdom' 'France' 'France' 'Deutschland'\n",
      " 'United Kingdom' 'Nederland' 'United Kingdom' 'United States'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'Espa√±a' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United States'\n",
      " 'United States' 'Espa√±a' '√âire / Ireland' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'Deutschland' 'United States'\n",
      " 'United States' 'United States' 'United Kingdom' 'Canada'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United States' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United States' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'France' 'France'\n",
      " 'Schweiz/Suisse/Svizzera/Svizra' 'United States' 'United States'\n",
      " 'United States' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'France' 'United Kingdom' 'United Kingdom'\n",
      " 'United States' 'United States' 'United States' 'United Kingdom' 'ŒïŒªŒªŒ¨œÇ'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'Alg√©rie ‚µç‚µ£‚µ£‚¥∞‚µ¢‚¥ª‚µî ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ±' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United States' 'United States'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'Espa√±a' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'ŒöœçœÄœÅŒøœÇ - Kƒ±brƒ±s' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'Sverige' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United States' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United States' 'Slovenija' 'Suomi / Finland'\n",
      " 'Suomi / Finland' 'Nederland' 'Polska' 'Schweiz/Suisse/Svizzera/Svizra'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United States' 'United States' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United States' 'United States'\n",
      " 'United Kingdom' 'United Kingdom' 'South Africa' 'United States'\n",
      " 'United States' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'Italia'\n",
      " 'United States' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'Canada' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United States' 'United Kingdom'\n",
      " 'Espa√±a' 'United Kingdom' 'United Kingdom' 'United Kingdom'\n",
      " 'United Kingdom' 'Australia' 'United Kingdom' 'United Kingdom' 'France'\n",
      " 'United Kingdom' 'United Kingdom' 'United States' 'United Kingdom'\n",
      " 'Italia' 'United Kingdom' 'United Kingdom' 'Deutschland' 'United Kingdom'\n",
      " 'United Kingdom' 'United Kingdom' 'United Kingdom' '√âire / Ireland']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  cleaned_df.loc[valid_coords_mask, 'Country'] = location_info.loc[valid_coords_mask, 'Country']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "import time\n",
    "\n",
    "def is_valid_coordinate(value):\n",
    "    \"\"\"\n",
    "    Check if the value is a valid numeric coordinate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to float and check if it's a valid number\n",
    "        coord = float(value)\n",
    "        # Check if coordinate is within plausible range\n",
    "        # Latitude: -90 to 90, Longitude: -180 to 180\n",
    "        return -90 <= coord <= 90 or -180 <= coord <= 180\n",
    "    except (TypeError, ValueError):\n",
    "        return False\n",
    "\n",
    "def extract_location_from_coordinates(row, geolocator):\n",
    "    \"\"\"\n",
    "    Use geopy to extract city and country from coordinates\n",
    "    \"\"\"\n",
    "    # Check if both longitude and latitude are valid\n",
    "    if (is_valid_coordinate(row['Longitude']) and \n",
    "        is_valid_coordinate(row['Latitude'])):\n",
    "        try:\n",
    "            # Add a small delay to avoid overwhelming the geocoding service\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Reverse geocode\n",
    "            location = geolocator.reverse(f\"{row['Latitude']}, {row['Longitude']}\")\n",
    "            \n",
    "            if location:\n",
    "                # Extract address components\n",
    "                address = location.raw.get('address', {})\n",
    "                \n",
    "                # Try to extract city (with multiple fallback options)\n",
    "                city = (\n",
    "                    address.get('city') or \n",
    "                    address.get('town') or \n",
    "                    address.get('village') or \n",
    "                    address.get('municipality') or \n",
    "                    'N/A'\n",
    "                )\n",
    "                \n",
    "                # Extract country\n",
    "                country = address.get('country', 'N/A')\n",
    "                \n",
    "                return pd.Series({'City': city, 'Country': country})\n",
    "            \n",
    "        except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
    "            print(f\"Geocoding error for coordinates {row['Latitude']}, {row['Longitude']}: {e}\")\n",
    "    \n",
    "    # Return N/A if no valid location found\n",
    "    return pd.Series({'City': 'N/A', 'Country': 'N/A'})\n",
    "\n",
    "def clean_coordinates_and_extract_location(df):\n",
    "    \"\"\"\n",
    "    Clean Longitude and Latitude columns and extract location\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Initialize geolocator\n",
    "    geolocator = Nominatim(user_agent=\"studio_location_extractor\")\n",
    "    \n",
    "    # Check if both Longitude and Latitude are valid\n",
    "    cleaned_df['Valid_Coordinates'] = (\n",
    "        cleaned_df['Longitude'].apply(is_valid_coordinate) & \n",
    "        cleaned_df['Latitude'].apply(is_valid_coordinate)\n",
    "    )\n",
    "    \n",
    "    # For rows without valid coordinates, set to NaN\n",
    "    mask = ~cleaned_df['Valid_Coordinates']\n",
    "    cleaned_df.loc[mask, 'Longitude'] = np.nan\n",
    "    cleaned_df.loc[mask, 'Latitude'] = np.nan\n",
    "    \n",
    "    # Extract city and country from valid coordinates\n",
    "    location_info = cleaned_df.apply(\n",
    "        lambda row: extract_location_from_coordinates(row, geolocator), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Update City and Country columns for valid coordinates\n",
    "    valid_coords_mask = cleaned_df['Valid_Coordinates']\n",
    "    cleaned_df.loc[valid_coords_mask, 'City'] = location_info.loc[valid_coords_mask, 'City']\n",
    "    cleaned_df.loc[valid_coords_mask, 'Country'] = location_info.loc[valid_coords_mask, 'Country']\n",
    "    \n",
    "    # Drop the temporary validation column\n",
    "    cleaned_df = cleaned_df.drop(columns=['Valid_Coordinates'])\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('data/studios_data.csv')\n",
    "\n",
    "# Clean the dataframe\n",
    "cleaned_df = clean_coordinates_and_extract_location(df)\n",
    "\n",
    "# Save the cleaned dataframe\n",
    "cleaned_df.to_csv('data/studios_data_cleaned.csv', index=False)\n",
    "\n",
    "print(f\"Updated data saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b597250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of changes\n",
    "print(\"Original DataFrame:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['Longitude', 'Latitude', 'City', 'Country']].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02909c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCleaned DataFrame:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_df[['Longitude', 'Latitude', 'City', 'Country']].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701008e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few rows to verify\n",
    "print(\"\\nSample Rows:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e64ca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to studios_contact_info.csv\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_df[['Studio Name', 'Longitude', 'Latitude', 'City', 'Country']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada84fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
