{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2579f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Set, Tuple\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc704b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class StudioEquipmentScraper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_workers: int = 5,\n",
    "        rate_limit_delay: float = 1.0,\n",
    "        batch_size: int = 50,\n",
    "        verify_ssl: bool = True\n",
    "    ):\n",
    "        self.max_workers = max_workers\n",
    "        self.rate_limit_delay = rate_limit_delay\n",
    "        self.batch_size = batch_size\n",
    "        self.verify_ssl = verify_ssl\n",
    "        \n",
    "        # Equipment-related keywords\n",
    "        self.url_keywords = {\n",
    "            'equipment', 'gear', 'tech', 'hardware', 'spec', 'specs',\n",
    "            'control room', 'studio', 'live room', 'tracking', 'isolation booth', 'recording', \n",
    "            'equipment list', 'gear list', 'inventory','facilities', 'mic list',\n",
    "            'instruments', 'microphones', 'microphone', 'equip', 'technik', \n",
    "            'plugins', 'plug-ins', 'outboard', 'backline', 'desk', 'console', \n",
    "            'monitoring', 'monitors', 'speakers', \n",
    "            'equalizer', 'equaliser','compressors', 'kompressor', \n",
    "            'processors', 'pre-amps', 'preamps', \n",
    "            'tube', 'dynamic', 'condensers', 'ribbon'\n",
    "        }\n",
    "        \n",
    "        # Sample manufacturers and equipment (expand this list)\n",
    "        self.manufacturers = {\n",
    "            'neumann', 'shure', 'sennheiser', 'akg', 'trident',\n",
    "            'universal audio', 'chandler', 'yamaha', 'hammond',\n",
    "            'telefunken','neve','ssl', 'krk', 'avantone', 'adam Audio', \n",
    "            'genelec', 'pmc', 'quested', 'atc', 'barefoot sound', 'behringer',\n",
    "            'gml', 'manley', 'pultec', 'focusrite', 'presonus', 'shadow hills',\n",
    "            'thermionic culture', 'dbx', 'tube-tech', 'empirical labs', \n",
    "            'teletronix', 'fairchild', 'sennheiser', 'royer', 'schoeps', \n",
    "            'coles', 'earthworks audio', 'dpa', 'lewitt', 'focal', 'eve', \n",
    "            'hedd', 'api', 'avalon', 'burl', 'dangerous music','mäag', 'moog', \n",
    "            'equential', 'arturia', 'antelope audio', 'apogee', 'eventide', \n",
    "            'lexicon','bricasti design', 'radial engineering', 'heritage audio',\n",
    "            'protools', 'logic', 'tla', 'urei'\n",
    "        }\n",
    "        \n",
    "        self.equipment = {\n",
    "            'telefunken ela m 251', 'neumann u47', 'neumann u67','akg c12'\n",
    "            'neumann u87', 'neumann m49', 'telefunken u47', 'akg c414', \n",
    "            'coles 4038', 'schoeps cmC6','royer r-121', 'akg c414 eb', 'neumann km84',\n",
    "            'sennheiser md421', 'shure sm7b', 'electro-voice re20', 'neumann tlm103',\n",
    "            'shure sm57', 'shure SM58', 'fairchild 670', 'neve 2254', 'teletronix la2a',\n",
    "            'universal audio 1176ln', 'empirical labs distressor', 'tube-tech cl1b', \n",
    "            'ssl g bus compressor', 'dbx 160vu', 'api 2500', 'thermionic culture phoenix',\n",
    "            'manley vari-uu', 'focusrite red', 'neve 1073', 'api 312', 'neve 1073', 'api 3124+',\n",
    "            'api 512c', 'neve 1084', 'ssl superanalogue', 'universal audio 610', 'focusrite isa 110',\n",
    "            'shadow hills gama', 'chandler tg2', 'focusrite scarlett', 'presonus studio 24c', \n",
    "            'pultec eqp-1a', 'manley massive passive', 'api 550a', 'neve 1073', 'gml 8200',\n",
    "            'ssl 4000', 'api 550b', 'focusrite isa 110', 'chandler curve bender', 'behringer graphic',\n",
    "            'barefoot sound mm27', 'atc scm150asl pro', 'atc scm25a', 'focal twin 6 be',\n",
    "            'genelec 8351a', 'pmc bb5/xbd', 'quested hq210', 'focal sm9', 'genelec 1031a',\n",
    "            'yamaha ns10m', 'adan audio s3h', 'yamaha hs8', 'krk rokit', 'avantone mixcubes',\n",
    "            'flea 47', 'flea 12', 'josephson c700a', 'brauner vm1', 'aea r84', 'sony c800g', \n",
    "            'earthworks sr314', 'dpa 4011', 'pearlman tm-1', 'api 529', 'rupert neve designs portico ii', \n",
    "            'chandler limited rs124', 'bricasti m7', 'smart research c1', 'avedis ma5', \n",
    "            'rupert neve 5211',  'grace design m101', 'avalon vt-737sp', 'api 560', \n",
    "            'rupert neve 5033', 'mäag audio eq4', 'sontec mes-432c', 'atc scm45a pro', \n",
    "            'dynaudio core 59', 'hedd type 20', 'focal trio11 be', 'pmc mb3-xbd', \n",
    "            'burl b2 bomber adc', 'antelope audio orion 32', 'dangerous music monitor st', \n",
    "            'thermionic culture fat bustard'\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Configure session with retries and backoff\n",
    "        self.session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=5,\n",
    "            backoff_factor=0.5,\n",
    "            status_forcelist=[500, 502, 503, 504]\n",
    "        )\n",
    "        self.session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "        \n",
    "        # Proxy configuration (implement your proxy rotation logic here)\n",
    "        self.proxies = []\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text for matching.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def generate_equipment_variations(self, model: str) -> Set[str]:\n",
    "        \"\"\"Generate common variations of equipment names.\"\"\"\n",
    "        variations = {model}\n",
    "        \n",
    "        # Handle spacing variations\n",
    "        variations.add(model.replace(' ', ''))\n",
    "        variations.add(model.replace(' ', '-'))\n",
    "        variations.add(model.replace('-', ' '))\n",
    "        \n",
    "        # Handle common prefixes/suffixes\n",
    "        if any(char.isdigit() for char in model):\n",
    "            variations.add(f\"model {model}\")\n",
    "            variations.add(f\"type {model}\")\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def calculate_relevance_score(self, url: str, content: str) -> float:\n",
    "        \"\"\"Calculate relevance score for a page based on multiple criteria.\"\"\"\n",
    "        score = 0.0\n",
    "        normalized_content = self.normalize_text(content)\n",
    "        \n",
    "        # URL relevance\n",
    "        if any(keyword in url.lower() for keyword in self.url_keywords):\n",
    "            score += 0.3\n",
    "        \n",
    "        # Manufacturer mentions\n",
    "        manufacturer_matches = sum(\n",
    "            1 for m in self.manufacturers \n",
    "            if m in normalized_content\n",
    "        )\n",
    "        score += min(0.3, manufacturer_matches * 0.05)\n",
    "        \n",
    "        # Equipment mentions\n",
    "        equipment_matches = sum(\n",
    "            1 for e in self.equipment \n",
    "            if any(\n",
    "                variation in normalized_content \n",
    "                for variation in self.generate_equipment_variations(e)\n",
    "            )\n",
    "        )\n",
    "        score += min(0.4, equipment_matches * 0.05)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def extract_download_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        \"\"\"Extract potential equipment list download links.\"\"\"\n",
    "        download_links = []\n",
    "        \n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            text = link.get_text().lower()\n",
    "            \n",
    "            if not href:\n",
    "                continue\n",
    "                \n",
    "            # Skip floor plans\n",
    "            if 'floor' in text and 'plan' in text:\n",
    "                continue\n",
    "            \n",
    "            # Look for equipment-related download links\n",
    "            if any(keyword in text for keyword in self.url_keywords):\n",
    "                if href.endswith(('.pdf', '.doc', '.docx')):\n",
    "                    download_links.append(urljoin(base_url, href))\n",
    "        \n",
    "        return download_links\n",
    "    \n",
    "    def scrape_studio(self, studio_name: str, website: str) -> Dict:\n",
    "        \"\"\"Scrape a single studio website for equipment information.\"\"\"\n",
    "        result = {\n",
    "            'studio_name': studio_name,\n",
    "            'status': 'Inactive',\n",
    "            'error_message': '',\n",
    "            'equipment_page_url': '',\n",
    "            'download_link': '',\n",
    "            'confidence_score': 0.0,\n",
    "            'context': ''\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # First try with SSL verification\n",
    "            try:\n",
    "                response = self.session.get(\n",
    "                    website,\n",
    "                    timeout=30,\n",
    "                    verify=self.verify_ssl\n",
    "                )\n",
    "            except requests.exceptions.SSLError:\n",
    "                if self.verify_ssl:\n",
    "                    urllib3.disable_warnings()\n",
    "                    response = self.session.get(\n",
    "                        website,\n",
    "                        timeout=30,\n",
    "                        verify=False\n",
    "                    )\n",
    "            \n",
    "            result['status'] = 'Active'\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            base_url = response.url\n",
    "            \n",
    "            # Extract and analyze all internal links\n",
    "            internal_pages = []\n",
    "            for link in soup.find_all('a'):\n",
    "                href = link.get('href')\n",
    "                if not href:\n",
    "                    continue\n",
    "                    \n",
    "                full_url = urljoin(base_url, href)\n",
    "                if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
    "                    internal_pages.append(full_url)\n",
    "            \n",
    "            # Process each internal page\n",
    "            best_score = 0.0\n",
    "            best_url = ''\n",
    "            best_context = ''\n",
    "            \n",
    "            for page_url in internal_pages:\n",
    "                try:\n",
    "                    page_response = self.session.get(\n",
    "                        page_url,\n",
    "                        timeout=30,\n",
    "                        verify=self.verify_ssl\n",
    "                    )\n",
    "                    page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "                    \n",
    "                    # Calculate relevance score\n",
    "                    score = self.calculate_relevance_score(\n",
    "                        page_url,\n",
    "                        page_soup.get_text()\n",
    "                    )\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_url = page_url\n",
    "                        \n",
    "                        # Extract context (surrounding text of equipment mentions)\n",
    "                        context_elements = []\n",
    "                        for manufacturer in self.manufacturers:\n",
    "                            if manufacturer in page_soup.get_text().lower():\n",
    "                                for element in page_soup.find_all(['p', 'li', 'h1', 'h2', 'h3']):\n",
    "                                    if manufacturer in element.get_text().lower():\n",
    "                                        context_elements.append(element.get_text().strip())\n",
    "                        \n",
    "                        best_context = '; '.join(context_elements[:3])  # Limit context to first 3 matches\n",
    "                    \n",
    "                    # Look for download links\n",
    "                    download_links = self.extract_download_links(page_soup, base_url)\n",
    "                    if download_links:\n",
    "                        result['download_link'] = '; '.join(download_links)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error processing internal page {page_url}: {str(e)}\")\n",
    "                \n",
    "                time.sleep(self.rate_limit_delay)\n",
    "            \n",
    "            if best_score > 0.5:  # Minimum confidence threshold\n",
    "                result['equipment_page_url'] = best_url\n",
    "                result['confidence_score'] = best_score\n",
    "                result['context'] = best_context\n",
    "            \n",
    "        except Exception as e:\n",
    "            result['status'] = 'Inactive'\n",
    "            result['error_message'] = str(e)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_batch(self, studios: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process a batch of studios using parallel execution.\"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_studio = {\n",
    "                executor.submit(\n",
    "                    self.scrape_studio,\n",
    "                    studio['studio_name'],\n",
    "                    studio['website']\n",
    "                ): studio\n",
    "                for studio in studios\n",
    "            }\n",
    "            \n",
    "            results = []\n",
    "            for future in as_completed(future_to_studio):\n",
    "                results.append(future.result())\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def scrape_studios(self, input_file: str, output_file: str, detailed_output_file: str):\n",
    "        \"\"\"Main method to process all studios from CSV.\"\"\"\n",
    "        # Read input CSV\n",
    "        df = pd.read_csv(input_file)\n",
    "        studios = df.to_dict('records')\n",
    "        \n",
    "        # Process in batches\n",
    "        all_results = []\n",
    "        for i in range(0, len(studios), self.batch_size):\n",
    "            batch = studios[i:i + self.batch_size]\n",
    "            batch_results = self.process_batch(batch)\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # Progress tracking\n",
    "            logger.info(f\"Processed {min(i + self.batch_size, len(studios))}/{len(studios)} studios\")\n",
    "        \n",
    "        # Save basic results\n",
    "        pd.DataFrame(all_results).to_csv(output_file, index=False)\n",
    "        \n",
    "        # Save detailed results\n",
    "        detailed_results = [\n",
    "            {\n",
    "                'studio_name': r['studio_name'],\n",
    "                'equipment_page_url': r['equipment_page_url'],\n",
    "                'confidence_score': r['confidence_score'],\n",
    "                'context': r['context']\n",
    "            }\n",
    "            for r in all_results\n",
    "            if r['equipment_page_url']  # Only include studios with found equipment pages\n",
    "        ]\n",
    "        pd.DataFrame(detailed_results).to_csv(detailed_output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88ebe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 11:06:49,302 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x13dce7f90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /\n",
      "2025-02-10 11:09:31,304 - INFO - Processed 50/51 studios\n",
      "2025-02-10 11:09:31,747 - INFO - Processed 51/51 studios\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = StudioEquipmentScraper(\n",
    "        max_workers=5,\n",
    "        rate_limit_delay=1.0,\n",
    "        batch_size=50,\n",
    "        verify_ssl=True\n",
    "    )\n",
    "    \n",
    "    scraper.scrape_studios(\n",
    "        input_file='data/studio_websites_ii.csv',\n",
    "        output_file='data/studio_equip_lists/equip_search_output_ii.csv',\n",
    "        detailed_output_file='data/detailed/detailed_equip_search_ii.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208c548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
