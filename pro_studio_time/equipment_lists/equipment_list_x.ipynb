{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff43857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Set, Tuple\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#### Core Setup Methods ####\n",
    "\n",
    "class StudioEquipmentScraper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_workers: int = 1,\n",
    "        rate_limit_delay: float = 4.7,\n",
    "        batch_size: int = 30,\n",
    "        verify_ssl: bool = True,\n",
    "        use_selenium: bool = True,\n",
    "        max_pages_per_studio: int = 20\n",
    "    ):\n",
    "        self.max_workers = max_workers\n",
    "        self.rate_limit_delay = rate_limit_delay\n",
    "        self.batch_size = batch_size\n",
    "        self.verify_ssl = verify_ssl\n",
    "        self.use_selenium = use_selenium\n",
    "        self.visited_urls = set()  # Global URL cache\n",
    "        self.url_scores = {}  # Cache for URL scores\n",
    "        self.max_pages_per_studio = max_pages_per_studio\n",
    "\n",
    "        \n",
    "        # Equipment-related keywords (expanded)\n",
    "        self.url_keywords = {\n",
    "            'equipment', 'gear', 'tech', 'hardware', 'spec', 'specs','specification','specifications',\n",
    "            'control room', 'studio', 'studios', 'live room', 'tracking', 'isolation booth', 'recording', \n",
    "            'equipment list', 'gear list', 'inventory','facilities', 'resources', 'setup',\n",
    "            'instruments', 'microphones', 'microphone', 'equip', 'technik', \n",
    "            'plugins', 'plug-ins', 'outboard', 'backline', 'desk', 'console', 'deck', 'decks',\n",
    "            'monitoring', 'monitors', 'speakers', \n",
    "            'equalizer', 'equaliser','compressors', 'kompressor', \n",
    "            'processors', 'pre-amps', 'preamps', \n",
    "            'tube', 'dynamic', 'condensers', 'ribbon', 'valve'\n",
    "        }\n",
    "        \n",
    "        self.equipment_hierarchy = {\n",
    "            'acoustic energy': ['ae2'],\n",
    "            'adam': ['a7'],\n",
    "            'adl': ['1000'],\n",
    "            'adr': ['compex 2'],\n",
    "            'aea': ['ku5a', 'r84', 'r84a', 'r88', 'r88 mk ii', 'a440', 'rpq500'],\n",
    "            'akg': ['414','454','c12', 'c12vr', 'c28b', 'c3000', 'c414', 'c414 eb', 'c414b-uls', \n",
    "                    'c414-xls', 'c414b-xls','c451', 'c451b', 'c451e', 'd112', 'd112e', \n",
    "                    'd12e', 'd224', 'd196','bx25', 'the tube'],\n",
    "            'alan smart': ['c1', 'c2'],\n",
    "            'alesis': ['adats', 'quadraverb', 'midiverb ii', '3630'],\n",
    "            'altec': ['rs124'],\n",
    "            'amek': ['9098', '9098i'],\n",
    "            'ams': ['dm 2-20', 'dmx', 'dmx15-80', 'rmx 16'],\n",
    "            'ams neve': ['1073opx', '1073opx', '1081', '33115', '33609/j', \n",
    "                         '88rs sp2', 'montserrat'],\n",
    "            'antelope': ['orion 32+'],\n",
    "            'anthony demaria labs': ['1000'],\n",
    "            'aphex': ['104', '662'],\n",
    "            'api': [ '2500', '312', '3124+', '512c', '529', '550a', \n",
    "                    '550b', '560', '525', '1608','lunchbox'],\n",
    "            'aston': ['stealth', 'spirit'],\n",
    "            'atc': ['scm25a', 'scm50'],\n",
    "            'audio technica': ['4033','4050','2040','at4033', 'at4050', 'at2040', 'atm31'],\n",
    "            'audix': ['om-6', 'd1','d6'],\n",
    "            'auratone': ['5c super sound', 'c5'],\n",
    "            'avalon': ['m2', 'm5','vt-737sp', 'ad2022', 'vt-747sp', 'ad2044'],\n",
    "            'avantone': ['mixcubes', 'cv-12','cla200'],\n",
    "            'b&k': ['4006','801 d4', '805 d3', '805 d4', '805 matrix', 'db1d', 'htm 1','800 d3'],\n",
    "            'barefoot sound': ['mm27', 'footprint01', 'micro-main27'],\n",
    "            'bbe': ['862 sonic maximizer','sonic maximizer 822'],\n",
    "            'bel electronics': ['bde 2400','bf-20'],\n",
    "            'bel': ['bde 2400','bf-20'],\n",
    "            'behringer': ['snr2000'],\n",
    "            'beyer': ['m500', 'm160', 'm201tg'],\n",
    "            'beyerdynamic': ['m500', 'm160', 'm-201', 'm-69', 'm-88', 'm-201tg'],\n",
    "            'black lion audio': ['revolution'],\n",
    "            'blue': ['reactor ldc'],\n",
    "            'brauner': ['vm 1'],\n",
    "            'brent averill': ['1073'],\n",
    "            'bricasti design': ['m7', 'm10'],\n",
    "            'bruel & kjaer (dpa)': ['4003', '4006', '4006 (tlx)',\n",
    "                                    '4007', '4011', '4015', '4061'],\n",
    "            'bss': ['dpr-402'],\n",
    "            'burl': ['b2 bomber adc', 'b80 mothership'],\n",
    "            'cad': ['trion'],\n",
    "            'calrec': ['cm1050c', 'cm4050'],\n",
    "            'cartec': ['pre-q5'],\n",
    "            'cascade': ['fathead', 'fathead ii lundahl', '731r', 'dr2', 'dr2s', 'x15'],\n",
    "            'cathedral pipes': ['regensburg dom'],\n",
    "            'charteroak': ['s700'],\n",
    "            'chandler': ['tg2', 'curve bender', 'rs124', 'tg-1','tg2-500','summing mixer','660',\n",
    "                         'germ compressor', 'redd.47', 'germanium','redd', 'tg', 'tg l', 'tg eq'],\n",
    "            'coastal acoustics': ['boxer t5', '5'],\n",
    "            'coles': ['4038'],\n",
    "            'conor audio': ['1107'],\n",
    "            'core sound': ['octo'],\n",
    "            'cortado': ['mkiii'],\n",
    "            'cranborne audio': ['camden 500'],\n",
    "            'crown': ['pzm','pzm-30 gpb','sass-p mkii'],\n",
    "            'daking': ['52270', '91579'],\n",
    "            'dangerous music': ['monitor st', 'bax eq', 'convert-2'],\n",
    "            'dbx': ['120a','120xp ', '160vu', '160xt', '160a', '165a',\n",
    "                    '160x', '160xt','166a','760x', '902', '903', '904'],\n",
    "            'deltalab': ['effectron ii'],\n",
    "            'drawmer': ['1960', 'ds-201', '201','ds201', 'm500', \n",
    "                        'vacuum tube 1960 compressors'],\n",
    "            'dpa': ['4011', '4006', '2011c'],\n",
    "            'dytronics': ['cyclosonic'],\n",
    "            'earthworks': ['dx7 dm20', 'dx7 sr25', 'pm40', 'qtc 40mp'],\n",
    "            'echoplex': ['ep3'],\n",
    "            'electro-voice': ['ev 1777', 're-15', 're-16', 're-18', 're-20', \n",
    "                              're-55', 're320', 'pl20', 're2000', '408', '641'],\n",
    "            'emi': ['(ribbon) rm1b', 'redd.17', 'tg 12412', 'tg 12413', 'tg 12416r', 'tg12345 mkii'],\n",
    "            'emotiva': ['stealth 6','stealth 8','pro stealth 6','pro stealth 8'],\n",
    "            'empirical labs': ['distressor', 'el8 distressors', \n",
    "                               'fatso jr', 'derresser'],\n",
    "            'emt': ['140','240','250','445'],\n",
    "            'event electronics': ['studios precisions'],\n",
    "            'eventide': ['h3000', 'h9', 'h8000fw', 'h3500','harmonizer model h949',\n",
    "                         'dsp4000', 'h9000', 'omnipressor', 'h910','clockworks model h910',\n",
    "                         'clockworks instant phaser'],\n",
    "            'fairchild': ['670', '660', '663'],\n",
    "            'federal': ['am864/u'],\n",
    "            'fender': ['fr1000'],\n",
    "            'flea': ['47', '12', '49'],\n",
    "            'focal': ['twin 6 be', 'sm9', 'trio11 be', 'shape 65','shape'],\n",
    "            'focusrite': ['isa 110', 'red 3', 'red 8pre','scarlett 18i20', 'isa 828', \n",
    "                          '315', '215','isa 215', 'platinum voicemaster'],\n",
    "            'funkberater': ['md30'],\n",
    "            'gefel': ['um-92.1s','mv102','um705'],\n",
    "            'geffell': ['um-92.1s','mv102','um705'],\n",
    "            'genelec': ['8351a', '1031a', '1032','1032a', '8260a', '8030c'],\n",
    "            'grace design': ['m101', 'm905'],\n",
    "            'groove tubes': ['the glory comp'],\n",
    "            'gml': ['8200', '8304', '8300'],\n",
    "            'hairball audio': ['lola'],\n",
    "            'hedd': ['type 20', 'type 30'],\n",
    "            'Heil': ['pr-20','pr-20'],\n",
    "            'inovonics': ['model 201'],\n",
    "            'jbl': ['4401','lsr28p','lsr6823p'],\n",
    "            'josephson': ['c700a', 'e22s', 'c42'],\n",
    "            'kii': ['three'],\n",
    "            'klark technik': ['dn27'],\n",
    "            'korg': ['a3'],\n",
    "            'krk': ['rokit 5', 'rokit 8', 'v8','7000b', '9000', 'e8', 'rok bottom sub'],\n",
    "            'ksm': ['353'],\n",
    "            'lang': ['peq-2'],\n",
    "            'lauten': ['ls-308'],\n",
    "            'lee jackson': ['big iron'],\n",
    "            'lewitt': ['lct 640 ts', 'lct 1040', 'pure tube'],\n",
    "            'lexicon': ['200', '224x','300','480l','224xl','960l', 'lxp-1',\n",
    "                        'lxp-5','pcm42','pcm70','pcm80','pcm91','prime time'],\n",
    "            'm&k': ['mps-2510'],\n",
    "            'mackie': ['hr624','hr824'],\n",
    "            'maestro': ['echoplex ep-3'],\n",
    "            'manley': ['vari-mu', 'massive passive', 'gold reference',\n",
    "                       'core', 'reference cardioid', 'vox box', 'pultec','enhanced pultec eqp-1a'],\n",
    "            'mäag audio': ['eq4', 'preq2'],\n",
    "            'marshall': ['5002 “a”'],\n",
    "            'mark-o-matic': ['mpag-d','c-12'],\n",
    "            'mercenary': ['km69'],\n",
    "            'micmix': ['master room mr-ii'],\n",
    "            'microtech gefell': ['m70','m900','m71'],\n",
    "            'millennia media': ['4-Channel Preamp'],\n",
    "            'mojave': ['ma 300'],\n",
    "            'mutronics': ['mutator'],\n",
    "            'mxr': ['ø1'],\n",
    "            'neumann': ['582','km53','km 54','km 56','km 64','km83','km84','km86','km84i',\n",
    "                        'km130','km140','km184','m 49','m 50','m147','m 250','m 269',\n",
    "                        'mk221','tlm103','tlm50','tlm127','mv102','sm 2','tlm170', \n",
    "                        'u47','u57','u67','u87','u87i','u 89 ai','um705'],\n",
    "            'neve': ['88rlb','88rs sp2','1073','1073opx','1084','1081','2254','33115',\n",
    "                     '33609','33609/j','33609/c','34628','5052','582','2264x','portico 5024',\n",
    "                     '9098i','vr60 legend','monserrat'],\n",
    "            'ns-10': ['subkick'],\n",
    "            'oktava': ['mk-219', 'ml-52'],\n",
    "            'osm': ['os800'],\n",
    "            'otari': ['mtr 10'],\n",
    "            'pcm': ['81', '91'],\n",
    "            'pearlman': ['tm-1', 'churcho mic'],\n",
    "            'peavey': ['Univerb'],\n",
    "            'presonus': ['studio 24c', 'eureka channel strip'],\n",
    "            'prism': ['maselec series mla-2'],\n",
    "            'proac studio': ['100'],\n",
    "            'pultec': ['eqp-1a', 'meq-5', 'peq-2','hlf-3c'],\n",
    "            'quantec': ['qrs/l'],\n",
    "            'quested': ['hq210', 'vh3208'],\n",
    "            'radial engineering': ['jdi', 'j48', 'pro48', 'd2', 'jdx'],\n",
    "            'rca': ['ba-6a', '(ribbon) 3043', '(ribbon) bx-44', '(ribbon) dx-77'],\n",
    "            'ridge farm': ['gas cooker'],\n",
    "            'rme': ['babyface pro', 'fireface ufx', 'adi-2 pro'],\n",
    "            'rode': ['nt1','nt2-a','nt1-a','nt-2'],\n",
    "            'roger mayer': ['456'],\n",
    "            'roland': ['re-501','chorus echo re-501','dc-30','e-660',\n",
    "                       'sde 2500','sde 330','sde 3000','sph 323',\n",
    "                       'srv 330','dimension d', 'gp-8 ', 're 201', 'rsp 550'],\n",
    "            'rode': ['nt1', 'nt2-a', 'nt-2', 'nt1-a'],\n",
    "            'roswell audio': ['k47'],\n",
    "            'royer': ['121','122','r-121', 'r-122', 'r-122 mkII', 'sf-12', 'r10', 'sf-24'],\n",
    "            'rupert neve designs': ['portico', 'portico ii', '5033', '5211', '5088'],\n",
    "            'sanken': ['cms 2', 'cu 41'],\n",
    "            'sansamp tech': ['21 nyc'],\n",
    "            'schoeps': ['cmc6', 'mk41','cmt 54', 'cmt 540', 'cmt 541', 'cmts 501 u', \n",
    "                        'ka 40', 'mk 2', 'mk 21', 'mk 2h', 'mk 2s', 'mk 4', \n",
    "                        'mk 41', 'mk 5', 'mk 6', 'mstc5'],\n",
    "            'se': ['4400a','rnr1','z5600'],\n",
    "            'sebatron': ['vmp quad plus'],\n",
    "            'sennheiser': ['421','441','ambeo vr','e609','e904','e906','gooseneck',\n",
    "                           'md409 u3','md421','md421-n','md421-u','md441','mkh20',\n",
    "                           'mkh40','mkh105','mkh404','mkh405','mkh416'],\n",
    "            'shadow hills': ['gama', 'mastering compressor', 'mono gama'],\n",
    "            'sherman': ['filterbank'],\n",
    "            'shure': ['55sh', '510', '520dx', '545sd','beta 52a','beta 52','beta 57','beta 57a', \n",
    "                      'beta 578a','beta 98amp','ksm32','ksm44','ksm141','sm7','sm7b','sm54',\n",
    "                      'sm56','sm57','sm58','sm81','sm91 pzm','super 55'],\n",
    "            'sintefex': ['fx8000'],\n",
    "            'smart research': ['c1', 'c2'],\n",
    "            'sontec': ['mes-432c'],\n",
    "            'sontronics': ['apollo', 'apollo 2', 'aria', 'corona', 'delta', 'delta 2', \n",
    "                           'dm-1b', 'halo', 'helios','mercury', 'omega','orpheus', \n",
    "                           'saturn', 'sigma', 'sigma 2','stc-1'],\n",
    "            'sony': ['a7 dat','c800g', 'c37a','c 37p', 'c 38', 'ecm 150', 'emc 22p'],\n",
    "            'spectrasonics': ['610'],\n",
    "            'spl': ['transient designer','transient designer 4'],\n",
    "            'ssl': ['g bus compressor', 'superanalogue','bus', 'bus +',\n",
    "                    '4000', '9000j', 'fusion', '4056 g+','fx g384'],\n",
    "            'stager': ['sr-2n'],\n",
    "            'stc/coles': ['4021 e', '4032 e', '4033', '4038', 'stereo pair & bar'],\n",
    "            'sterling': ['st31'],\n",
    "            'summit': ['tpa-200','eqp-200','dcl-200', 'tpa-200a', ' 2290'],\n",
    "            'summit audio': ['tpa-200','eqp-200','dcl-200', 'tpa-200a', ' 2290'],\n",
    "            't.c. electronic': ['fireworx','m2000','m3000','m5000','2290', 'system 6000', 'tc 1128'],\n",
    "            'tc electronic': ['fireworx','m2000','m3000','m5000','2290', 'system 6000', 'tc 1128'],\n",
    "            'tab funkenwerk': ['u67'],\n",
    "            'tannoy': ['little gold monitors'],\n",
    "            'teegarden audio': ['ppc-125'],\n",
    "            'telefunken': ['ela m 251', 'u47', 'ak47','cu-29 copperhead', \n",
    "                           'tf39', 'u48','m 81', 'm12f'],\n",
    "            'teletronix': ['la2a','la-2a'],\n",
    "            'tl audio': ['eq-2 parametric', 'pp 10', 'tla n1'],\n",
    "            'thermionic culture': ['phoenix', 'fat bustard', \n",
    "                                   'vulture', 'earlybird', 'pullet'],\n",
    "            'tree audio': ['branch ii'],\n",
    "            'trident': ['88'],\n",
    "            'tube-tech': ['cl1b', 'pe-1c', 'pe-1b','cl 1a', 'cl 1b', 'lca 2b'],\n",
    "            'universal audio': ['1176','1176ln', '610','la-2a','la-610 mk ii', '2-610','dbx 160','dbx 160a'],\n",
    "            'urei': ['545','1176','1175b','1176 ln','1178','1178ln','la-10','la-4'],\n",
    "            'valley people': ['dyna-mite'],\n",
    "            'vintech audio': ['x-81', 'x73i'],\n",
    "            'vintech': ['x-81', 'x73i'],\n",
    "            'vitavox': ['b 50'],\n",
    "            'ward beck': ['M441'],\n",
    "            'warm': ['76','251','414','412','8000', 'bus comp','eqp wa','u47','u67','u87',\n",
    "                     'wa14','wa2a','wa44','wa412','wa73','wa76','wa87'],\n",
    "            'warm audio': ['76','251','414','412','8000', 'bus comp','eqp wa','u47','u67','u87',\n",
    "                     'wa14','wa2a','wa44','wa412','wa73','wa76','wa87'],\n",
    "            'wem': ['Copicat'],\n",
    "            'yamaha': ['ns10m', 'hs8', 'spx90', 'spx90ii','spx1000s','rev7', 'pro r3', 'yst-sw200']\n",
    "        }\n",
    "  \n",
    "        self.equipment = set()\n",
    "        for manufacturer, models in self.equipment_hierarchy.items():\n",
    "            # Add full names (manufacturer + model)\n",
    "            self.equipment.update(f\"{manufacturer} {model}\" for model in models)\n",
    "            # Add model numbers alone\n",
    "            self.equipment.update(models)\n",
    "\n",
    "        # Terms that should score higher when used as headers or links\n",
    "        self.header_boost_terms = {\n",
    "            'equipment list': 3.0, 'gear list': 3.0, 'microphone list': 2.5,\n",
    "            'equipment': 2.0, 'gear': 2.0, 'outboard': 2.0, 'tech specs': 2.0,\n",
    "            'specifications': 1.5, 'studio specs': 1.5, 'microphones': 1.0, 'valve': 1.0,\n",
    "            'tube': 1.0, 'condensers': 1.5, 'ribbon': 1.0, 'hardware': 1.0, 'monitors': 1.0, \n",
    "            'compressors': 0.5, 'preamps': 0.5, 'equalisers': 0.5, 'equalizers': 0.5,\n",
    "            'processors': 0.4, 'plugins': 0.2, \n",
    "        }\n",
    "\n",
    "        \n",
    "        self.url_score_weights = {\n",
    "            # High priority terms\n",
    "            'equipment list': 20.0, 'gear list': 20.0, 'microphone list': 20.0, 'mic list': 20.0,\n",
    "            # Medium-high priority terms\n",
    "            'gear': 10.0, 'equipment': 10.0, 'inventory': 10.0, 'microphones': 8.0, 'tech spec': 9.0,\n",
    "            'studio': 8.5, 'studios': 8.5, 'microphone': 7.0,\n",
    "            # Medium priority terms\n",
    "            'specs': 6.5, 'spec': 6.5, 'technik': 6.5,'studo': 6.0,'instruments': 5.0, \n",
    "            # Lower priority terms\n",
    "            'list': 4.0,\n",
    "            'console': 0.7, 'monitors': 0.7, 'desk': 0.7, 'mic locker': 0.7,\n",
    "            'monitoring': 0.6, 'speakers': 0.6, 'outboard': 0.6, 'hardware': 0.6, 'backline': 0.6,\n",
    "            'compressors': 0.5, 'preamps': 0.5, 'equalisers': 0.5, 'equalizers': 0.5,\n",
    "            'processors': 0.4,\n",
    "            'live room': 0.3, 'tracking': 0.3, 'control room': 0.3,\n",
    "            'tech': 0.3, 'condensers': 0.3, 'tube': 0.3, 'ribbon': 0.3, 'valve': 0.3,\n",
    "            'setup': 0.2, 'isolation booth': 0.1, 'services': 0.05,\n",
    "            'facilities': 0.01, 'recording': 0.01,\n",
    "            'tour': -3.0\n",
    "        }\n",
    "\n",
    "        # Terms to ignore/skip\n",
    "        self.ignore_terms = {\n",
    "            'clients', 'blog', 'contact', 'location', 'meet', 'team', 'staff', 'music', 'techniques',\n",
    "            'news', 'rates', 'calendar', 'experience', 'restaurant', 'hotel', 'sleep', 'content', \n",
    "            'cafe', 'shop', 'gallery', 'merch', 'merchandise', 'past', 'previous', 'demo', 'package',\n",
    "            'work', 'cart', 'artists', 'acts', 'bands', 'policies', 'conditions', 'listen','quote', \n",
    "            'sounds', 'accommodation', 'links', 'photos', 'records', 'event', 'events','community',\n",
    "            'visit', 'online', 'learn', 'teach', 'study', 'institute', 'mastering', 'archive', \n",
    "            'filming', 'streaming', 'story', 'press', 'talk', 'newsletter', 'chatting', 'chat', \n",
    "            'jobs', 'careers', 'film', 'engineers', 'history', 'guests', 'guest book', 'label',\n",
    "            'publishing', 'management', 'resident', 'residential', 'covid', 'book', 'booking', \n",
    "            'mailto:', 'tel:', 'virtual', 'project', 'food', 'drink', 'snacks', 'lounge', \n",
    "            'workshop', 'program', 'school', 'login', 'retreat', 'producer', 'floor plan', \n",
    "            'photography', 'privacy', 'policy', 'touch', 'party', 'parties', 'sitemap','art',\n",
    "            'tips', 'advice', 'expert', 'singer', 'songwriter', 'writing', 'songwriting','payment',\n",
    "            'cinema', 'visual', 'entertainment', 'preparing', 'prepare', 'pricing', '.jpg',\n",
    "            'voice','voice over','discography', 'samples','radio','affiliation','sponsor', 'virual',\n",
    "            'tab','tabs','searchbox','widget','subtitling'\n",
    "        }\n",
    "\n",
    "        # Content scoring weights\n",
    "        self.content_score_weights = {\n",
    "            'instruments': 0.5, 'microphone': 0.5, 'console': 0.5, 'monitors': 1.5, 'desk': 1.5, \n",
    "            'compressors': 1.5, 'equalisers': 1.5, 'equalizers': 1.5, \n",
    "            'processors': 0.5, 'outboard': 1.0,\n",
    "            'condensers': 1.0, 'tube': 1.0, 'ribbon': 1.0, 'preamps': 1.2, 'valve': 1.0,\n",
    "            'backline': 1.0, 'plugins': 0.5, 'converters': 0.5, 'daws': 0.5, 'plug ins': 0.5,\n",
    "            'synths': 0.3, 'guitars': 0.2, 'electric piano': 0.3, 'electric keyboards': 0.3,\n",
    "            'electric organ': 0.3, 'pedals': 0.3\n",
    "        }\n",
    "\n",
    "        # Equipment and manufacturer scoring weights\n",
    "        self.equipment_score = 6.5\n",
    "        self.manufacturer_score = 2.5\n",
    "        \n",
    "        # Configure requests session\n",
    "        self.session = self._setup_requests_session()\n",
    "        \n",
    "        # Configure Selenium if enabled\n",
    "        if self.use_selenium:\n",
    "            self.driver = self._setup_selenium_driver()\n",
    "        \n",
    "        # Proxy configuration\n",
    "        self.proxies = []\n",
    "    \n",
    "    def _setup_requests_session(self):\n",
    "        \"\"\"Configure requests session with proper connection pooling.\"\"\"\n",
    "        session = requests.Session()\n",
    "        # Create separate connection pools for different domains\n",
    "        adapter = HTTPAdapter(\n",
    "            pool_connections=25,\n",
    "            pool_maxsize=25,\n",
    "            pool_block=False,  # Don't block when pool is full\n",
    "            max_retries=Retry(\n",
    "                total=5,\n",
    "                backoff_factor=0.5,\n",
    "                status_forcelist=[500, 502, 503, 504]\n",
    "            )\n",
    "        )\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "        return session\n",
    "    \n",
    "    def _setup_selenium_driver(self):\n",
    "        \"\"\"Configure Selenium WebDriver with appropriate options.\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        return webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.use_selenium:\n",
    "            self.driver.quit()  \n",
    "\n",
    "#### Main Processing Pipeline ####\n",
    "\n",
    "    def scrape_studios(self, input_file: str, output_file: str, detailed_output_file: str):\n",
    "        \"\"\"Main method to process all studios from CSV.\"\"\"\n",
    "        # Read input CSV\n",
    "        df = pd.read_csv(input_file)\n",
    "        studios = df.to_dict('records')\n",
    "        \n",
    "        # Process in batches\n",
    "        all_results = []\n",
    "        for i in range(0, len(studios), self.batch_size):\n",
    "            batch = studios[i:i + self.batch_size]\n",
    "            batch_results = self.process_batch(batch)\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # Progress tracking\n",
    "            logger.info(f\"Processed {min(i + self.batch_size, len(studios))}/{len(studios)} studios\")\n",
    "        \n",
    "        # Save basic results\n",
    "        pd.DataFrame(all_results).to_csv(output_file, index=False)\n",
    "        \n",
    "        # Save detailed results\n",
    "        detailed_results = [\n",
    "            {\n",
    "                'studio_name': r['studio_name'],\n",
    "                'equipment_page_url': r['equipment_page_url'],\n",
    "                'confidence_score': r['confidence_score'],\n",
    "                'context': r['context']\n",
    "            }\n",
    "            for r in all_results\n",
    "            if r['equipment_page_url']  # Only include studios with found equipment pages\n",
    "        ]\n",
    "        pd.DataFrame(detailed_results).to_csv(detailed_output_file, index=False)\n",
    "    \n",
    "    def process_batch(self, studios: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process a batch of studios using parallel execution.\"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_studio = {\n",
    "                executor.submit(\n",
    "                    self.scrape_studio,\n",
    "                    studio['studio_name'],\n",
    "                    studio['website']\n",
    "                ): studio\n",
    "                for studio in studios\n",
    "            }\n",
    "            \n",
    "            results = []\n",
    "            for future in as_completed(future_to_studio):\n",
    "                results.append(future.result())\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def scrape_studio(self, studio_name: str, website: str) -> Dict:\n",
    "        result = {\n",
    "            'studio_name': studio_name,\n",
    "            'website': website,\n",
    "            'status': 'Inactive',\n",
    "            'error_message': '',\n",
    "            'equipment_page_url': '',\n",
    "            'download_link': '',\n",
    "            'confidence_score': 0.0,\n",
    "            'context': '',\n",
    "            'structured_content': []\n",
    "        }\n",
    "\n",
    "        equipment_pages = set()\n",
    "        pages_checked = 0\n",
    "\n",
    "        try:\n",
    "            pages_to_check = [(website, 0.0)]\n",
    "            base_domain = urlparse(website).netloc\n",
    "            visited_urls = set()\n",
    "\n",
    "            while pages_to_check and pages_checked < self.max_pages_per_studio:\n",
    "                current_url, base_score = pages_to_check.pop(0)\n",
    "\n",
    "                # Skip if already visited\n",
    "                if current_url in visited_urls:\n",
    "                    continue\n",
    "\n",
    "                # First check if URL should be ignored\n",
    "                url_lower = current_url.lower()\n",
    "                if any(term in url_lower for term in self.ignore_terms):\n",
    "                    continue\n",
    "\n",
    "                # Calculate URL score before making request\n",
    "                url_score = self.calculate_url_score(current_url)\n",
    "                if url_score < 0:  # Skip if negative score\n",
    "                    continue\n",
    "\n",
    "                visited_urls.add(current_url)\n",
    "                pages_checked += 1\n",
    "\n",
    "                try:\n",
    "                    if self.use_selenium:\n",
    "                        page_content = self.scrape_with_selenium(current_url)\n",
    "                    else:\n",
    "                        response = self.session.get(current_url, timeout=30, verify=self.verify_ssl)\n",
    "                        page_content = response.text\n",
    "\n",
    "                    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "                    result['status'] = 'Active'\n",
    "\n",
    "                    # Calculate total page score\n",
    "                    page_score = url_score  # Start with URL score\n",
    "                    structured_content = self.extract_structured_content(soup)\n",
    "                    download_links = self.extract_download_links(soup, current_url)\n",
    "\n",
    "                    # Add content-based scoring\n",
    "                    if structured_content:\n",
    "                        page_score += self.calculate_content_score(structured_content)\n",
    "                    if download_links:\n",
    "                        page_score += 0.2\n",
    "                        result['download_link'] = '; '.join(download_links)\n",
    "\n",
    "                    # Only add pages with positive scores\n",
    "                    if page_score > 0:\n",
    "                        equipment_pages.add((current_url, page_score))\n",
    "                        self.url_scores[current_url] = page_score\n",
    "\n",
    "                    # Only process new internal links if we haven't hit our limit\n",
    "                    if pages_checked < self.max_pages_per_studio:\n",
    "                        for link in soup.find_all('a'):\n",
    "                            href = link.get('href')\n",
    "                            if href:\n",
    "                                full_url = urljoin(current_url, href)\n",
    "                                parsed_url = urlparse(full_url)\n",
    "                                if (parsed_url.netloc == base_domain and \n",
    "                                    full_url not in visited_urls and\n",
    "                                    not any(term in full_url.lower() for term in self.ignore_terms)):\n",
    "                                    pages_to_check.append((full_url, 0.0))\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error processing {current_url}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if equipment_pages:\n",
    "                # Sort by score and filter out pages below threshold\n",
    "                sorted_pages = sorted(equipment_pages, key=lambda x: x[1], reverse=True)\n",
    "                filtered_pages = [(url, score) for url, score in sorted_pages if score >= 290]\n",
    "\n",
    "                if filtered_pages:\n",
    "                    result['equipment_page_url'] = '; '.join(\n",
    "                        f\"{url} ({score:.2f})\" for url, score in filtered_pages\n",
    "                    )\n",
    "                    result['confidence_score'] = max(score for _, score in filtered_pages)\n",
    "                else:\n",
    "                    # If no pages meet the threshold, leave equipment_page_url empty\n",
    "                    result['equipment_page_url'] = ''\n",
    "                    result['confidence_score'] = max(score for _, score in sorted_pages) if sorted_pages else 0.0\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            result['status'] = 'Inactive'\n",
    "            result['error_message'] = str(e)\n",
    "            return result\n",
    "\n",
    "#### URL and Content Processing ####\n",
    "        \n",
    "    def calculate_url_score(self, url: str) -> float:\n",
    "        \"\"\"Calculate URL score using only the highest scoring term found.\"\"\"\n",
    "        # Handle URL fragments\n",
    "        url = url.split('#')[0]  # Remove fragment before processing\n",
    "\n",
    "        url_lower = url.lower()\n",
    "        parsed_url = urlparse(url)\n",
    "        path_parts = parsed_url.path.split('/')\n",
    "\n",
    "        # Early exit for ignored terms\n",
    "        if any(term in url_lower for term in self.ignore_terms):\n",
    "            return -1.0\n",
    "\n",
    "        score = 0.0\n",
    "        highest_term_score = 0.0\n",
    "\n",
    "        # Check navigation elements for header boost terms\n",
    "        if path_parts:\n",
    "            nav_element = path_parts[-1].lower() if path_parts[-1] else (\n",
    "                path_parts[-2].lower() if len(path_parts) > 1 else '')\n",
    "\n",
    "            # Header boost terms are still all counted as they indicate structure\n",
    "            for term, boost in self.header_boost_terms.items():\n",
    "                if term.replace(' ', '-') in nav_element or term.replace(' ', '') in nav_element:\n",
    "                    score += boost\n",
    "\n",
    "        # Find highest scoring term from url_score_weights\n",
    "        for term, weight in self.url_score_weights.items():\n",
    "            if term in url_lower:\n",
    "                highest_term_score = max(highest_term_score, weight)\n",
    "\n",
    "        # Add only the highest term score to the total\n",
    "        score += highest_term_score\n",
    "\n",
    "        return score\n",
    "    \n",
    "    def calculate_content_score(self, structured_content: List[Dict]) -> float:\n",
    "        \"\"\"Calculate score based on structured content with improved equipment detection.\"\"\"\n",
    "        score = 0.0\n",
    "        term_counts = {}\n",
    "        equipment_counts = {}\n",
    "        manufacturer_counts = {}\n",
    "\n",
    "        for section in structured_content:\n",
    "            header_text = self.normalize_text(section['header'])\n",
    "            content_text = self.normalize_text(' '.join(section['content']))\n",
    "\n",
    "            # Header boost terms remain unchanged\n",
    "            for term, boost in self.header_boost_terms.items():\n",
    "                if term in header_text:\n",
    "                    score += boost\n",
    "\n",
    "            # Content type scoring remains unchanged\n",
    "            if section['type'] == 'list' and len(section['content']) > 3:\n",
    "                score += 0.5\n",
    "            elif section['type'] == 'table' and len(section['content']) > 3:\n",
    "                score += 0.75\n",
    "\n",
    "            # Enhanced equipment detection\n",
    "            for manufacturer, models in self.equipment_hierarchy.items():\n",
    "                normalized_manufacturer = self.normalize_text(manufacturer)\n",
    "\n",
    "                # Check manufacturer presence\n",
    "                if normalized_manufacturer in header_text or normalized_manufacturer in content_text:\n",
    "                    manufacturer_counts[manufacturer] = manufacturer_counts.get(manufacturer, 0) + 1\n",
    "                    if manufacturer_counts[manufacturer] <= 6:\n",
    "                        score += self.manufacturer_score\n",
    "\n",
    "                # Check each model with variations\n",
    "                for model in models:\n",
    "                    variations = self.generate_equipment_variations(manufacturer, model)\n",
    "                    equipment_key = f\"{manufacturer} {model}\"\n",
    "\n",
    "                    # Check if any variation matches\n",
    "                    if not equipment_counts.get(equipment_key):\n",
    "                        for variation in variations:\n",
    "                            if variation in content_text:\n",
    "                                equipment_counts[equipment_key] = True\n",
    "                                score += self.equipment_score\n",
    "                                break\n",
    "\n",
    "            # Content terms (counted once)\n",
    "            for term, weight in self.content_score_weights.items():\n",
    "                if term not in term_counts and term in content_text:\n",
    "                    term_counts[term] = True\n",
    "                    score += weight\n",
    "\n",
    "        return score\n",
    "\n",
    "    def is_valid_url(url: str) -> bool:\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Enhanced text normalization to handle various formats.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[/\\\\_-]', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def generate_equipment_variations(self, manufacturer: str, model: str) -> Set[str]:\n",
    "        \"\"\"Generate common variations of equipment names.\"\"\"\n",
    "        variations = set()\n",
    "\n",
    "        # Normalize manufacturer and model\n",
    "        manufacturer = self.normalize_text(manufacturer)\n",
    "        model = self.normalize_text(model)\n",
    "\n",
    "        # Add full name variations\n",
    "        variations.add(f\"{manufacturer} {model}\")\n",
    "        variations.add(f\"{manufacturer}{model}\")\n",
    "        variations.add(model)  # Model number alone\n",
    "\n",
    "        # Handle number-only models\n",
    "        if model.isdigit():\n",
    "            variations.add(f\"model {model}\")\n",
    "            variations.add(f\"type {model}\")\n",
    "\n",
    "        return variations\n",
    "\n",
    "#### Content Extraction Methods ####\n",
    "\n",
    "    def extract_structured_content(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"Enhanced extraction of structured content, handling lists, tables, and standalone content.\"\"\"\n",
    "        structured_content = []\n",
    "\n",
    "        # First process header-based sections as in original code\n",
    "        headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        processed_elements = set()  # Track elements we've processed\n",
    "\n",
    "        for header in headers:\n",
    "            header_text = header.get_text().strip()\n",
    "\n",
    "            section = {\n",
    "                'header': header_text,\n",
    "                'content': [],\n",
    "                'type': 'text'  # Default type\n",
    "            }\n",
    "\n",
    "            current = header.find_next_sibling()\n",
    "            while current and current.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                processed_elements.add(current)\n",
    "\n",
    "                if current.name in ['ul', 'ol']:\n",
    "                    section['type'] = 'list'\n",
    "                    for item in current.find_all('li'):\n",
    "                        section['content'].append(item.get_text().strip())\n",
    "                        processed_elements.add(item)\n",
    "\n",
    "                elif current.name == 'table':\n",
    "                    section['type'] = 'table'\n",
    "                    for row in current.find_all('tr'):\n",
    "                        cells = row.find_all(['td', 'th'])\n",
    "                        if cells:\n",
    "                            row_content = ' | '.join(cell.get_text().strip() for cell in cells)\n",
    "                            section['content'].append(row_content)\n",
    "                        processed_elements.add(row)\n",
    "\n",
    "                elif current.name == 'div':\n",
    "                    # Check for table-like divs (grid layouts)\n",
    "                    if 'table' in current.get('class', []) or 'grid' in current.get('class', []):\n",
    "                        section['type'] = 'table'\n",
    "                        items = current.find_all(['div', 'span'])\n",
    "                        for item in items:\n",
    "                            text = item.get_text().strip()\n",
    "                            if text:\n",
    "                                section['content'].append(text)\n",
    "                                processed_elements.add(item)\n",
    "\n",
    "                    # Check for list-like divs\n",
    "                    elif any(cls in current.get('class', []) for cls in ['list', 'items']):\n",
    "                        section['type'] = 'list'\n",
    "                        items = current.find_all(['div', 'span'])\n",
    "                        for item in items:\n",
    "                            text = item.get_text().strip()\n",
    "                            if text:\n",
    "                                section['content'].append(text)\n",
    "                                processed_elements.add(item)\n",
    "\n",
    "                elif current.name == 'p':\n",
    "                    if section['type'] == 'text':  # Only add if we haven't found a list/table yet\n",
    "                        section['content'].append(current.get_text().strip())\n",
    "\n",
    "                current = current.find_next_sibling()\n",
    "\n",
    "            # Only add sections with meaningful content\n",
    "            if len(section['content']) > 0:\n",
    "                structured_content.append(section)\n",
    "\n",
    "        # Now process standalone content that wasn't part of any header section\n",
    "        standalone_content = []\n",
    "        for element in soup.find_all(['p', 'div', 'ul', 'ol', 'table']):\n",
    "            # Skip if we've already processed this element\n",
    "            if element in processed_elements:\n",
    "                continue\n",
    "\n",
    "            content_text = element.get_text().strip()\n",
    "            if not content_text:\n",
    "                continue\n",
    "\n",
    "            if element.name in ['ul', 'ol']:\n",
    "                section = {\n",
    "                    'header': '',\n",
    "                    'content': [item.get_text().strip() for item in element.find_all('li')],\n",
    "                    'type': 'list'\n",
    "                }\n",
    "            elif element.name == 'table':\n",
    "                section = {\n",
    "                    'header': '',\n",
    "                    'content': [],\n",
    "                    'type': 'table'\n",
    "                }\n",
    "                for row in element.find_all('tr'):\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    if cells:\n",
    "                        row_content = ' | '.join(cell.get_text().strip() for cell in cells)\n",
    "                        section['content'].append(row_content)\n",
    "            elif element.name == 'div':\n",
    "                # Only process divs that look like they contain equipment info\n",
    "                text_content = element.get_text().strip().lower()\n",
    "                if any(keyword in text_content for keyword in self.url_keywords):\n",
    "                    section = {\n",
    "                        'header': '',\n",
    "                        'content': [content_text],\n",
    "                        'type': 'text'\n",
    "                    }\n",
    "                else:\n",
    "                    continue\n",
    "            else:  # paragraphs\n",
    "                section = {\n",
    "                    'header': '',\n",
    "                    'content': [content_text],\n",
    "                    'type': 'text'\n",
    "                }\n",
    "\n",
    "            if section['content']:\n",
    "                standalone_content.append(section)\n",
    "\n",
    "        # Combine all content, putting header-based sections first\n",
    "        structured_content.extend(standalone_content)\n",
    "\n",
    "        return structured_content\n",
    "    \n",
    "    def extract_download_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        \"\"\"Extract equipment-specific download links with enhanced detection for interactive elements.\"\"\"\n",
    "        download_links = []\n",
    "\n",
    "        # Enhanced selectors for download elements\n",
    "        download_selectors = {\n",
    "            'links': [\n",
    "                'a[href*=\".pdf\"]',\n",
    "                'a[href*=\"download\"]',\n",
    "                'a[href*=\"equipment\"]',\n",
    "                'a[class*=\"download\"]',\n",
    "                'a[class*=\"btn\"]',\n",
    "                '.download a',\n",
    "                '.btn-download',\n",
    "                '[data-link*=\"tech-equip\"]',  # Abbey Road specific\n",
    "                '[data-link*=\"equipment\"]'\n",
    "            ],\n",
    "            'containers': [\n",
    "                '.download',\n",
    "                '.equipment-list',\n",
    "                '.tech-specs',\n",
    "                '.specifications',\n",
    "                '[id*=\"tech\"]',\n",
    "                '[class*=\"tech\"]',\n",
    "                '.detail'  # Abbey Road specific\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # First check direct download links\n",
    "        for selector in download_selectors['links']:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                href = element.get('href', '')\n",
    "                data_link = element.get('data-link', '')\n",
    "\n",
    "                # Handle both regular links and data-link attributes\n",
    "                if href:\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                elif data_link:\n",
    "                    full_url = urljoin(base_url, data_link)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if self._is_valid_download_link(full_url, element):\n",
    "                    download_links.append(full_url)\n",
    "\n",
    "        # Then check container elements that might have nested download links\n",
    "        for selector in download_selectors['containers']:\n",
    "            containers = soup.select(selector)\n",
    "            for container in containers:\n",
    "                # Look for download links within containers\n",
    "                for link in container.find_all('a'):\n",
    "                    href = link.get('href', '')\n",
    "                    if href:\n",
    "                        full_url = urljoin(base_url, href)\n",
    "                        if self._is_valid_download_link(full_url, link):\n",
    "                            download_links.append(full_url)\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        return list(dict.fromkeys(download_links))\n",
    "\n",
    "    def _is_valid_download_link(self, url: str, element) -> bool:\n",
    "        \"\"\"Helper method to validate download links with enhanced checks.\"\"\"\n",
    "        url_lower = url.lower()\n",
    "\n",
    "        # Get all relevant text\n",
    "        element_text = element.get_text().strip().lower()\n",
    "        title_text = element.get('title', '').lower()\n",
    "        aria_label = element.get('aria-label', '').lower()\n",
    "\n",
    "        # Get parent context\n",
    "        parent = element.find_parent()\n",
    "        parent_text = parent.get_text().strip().lower() if parent else ''\n",
    "        parent_class = ' '.join(parent.get('class', [])).lower() if parent else ''\n",
    "\n",
    "        # Combine all text for checking\n",
    "        all_text = f\"{element_text} {title_text} {aria_label} {parent_text} {url_lower}\"\n",
    "\n",
    "        # Skip unwanted content\n",
    "        if any(term in all_text for term in ['floor plan', 'directions', 'contact']):\n",
    "            return False\n",
    "\n",
    "        # Check file extensions\n",
    "        valid_extensions = {'.pdf', '.doc', '.docx', '.xls', '.xlsx', '.txt', '.csv'}\n",
    "        if any(url_lower.endswith(ext) for ext in valid_extensions):\n",
    "            return True\n",
    "\n",
    "        # Check for download indicators\n",
    "        download_indicators = {\n",
    "            'download', 'equipment', 'spec', 'technical', 'gear list',\n",
    "            'mic list', 'specifications', 'tech sheet'\n",
    "        }\n",
    "\n",
    "        # Check if parent has download-related classes\n",
    "        if 'download' in parent_class or 'btn' in parent_class:\n",
    "            return True\n",
    "\n",
    "        # Check if any download indicators are present in text\n",
    "        return any(indicator in all_text for indicator in download_indicators)\n",
    "\n",
    "    def extract_content_from_pdf(self, pdf_url: str) -> str:\n",
    "        \"\"\"Download and extract text content from PDF.\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(pdf_url, timeout=30)\n",
    "            pdf_content = io.BytesIO(response.content)\n",
    "            \n",
    "            reader = PdfReader(pdf_content)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            \n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error extracting PDF content from {pdf_url}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def scrape_with_selenium(self, url: str) -> str:\n",
    "        \"\"\"Scrape page content using Selenium with improved JavaScript handling and interactive element detection.\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "\n",
    "            # Initial wait for page load\n",
    "            time.sleep(3)\n",
    "\n",
    "            # First find and click any collapsible sections/accordions\n",
    "            collapsible_selectors = [\n",
    "                \"[data-toggle='collapse']\",  # Bootstrap collapse\n",
    "                \".collapsible\",             # Common class name\n",
    "                \".accordion-button\",        # Bootstrap accordion\n",
    "                \".expandable\",             # Generic expandable\n",
    "                \"[aria-expanded]\",         # ARIA attribute\n",
    "                \".toggle-content\",         # Common toggle class\n",
    "                \"[role='button']\",         # ARIA role button\n",
    "                \".item[data-link]\"         # Abbey Road specific\n",
    "            ]\n",
    "\n",
    "            for selector in collapsible_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for element in elements:\n",
    "                        try:\n",
    "                            # Check if element is visible and clickable\n",
    "                            if element.is_displayed():\n",
    "                                # Scroll element into view\n",
    "                                self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "                                time.sleep(0.5)  # Brief pause for scroll\n",
    "\n",
    "                                # Try regular click first\n",
    "                                try:\n",
    "                                    element.click()\n",
    "                                except ElementClickInterceptedException:\n",
    "                                    # If regular click fails, try JavaScript click\n",
    "                                    self.driver.execute_script(\"arguments[0].click();\", element)\n",
    "\n",
    "                                # Wait for potential animations\n",
    "                                time.sleep(1)\n",
    "\n",
    "                                # If element has data-link attribute (Abbey Road specific)\n",
    "                                data_link = element.get_attribute('data-link')\n",
    "                                if data_link:\n",
    "                                    # Construct full URL if needed\n",
    "                                    if data_link.startswith('/'):\n",
    "                                        parsed_url = urlparse(url)\n",
    "                                        modal_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{data_link}\"\n",
    "                                        # Load modal content\n",
    "                                        self.driver.get(modal_url)\n",
    "                                        time.sleep(1)\n",
    "                                        # Go back to main page\n",
    "                                        self.driver.back()\n",
    "                                        time.sleep(1)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            logger.debug(f\"Error interacting with element: {str(e)}\")\n",
    "                            continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error finding elements with selector {selector}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            # Scroll through the page to trigger lazy loading\n",
    "            last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            while True:\n",
    "                # Scroll down in smaller increments\n",
    "                for i in range(0, last_height, 300):\n",
    "                    self.driver.execute_script(f\"window.scrollTo(0, {i});\")\n",
    "                    time.sleep(0.2)\n",
    "\n",
    "                # Calculate new scroll height\n",
    "                new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "                last_height = new_height\n",
    "\n",
    "            # Wait for any final dynamic content\n",
    "            time.sleep(2)\n",
    "\n",
    "            return self.driver.page_source\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error using Selenium for {url}: {str(e)}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = StudioEquipmentScraper(\n",
    "        max_workers=1,\n",
    "        rate_limit_delay=2.0,\n",
    "        batch_size=5,\n",
    "        verify_ssl=True,\n",
    "        use_selenium=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        scraper.scrape_studios(\n",
    "            input_file='data/studio_websites_ii.csv',\n",
    "            output_file='data/studio_equip_lists/equip_search_sel_ii2.csv',\n",
    "            detailed_output_file='data/detailed/detailed_equip_sel_ii2.csv'\n",
    "        )\n",
    "    finally:\n",
    "        scraper.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e268317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
